{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this first part of the exercise we will build a convolutional neural network from scratch in Numpy.\n",
    "We will also implement vanilla gradient descent and use it to train a classification head for the MNIST dataset.\n",
    "MNIST is a dataset that contains images of the digits 0-9 in 28px resolution. We will use it to classify the digit based on the image.\n",
    "\n",
    "Some remarks:\n",
    "- make sure to carefully read the markdowns as well as the comments present in the code, they contain all the information that you will need to finish this exercise\n",
    "- please add comments to the code you write, they do not have to be thorough but should provide a clear outline to what you are doing\n",
    "- sometimes you merely have to complete the lines of the code, sometimes you need to write entire code blocks yourself\n",
    "- There is a second part to this exercise (exercise_1.2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will load the necessary packages.\n",
    "Numpy for mathematical operations and torchvision as it contains the MNIST data that we will work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-17T20:09:01.835848086Z",
     "start_time": "2023-05-17T20:09:01.008014571Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torchvision.datasets import MNIST"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will load the training dataset and have a look at the first sample.\n",
    "Note that the images are stored as PIL images and we will need to convert them to numpy arrays later.\n",
    "The __getitem__() method loads the sample with the inserted index. The first entry of the sample is the image, the second is the label / corresponding digit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-17T20:09:01.883327012Z",
     "start_time": "2023-05-17T20:09:01.835219479Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<PIL.Image.Image image mode=L size=28x28>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABAElEQVR4nGNgGMyAWUhIqK5jvdSy/9/rGRgYGFhgEnJsVjYCwQwMDAxPJgV+vniQgYGBgREqZ7iXH8r6l/SV4dn7m8gmCt3++/fv37/Htn3/iMW+gDnZf/+e5WbQnoXNNXyMs/5GoQoxwVmf/n9kSGFiwAW49/11wynJoPzx4YIcRlyygR/+/i2XxCWru+vv32nSuGQFYv/83Y3b4p9/fzpAmSyoMnohpiwM1w5h06Q+5enfv39/bcMiJVF09+/fv39P+mFKiTtd/fv3799jgZiBJLT69t+/f/8eDuDEkDJf8+jv379/v7Ryo4qzMDAwMAQGMjBc3/y35wM2V1IfAABFF16Aa0wAOwAAAABJRU5ErkJggg=="
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_data = MNIST(root=\"./\", train=True, transform=None, target_transform=None, download=True)\n",
    "img = train_data.__getitem__(0)[0]\n",
    "display(img)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets have a look of some more samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-17T20:09:01.886584169Z",
     "start_time": "2023-05-17T20:09:01.869088932Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<PIL.Image.Image image mode=L size=28x28>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA/0lEQVR4nGNgGHhgPP/vfCMccgbv/vz58xa7nNnjv3/ev/xjyYYpxWXz4M/fP6dC/vytgggwIUnOPCDDwMBgxHOQQRdD0tibkfFQKeOL85OYGLG5ZTOPd6UoA8Pfz2gOVlv69+WFEAj775+lKHLsm/58cBeWgUkeRpG0/PPHHs5Blzz2dx+C8//vEWTX+hj834SQ/Pf/ArLG0D/PJOHWt//dxYMqeR8u1/znoTsDquREKMtg6Z+1DKgg7O9DCKPo3d9FaHIMoX9+TjKQDd308O/95RaYkn/+PL3+58+fI03oUgwMMsf//Pn758/LiZhSDAwMkg1//v7pVcUqR1cAAKxwbkTVIzd2AAAAAElFTkSuQmCC"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "data": {
      "text/plain": "<PIL.Image.Image image mode=L size=28x28>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA1ElEQVR4nGNgGArA+YU6AwMDAwMTAwMDg10gqqTpGQaEpEMQihyTohwjgndnMYqk9L9FSDqZUE2dw3AbIaknjirJz7AbIenFiSInrsjwFCGpznAVWbJH/NZnCIuFgYGBgeE0XIbPI8aNofkDsqQQAwODPpOzDFs00/eTP1nOQlUyMjAwTEv/8IiBQY/xz7drJ88cfPlEkI0BoTProRUDA8OjjddOMDAwMKSJ3mPACVb+64QxmbBIb8AnyYBHklEVj+R/JjySDJb4jMVj5/b/OB1IJQAAg3ksR3QPgSAAAAAASUVORK5CYII="
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    },
    {
     "data": {
      "text/plain": "<PIL.Image.Image image mode=L size=28x28>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAnElEQVR4nGNgGPyg5u9/e1xyCV9+/7WDMJkwJOXZcRvq8ub3ZXkO7HI2T37/jsOlcfbfv3txyYn8/f3aCYecwtm/v+twacz4/XcHPw65gA+/D4rjMvTv37/zcRk6/ffv3+o45Azu/v69BpfGV79/H+HBJfn39+9IXHLz///9K4/Lxid/v/fgCHAGh99/76CLYcYnNskbx/ApoyoAAGeYO0QsY6cRAAAAAElFTkSuQmCC"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "data": {
      "text/plain": "<PIL.Image.Image image mode=L size=28x28>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA1ElEQVR4nN3QPwtBYRQG8EMU0e0uZLIw+QKXRZlMGC0GX8CglE0pk0VxPwQmE5YrJYPVIjYMlImSwXNiMOi97319AM/6O6fzh+g/Y5hr5mrRNByseAZba4D7EnlSN8wy3uAYXJOwDEw0ohKwD9mtxehqRLQBCnZr8GPkJ/Ll79y0m37GiIjiK2AQsGMYiIbryyvjmZO20U9gAIcjTg43GhfethOROToO+En6xRUlZhnSjd+I6BY7xVIRY79w4XapR9IOSTWWYSWUqE0xlH771R7UrULefm5U2pxVCt0AAAAASUVORK5CYII="
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    },
    {
     "data": {
      "text/plain": "<PIL.Image.Image image mode=L size=28x28>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABD0lEQVR4nGNgGGSAEY3Py+Mt1vsTq1LF6Rf+/PkzCZuUxowvf/4+uPznhQaGFP+M93/+/Lkhr/rnjw2GZMKfP3/+3JRlQJJkgkuGMjA8WO36mAHJTBY4KzVt151XDAwM4ti9BQFzEcayoEjkcTP+12U4dhxTC5fp5r9////9+0QZQ4rV7PGfz09Wffrz53kpG5ocm9+fP7XWDEIX/vz58yecHVVf+58/WwQYRE///d649s+fHU6GhnA55o4/H7MEGUxP/LnhyMDnsfjjnz/34ZKZfz5FCHmu+vKnTpaBgYGBIXLLFlW45PM/X8/e+PPnTw0zFo+f//Pnz59NJSqovoZGNm+A0at5739h0Ta4AABroXIjERrLHgAAAABJRU5ErkJggg=="
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "data": {
      "text/plain": "<PIL.Image.Image image mode=L size=28x28>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAh0lEQVR4nGNgGGAw8f9leVxyCm///nFHFmBCYr8+hKYaWfLrQzySAvp4JLnkGBhMcbqo9u+fPzm4JBnQJJlQJJkYGZG5LCiS//7jdBAGIEGSiZHRDqfSv3/+/NHCpXMGAwNDGi7JG/hcwHDr79//yjh0Mlz9//8fLmMZZqHw0CSvXcdrKx0AAOciI63Ko1kqAAAAAElFTkSuQmCC"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "data": {
      "text/plain": "<PIL.Image.Image image mode=L size=28x28>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABEklEQVR4nM2RMS+DURiFn/ullKXC1KWJyVId2ARBQpqUHyBRC0NjsPsPNktj0F9QEgYiIvEDJG3CYhEpMTBI2qEk5+YzfP1uuD6bwVnum3ve877n5IV/jLH8Vmittfao36fyuw8tWUmSahmPPJEUk5oGIOXIixIvNRMyNZewMZXLZQEyLame9pR6jN7iMDx9JFtevZTk+4mwdtuVdD2IN3Z0fRFmQmjvnHY9TeE+jnLs/gJXGWOMCYwxKyUXIC5u5svn78DmdrJRAIYkpwx8svizv2+5536j/UUZYfZMOYCR8pvUWXAeAWiOU+0AS5MhV9XD78pm71Kyz/sD/sqJA0nSXWOvkBAgXXlVvZL9Jd4f4xPJmHJ5CeNkqwAAAABJRU5ErkJggg=="
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "data": {
      "text/plain": "<PIL.Image.Image image mode=L size=28x28>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAc0lEQVR4nGNgGMyA1f4obkmRf88kkPlMqNIS+CQZGfBI/ufEI8lgjFPyz0cGZZySHw6jGoNuLF5JYXySfrgl9+Mz9hEDqzxOyT8MjOy43Xft3zTckhM+cuA0loHh/y88knwBuI199l0Dt85Dt77j1kktAADVQhZzhi0BcQAAAABJRU5ErkJggg=="
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "data": {
      "text/plain": "<PIL.Image.Image image mode=L size=28x28>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA7klEQVR4nM3QsUtCURiG8QdRFIKEoCHIGtouSM4SRn9CixE0REtjS4tu0tLm4tIS4tLeFqE0FNjukIqLDrchCBq85H1Pt6Gl7vGs4bed78cDHwcWZtodgMRcqxeHzu4y+Cg78UH39rJ0twJw+NbftvHF7AD0ov2f95+DplEGKGx8ZezwIuytwtKNnlKW5V6DXeBKY7vLD1UHzj91GqfksYlMt5pee55dW92RZPpSdyLfsoMw8PcKbckonGzFsDM6AbxHGakVL89yAKV3lT1v2T4WyDbMYC4AVOSvu2xzFNac4UDN2ObXxze5dYb/N9+FeFNxEamP7gAAAABJRU5ErkJggg=="
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,10):\n",
    "    img = train_data.__getitem__(i)[0]\n",
    "    label = train_data.__getitem__(i)[1]\n",
    "    display(img)\n",
    "    print(label)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is an example of how we can transform the data into numpy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-17T20:09:01.886887752Z",
     "start_time": "2023-05-17T20:09:01.877906970Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28\n"
     ]
    }
   ],
   "source": [
    "img = np.array(train_data.__getitem__(0)[0])\n",
    "label = np.array(train_data.__getitem__(0)[1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your first task is to write a convolutional layer in numpy. For simplicity, you only need to implement the layer such that it has only one input and output channel. Check slides 49-55 of the lecture for information on conv layers.\n",
    "\n",
    "Processing input through the neural network to get some form of output is called a \"forward\" operation, make sure that the forward operation applies the convolution operation to an input of arbitrary size. The input to the forward function is supposed to be a \"batched\" numpy array, meaning that the layer can process multiple different inputs at once. Input and output arrays then consist of a batch-dimension; its common practice to use the first dimension as batch-dimension. Even input arrays that only contain one sample should consist of a batch-dimension (this can f.i. be done with np.expand_dims). Moreover, code your layer such that you can provide a list that contains the weights of the convolutional kernel, if no list is provided the kernel should be initialized with random weights.\n",
    "\n",
    "Refer to slide 101 for batching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "class conv_layer():\n",
    "    # implement init here\n",
    "    # layer should operate with arbitrary (but reasonable) parameters\n",
    "    def __init__(self, kernel_size: int, zero_padding_size: int, stride: int, weights: list = None):\n",
    "        self.kernel_size = kernel_size\n",
    "        self.zero_padding_size = zero_padding_size\n",
    "        self.stride = stride\n",
    "        self.weights = weights\n",
    "        if self.weights is None:\n",
    "            self.weights = np.random.rand(self.kernel_size, self.kernel_size) # init random kernel\n",
    "\n",
    "    def add_zero_padding(self, image):\n",
    "        height = image.shape[0]\n",
    "        width = image.shape[1]\n",
    "        new_height = height + 2 * self.zero_padding_size\n",
    "        new_width = width + 2 * self.zero_padding_size\n",
    "        padded_image = np.zeros((new_height, new_width))\n",
    "        padded_image[self.zero_padding_size:self.zero_padding_size+height,\n",
    "        self.zero_padding_size:self.zero_padding_size+width] = image\n",
    "        return padded_image\n",
    "\n",
    "    # implement conv operation here\n",
    "    def forward(self, input):\n",
    "        batch_size = input.shape[0]\n",
    "        outputs = []\n",
    "        for i in range(batch_size):\n",
    "            input_sample = self.add_zero_padding(input[i])\n",
    "            for x in range(1, input_sample.shape[1] - 1, self.stride):\n",
    "                 for y in range(1, input_sample.shape[0] - 1, self.stride):\n",
    "                     v = np.sum(input_sample[(y - self.kernel_size // 2):(y + (self.kernel_size // 2 + 1)),(x - self.kernel_size // 2):(x + (self.kernel_size // 2 + 1))] * self.weights)\n",
    "                     input_sample[y, x] = v\n",
    "            outputs.append(input_sample)\n",
    "        output_batch = np.array(outputs)\n",
    "        return output_batch"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-17T20:09:46.743024864Z",
     "start_time": "2023-05-17T20:09:46.694701949Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00]\n",
      "  [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 1.67262672e+12\n",
      "   7.94052426e+12 0.00000000e+00]\n",
      "  [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 7.36210307e+12\n",
      "   3.47800896e+13 0.00000000e+00]\n",
      "  ...\n",
      "  [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 1.51904733e+20\n",
      "   8.30980429e+20 0.00000000e+00]\n",
      "  [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 1.58270959e+20\n",
      "   8.68566432e+20 0.00000000e+00]\n",
      "  [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00]]]\n"
     ]
    }
   ],
   "source": [
    "# Test your conv layer here\n",
    "conv = conv_layer(3, 2, 1, None)\n",
    "out = conv.forward(np.expand_dims(img, axis=0))\n",
    "print(out)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your next task is to write a fully connected layer. For details refer to slides 38-39,47 in the course material. \n",
    "Again, make sure that your layer can process a batched input and that weights can be provided manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "class fc_layer():\n",
    "    # implement init here\n",
    "    def __init__(self, in_neurons: int, out_neurons: int, weights: list = None):\n",
    "        ...\n",
    "    # implement forward pass here (hint: its just a matrix multiplication)\n",
    "    def forward(self, input):\n",
    "        ..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to build or small convolutional neural network. \n",
    "Here are some activations that we use in the net. The softmax function transforms the output of our network into probabilites for each possible output (in our case the digits from 0-9).\n",
    "Refer to slide 66 for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return x * (x > 0)\n",
    "\n",
    "def softmax(x):\n",
    "    exp_vals = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
    "    return exp_vals / np.sum(exp_vals, axis=-1, keepdims=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your next task is to build a simple cnn with one conv layer followed by a fully connected layer.\n",
    "The cnn shall be build such that input images with 28px resolution are mapped to an output dimension of 10 (digits 0-9, one output probability for each class).\n",
    "Make sure that the conv layer is followed by relu activation before being fed into the fully connected layer. Also ensure that the dimensions are adjusted appropriately such that the output of the conv layer can be processed by the fc layer. Finally, the output shall be passed through the softmax function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "class simple_cnn():\n",
    "    def __init__(self):\n",
    "        ...\n",
    "    def forward(self, input):\n",
    "        ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test your cnn with an input of size (batch_size, 28, 28) here, output dim should be (batch_size, 10)\n",
    "# Set the non-weight params of the conv layer to 3, 1 and 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the labels shown earlier contained the actual displayed digit, we want to transform them into a so called one-hot-encoded vector that contains a 1 at the position of the true class and otherwise 0s. Check slides 89 or https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html for reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_one_hot_encoding(x, dim=10):\n",
    "    output = np.zeros(dim)\n",
    "    output[x] = 1\n",
    "    return output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function return the mean squared error of a target/label vector and a prediction. See slide 87 for reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(target, pred):\n",
    "    return np.mean(np.sum((target-pred)**2))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final task of this exercise is to train a fully connected layer to predict the digit based on the corresponding 28px input.\n",
    "To do so, you need to implement the backpropagation algorithm with respect to the above mse function.\n",
    "Check slides 96-98 for references to the backpropagation algorithm.\n",
    "\n",
    "Some remarks:\n",
    "- you can ignore the conv layer and cnn from now on as we will only train a fully connected layer for simplicity\n",
    "- the following backward function needs to compute the gradient of the passed weights of a fully connected layer for each weight and should have the same shape as the weights\n",
    "- gradients over batches need to be averaged\n",
    "- all of input, target, pred and fc_weights need to be used in order to compute the correct gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_mse_to_fc(input, target, pred, fc_weights):\n",
    "    gradients = np.zeros(fc_weights.shape)\n",
    "    # compute gradients here\n",
    "\n",
    "\n",
    "    return gradients"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to implement a training loop. We want to feed each sample of the train data exactly once to our fc layer (one epoch). At the beginning of each training step we want to obtain the batched samples and one-hot-encoded labels. Then we feed the samples to our layer and obtain the predictions, which then need to be fed to the softmax function. Finally, we need to compute the gradients and perform basic gradient descent on the weights of the fc layer. Write the code such that arbitrary learning rates and batch sizes can be used.\n",
    "\n",
    "See slide 99 for references on the gradient descent algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define fc layer here\n",
    "in_neurons = ...\n",
    "out_neurons = ...\n",
    "fc = fc_layer(in_neurons, out_neurons)\n",
    "\n",
    "# define learning rate and batch_size\n",
    "lr = ...\n",
    "batch_size = ...\n",
    "# num_samples = 60000\n",
    "num_samples = len(train_data)\n",
    "\n",
    "for i in range(num_samples // batch_size):\n",
    "    # fill input and target batches here\n",
    "    # make sure that labels are one_hot_encoded\n",
    "    # make sure that image dimensions are changed such that the fc layer can handle it\n",
    "    # the layer can process the input easier if you divide it by 255\n",
    "    input = np.zeros((batch_size, in_neurons))\n",
    "    targets = np.zeros((batch_size, out_neurons))\n",
    "\n",
    "    # perform prediction here\n",
    "    pred = ...\n",
    "\n",
    "    # compute loss here, if you want also monitor your loss here to see if it improves during training\n",
    "    loss = ...\n",
    "\n",
    "    # compute gradients here\n",
    "    gradients = backward_mse_to_fc(input, targets, pred, fc.weights)\n",
    "\n",
    "    # update layer weights here\n",
    "    fc.weights = ..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we ran the training loop, we want to test our model on hold-out test data.\n",
    "First we load the test data. (10000 samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = MNIST(root=\"./\", train=False, transform=None, target_transform=None, download=True)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the final task, compute the accuracy of your model on the test data.\n",
    "For simplicity, you can use a batch_size of 1 here if you want to.\n",
    "\n",
    "Check slides 103-104 for references to the accuracy score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_test_samples = len(test_data)\n",
    "results = []\n",
    "\n",
    "for i in range(num_test_samples):\n",
    "    # load sample and label here\n",
    "    # dont forget: change of dims for forward pass through fc, divide by 255 if you did so before, one-hot-encoding, adding batch_dimension\n",
    "    input = ...\n",
    "    label = ...\n",
    "\n",
    "    # compute prediction\n",
    "    pred = ...\n",
    "\n",
    "    # use prediction to update the accuracy score or store in results lists \n",
    "\n",
    "# compute and report final accuracy\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrain the fc layer at least 2 more times with different batch_sizes and or learning_rates. \n",
    "Report the results either in this submission notebook or just report them in text."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepfake",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
