{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Synthetic Data Generation and Data Augmentation (SOLUTION)\n",
        "\n",
        "## Introduction\n",
        "\n",
        "In this exercise, we will explore an exciting approach to tackling vision problems when labeled ground truth data is either unavailable or limited in quantity. We aim to conduct experiments with the benefits of full supervision, even under such challenging circumstances.\n",
        "\n",
        "Our specific focus will be on solving the problem of semantic segmentation, where we aim to accurately segment a target object in an image.\n",
        "\n",
        "In our experiment, the target object is available as a 3D model and no labelled real-world training data is available.\n",
        "\n",
        "To still achieve our goal, we will follow a structured approach.\n",
        "\n",
        "1. **Synthetic Data Generation**: We will leverage the power of **BlenderProc**, a versatile tool, to create synthetic training data. This allows to generate labeled data without manual efford.\n",
        "\n",
        "2. **Data Augmentation**: To further increase the varity of the data during training, we will explore different data augmentation techniques. By augmenting our dataset, we can introduce variations and increase the robustness of the trained model.\n",
        "\n",
        "3. **Building the Segmentation Network**: We will construct a state-of-the-art segmentation network called **U-NET**. This network architecture employs a pretrained **MobileNetV2** backbone, which provides a strong foundation for accurate segmentation. We will train the network with our generated data and we will also use the implemented data augmentation.\n",
        "\n",
        "\n",
        "**IMPORTANT NOTE ON HAND-IN PROCESS**\n",
        "\n",
        "Besides the saved notebook, please also hand in the maniuplated files `dataset_generator.py` and `quickstart.py`, as well as a ***sample from your generated dataset*** (one image and mask) and the `loss_plot.png` image (several versions, if you did several tests) and a ***brief interpretation of the results***. Everything can be handed in as one zip file.\n",
        "\n",
        "**Besides this notebook the edited files are not saved in Colab persistently, so  remember to copy the intermediate results to your computer.**\n"
      ],
      "metadata": {
        "id": "OjosY2MvsyMS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prerequirities\n",
        "\n",
        "We will first import the necessary data. Please, before running the following code, access the link below, right click on the .zip file, then click \"Add shortcut to Drive\" and select your Drive folder:\n",
        "\n",
        "**(Since this this the solution version of the notebook, this file already contains the prepared scripts as well as generated training data)**\n",
        "\n",
        "https://drive.google.com/drive/folders/1wABnew5eYclFMM6jwyNnX208Sx4qJavl?usp=sharing"
      ],
      "metadata": {
        "id": "T2gXSmdLzjqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!unzip -qq '/content/drive/MyDrive/Resources_DNN4VC_Synthetic.zip'"
      ],
      "metadata": {
        "id": "_qxuEWoGvB3N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "86f1bafc-8c55-464e-d6c3-7cde32614013"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Make shure to request GPUs from Colab!**\n",
        "\n",
        "-> Small arrow in the top-right corner -> \"View Resources\" -> \"Change runtime type\" (in the bottom)\n",
        "\n",
        "**Make a copy to allow saving!**\n"
      ],
      "metadata": {
        "id": "vSIBTMPvzdi1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 1: Data generation\n",
        "\n",
        "Next we are going to take a look at the object we want to recognize. Luckily, we have got a 3D scan of it.\n",
        "We can use the `trimesh` to take a look at the object."
      ],
      "metadata": {
        "id": "pM_o6PnY2RVP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install trimesh"
      ],
      "metadata": {
        "id": "shXfvS9P3IpM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf1668d6-c1b4-4d10-bff3-ba90d51a466b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting trimesh\n",
            "  Downloading trimesh-3.22.2-py3-none-any.whl (682 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/682.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m682.2/682.2 kB\u001b[0m \u001b[31m48.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from trimesh) (1.22.4)\n",
            "Installing collected packages: trimesh\n",
            "Successfully installed trimesh-3.22.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import trimesh\n",
        "import numpy as np\n",
        "\n",
        "# Load the PLY file\n",
        "mesh = trimesh.load_mesh('/content/Resources_DNN4VC_Synthetic/resources/lm/models/obj_000002.ply')\n",
        "s = trimesh.Scene()\n",
        "s.add_geometry(mesh)\n",
        "s.show()\n"
      ],
      "metadata": {
        "id": "_hs4X4F62wmS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we will have a look at our test data. Use the following script to show example images. In the path, you can replace `test_data` with `train_data/example` to see a preview image similar to the dataset you will create later.\n",
        "\n",
        "\n",
        "**Important**: In our example we have ground truth mask for the test images to evaluate the models performance."
      ],
      "metadata": {
        "id": "HwQZ8H54DdBl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "\n",
        "img_no = 2\n",
        "# Load the color image and the mask image\n",
        "color_image = cv2.imread('/content/Resources_DNN4VC_Synthetic/test_data/img/{:04}.png'.format(img_no))\n",
        "mask_image = cv2.imread('/content/Resources_DNN4VC_Synthetic/test_data/masks/{:04}.png'.format(img_no))\n",
        "\n",
        "# Convert the images from BGR to RGB\n",
        "color_image = cv2.cvtColor(color_image, cv2.COLOR_BGR2RGB)\n",
        "mask_image = cv2.cvtColor(mask_image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "# Create a figure with two subplots\n",
        "fig, axs = plt.subplots(1, 2)\n",
        "\n",
        "axs[0].imshow(color_image)\n",
        "axs[0].axis('off')\n",
        "axs[0].set_title('Color Image')\n",
        "axs[1].imshow(mask_image)\n",
        "axs[1].axis('off')\n",
        "axs[1].set_title('Mask Image')\n",
        "\n",
        "# Show the figure\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "VolaNOOiDtXW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.1 Preparation of BlenderProc\n",
        "\n",
        "Next, we are going to use [BlenderProc](https://github.com/DLR-RM/BlenderProc]). We install it by cloning the official git repository and then install it via pip."
      ],
      "metadata": {
        "id": "PkGAFt4ZC1gy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/DLR-RM/BlenderProc.git\n",
        "%cd \"BlenderProc\"\n",
        "!pip install -e ."
      ],
      "metadata": {
        "id": "rSMAeYxfBZwr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3cd77070-dc51-4231-b220-f7ecfe07fac8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'BlenderProc'...\n",
            "remote: Enumerating objects: 43049, done.\u001b[K\n",
            "remote: Counting objects: 100% (254/254), done.\u001b[K\n",
            "remote: Compressing objects: 100% (111/111), done.\u001b[K\n",
            "remote: Total 43049 (delta 162), reused 212 (delta 142), pack-reused 42795\u001b[K\n",
            "Receiving objects: 100% (43049/43049), 90.45 MiB | 14.38 MiB/s, done.\n",
            "Resolving deltas: 100% (32455/32455), done.\n",
            "Updating files: 100% (645/645), done.\n",
            "/content/BlenderProc\n",
            "Obtaining file:///content/BlenderProc\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from blenderproc==2.5.0) (67.7.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from blenderproc==2.5.0) (6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from blenderproc==2.5.0) (2.27.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from blenderproc==2.5.0) (3.7.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from blenderproc==2.5.0) (1.22.4)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from blenderproc==2.5.0) (8.4.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from blenderproc==2.5.0) (3.8.0)\n",
            "Collecting progressbar (from blenderproc==2.5.0)\n",
            "  Downloading progressbar-2.5.tar.gz (10 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->blenderproc==2.5.0) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->blenderproc==2.5.0) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->blenderproc==2.5.0) (4.40.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->blenderproc==2.5.0) (1.4.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->blenderproc==2.5.0) (23.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->blenderproc==2.5.0) (3.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->blenderproc==2.5.0) (2.8.2)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->blenderproc==2.5.0) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->blenderproc==2.5.0) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->blenderproc==2.5.0) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->blenderproc==2.5.0) (3.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->blenderproc==2.5.0) (1.16.0)\n",
            "Building wheels for collected packages: progressbar\n",
            "  Building wheel for progressbar (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for progressbar: filename=progressbar-2.5-py3-none-any.whl size=12067 sha256=74469f9cab75dab5d0bce81af73e25dafa30505cfa21430fb9da7d219410e574\n",
            "  Stored in directory: /root/.cache/pip/wheels/cd/17/e5/765d1a3112ff3978f70223502f6047e06c43a24d7c5f8ff95b\n",
            "Successfully built progressbar\n",
            "Installing collected packages: progressbar, blenderproc\n",
            "  Running setup.py develop for blenderproc\n",
            "Successfully installed blenderproc-2.5.0 progressbar-2.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "To be able to use matplotlib inside BlenderProc, we have to install ipykernel inside blender's python environment. As this is the first blenderproc command, it will also install blender first:"
      ],
      "metadata": {
        "id": "38vWtXCvJxWU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!blenderproc pip install ipykernel --blender-install-path ./"
      ],
      "metadata": {
        "id": "IXCvaNuEJuKX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e651c7cc-0845-4981-a517-921d9f6c38db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading blender from https://download.blender.org/release/Blender3.3/blender-3.3.1-linux-x64.tar.xz\n",
            "100% ||\n",
            "Using blender in ./blender-3.3.1-linux-x64\n",
            "Looking in links: /tmp/tmpaga0fet3\n",
            "Processing /tmp/tmpaga0fet3/setuptools-58.1.0-py3-none-any.whl\n",
            "Processing /tmp/tmpaga0fet3/pip-21.2.4-py3-none-any.whl\n",
            "Installing collected packages: setuptools, pip\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "blenderproc 2.5.0 requires h5py, which is not installed.\n",
            "blenderproc 2.5.0 requires matplotlib, which is not installed.\n",
            "blenderproc 2.5.0 requires numpy, which is not installed.\n",
            "blenderproc 2.5.0 requires Pillow, which is not installed.\n",
            "blenderproc 2.5.0 requires progressbar, which is not installed.\n",
            "blenderproc 2.5.0 requires pyyaml, which is not installed.\n",
            "blenderproc 2.5.0 requires requests, which is not installed.\u001b[0m\n",
            "Successfully installed pip-21.2.4 setuptools-58.1.0\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
            "Requirement already satisfied: pip in ./blender-3.3.1-linux-x64/3.3/python/lib/python3.10/site-packages (21.2.4)\n",
            "Collecting pip\n",
            "  Downloading pip-23.1.2-py3-none-any.whl (2.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.1 MB 10.3 MB/s \n",
            "\u001b[?25hInstalling collected packages: pip\n",
            "  Attempting uninstall: pip\n",
            "    Found existing installation: pip 21.2.4\n",
            "    Uninstalling pip-21.2.4:\n",
            "      Successfully uninstalled pip-21.2.4\n",
            "Successfully installed pip-23.1.2\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
            "Installing pip package ipykernel None\n",
            "Collecting ipykernel\n",
            "  Downloading ipykernel-6.23.3-py3-none-any.whl (152 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m152.8/152.8 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting comm>=0.1.1 (from ipykernel)\n",
            "  Downloading comm-0.1.3-py3-none-any.whl (6.6 kB)\n",
            "Collecting debugpy>=1.6.5 (from ipykernel)\n",
            "  Downloading debugpy-1.6.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m81.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ipython>=7.23.1 (from ipykernel)\n",
            "  Downloading ipython-8.14.0-py3-none-any.whl (798 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m798.7/798.7 kB\u001b[0m \u001b[31m66.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jupyter-client>=6.1.12 (from ipykernel)\n",
            "  Downloading jupyter_client-8.3.0-py3-none-any.whl (103 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.2/103.2 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jupyter-core!=5.0.*,>=4.12 (from ipykernel)\n",
            "  Downloading jupyter_core-5.3.1-py3-none-any.whl (93 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.7/93.7 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting matplotlib-inline>=0.1 (from ipykernel)\n",
            "  Downloading matplotlib_inline-0.1.6-py3-none-any.whl (9.4 kB)\n",
            "Collecting nest-asyncio (from ipykernel)\n",
            "  Downloading nest_asyncio-1.5.6-py3-none-any.whl (5.2 kB)\n",
            "Collecting packaging (from ipykernel)\n",
            "  Downloading packaging-23.1-py3-none-any.whl (48 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.9/48.9 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting psutil (from ipykernel)\n",
            "  Downloading psutil-5.9.5-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (282 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m282.1/282.1 kB\u001b[0m \u001b[31m32.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyzmq>=20 (from ipykernel)\n",
            "  Downloading pyzmq-25.1.0-cp310-cp310-manylinux_2_28_x86_64.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m80.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tornado>=6.1 (from ipykernel)\n",
            "  Downloading tornado-6.3.2-cp38-abi3-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (426 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m426.9/426.9 kB\u001b[0m \u001b[31m42.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting traitlets>=5.4.0 (from ipykernel)\n",
            "  Downloading traitlets-5.9.0-py3-none-any.whl (117 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.4/117.4 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting backcall (from ipython>=7.23.1->ipykernel)\n",
            "  Downloading backcall-0.2.0-py2.py3-none-any.whl (11 kB)\n",
            "Collecting decorator (from ipython>=7.23.1->ipykernel)\n",
            "  Downloading decorator-5.1.1-py3-none-any.whl (9.1 kB)\n",
            "Collecting jedi>=0.16 (from ipython>=7.23.1->ipykernel)\n",
            "  Downloading jedi-0.18.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m58.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pickleshare (from ipython>=7.23.1->ipykernel)\n",
            "  Downloading pickleshare-0.7.5-py2.py3-none-any.whl (6.9 kB)\n",
            "Collecting prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30 (from ipython>=7.23.1->ipykernel)\n",
            "  Downloading prompt_toolkit-3.0.38-py3-none-any.whl (385 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m385.8/385.8 kB\u001b[0m \u001b[31m37.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pygments>=2.4.0 (from ipython>=7.23.1->ipykernel)\n",
            "  Downloading Pygments-2.15.1-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m77.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting stack-data (from ipython>=7.23.1->ipykernel)\n",
            "  Downloading stack_data-0.6.2-py3-none-any.whl (24 kB)\n",
            "Collecting pexpect>4.3 (from ipython>=7.23.1->ipykernel)\n",
            "  Downloading pexpect-4.8.0-py2.py3-none-any.whl (59 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.0/59.0 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-dateutil>=2.8.2 (from jupyter-client>=6.1.12->ipykernel)\n",
            "  Downloading python_dateutil-2.8.2-py2.py3-none-any.whl (247 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m247.7/247.7 kB\u001b[0m \u001b[31m30.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting platformdirs>=2.5 (from jupyter-core!=5.0.*,>=4.12->ipykernel)\n",
            "  Downloading platformdirs-3.8.0-py3-none-any.whl (16 kB)\n",
            "Collecting parso<0.9.0,>=0.8.0 (from jedi>=0.16->ipython>=7.23.1->ipykernel)\n",
            "  Downloading parso-0.8.3-py2.py3-none-any.whl (100 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m100.8/100.8 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ptyprocess>=0.5 (from pexpect>4.3->ipython>=7.23.1->ipykernel)\n",
            "  Downloading ptyprocess-0.7.0-py2.py3-none-any.whl (13 kB)\n",
            "Collecting wcwidth (from prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30->ipython>=7.23.1->ipykernel)\n",
            "  Downloading wcwidth-0.2.6-py2.py3-none-any.whl (29 kB)\n",
            "Collecting six>=1.5 (from python-dateutil>=2.8.2->jupyter-client>=6.1.12->ipykernel)\n",
            "  Downloading six-1.16.0-py2.py3-none-any.whl (11 kB)\n",
            "Collecting executing>=1.2.0 (from stack-data->ipython>=7.23.1->ipykernel)\n",
            "  Downloading executing-1.2.0-py2.py3-none-any.whl (24 kB)\n",
            "Collecting asttokens>=2.1.0 (from stack-data->ipython>=7.23.1->ipykernel)\n",
            "  Downloading asttokens-2.2.1-py2.py3-none-any.whl (26 kB)\n",
            "Collecting pure-eval (from stack-data->ipython>=7.23.1->ipykernel)\n",
            "  Downloading pure_eval-0.2.2-py3-none-any.whl (11 kB)\n",
            "Installing collected packages: wcwidth, pure-eval, ptyprocess, pickleshare, executing, backcall, traitlets, tornado, six, pyzmq, pygments, psutil, prompt-toolkit, platformdirs, pexpect, parso, packaging, nest-asyncio, decorator, debugpy, python-dateutil, matplotlib-inline, jupyter-core, jedi, comm, asttokens, stack-data, jupyter-client, ipython, ipykernel\n",
            "\u001b[33m  WARNING: The script pygmentize is installed in '/content/BlenderProc/blender-3.3.1-linux-x64/custom-python-packages/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33m  WARNING: The scripts jupyter, jupyter-migrate and jupyter-troubleshoot are installed in '/content/BlenderProc/blender-3.3.1-linux-x64/custom-python-packages/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33m  WARNING: The scripts jupyter-kernel, jupyter-kernelspec and jupyter-run are installed in '/content/BlenderProc/blender-3.3.1-linux-x64/custom-python-packages/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33m  WARNING: The scripts ipython and ipython3 are installed in '/content/BlenderProc/blender-3.3.1-linux-x64/custom-python-packages/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
            "\u001b[0mSuccessfully installed asttokens-2.2.1 backcall-0.2.0 comm-0.1.3 debugpy-1.6.7 decorator-5.1.1 executing-1.2.0 ipykernel-6.23.3 ipython-8.14.0 jedi-0.18.2 jupyter-client-8.3.0 jupyter-core-5.3.1 matplotlib-inline-0.1.6 nest-asyncio-1.5.6 packaging-23.1 parso-0.8.3 pexpect-4.8.0 pickleshare-0.7.5 platformdirs-3.8.0 prompt-toolkit-3.0.38 psutil-5.9.5 ptyprocess-0.7.0 pure-eval-0.2.2 pygments-2.15.1 python-dateutil-2.8.2 pyzmq-25.1.0 six-1.16.0 stack-data-0.6.2 tornado-6.3.2 traitlets-5.9.0 wcwidth-0.2.6\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lastly, we execute the BlenderProc program by invoking blenderproc run along with the necessary command line arguments. The initial argument indicates the path to the Python file that should be executed. The second argument corresponds to the camera pose file. In this instance, we have specified two camera poses in the `examples/basics/basic/camera_positions` file. The third argument pertains to the output directory where the generated data will be saved.\n",
        "\n",
        "To ensure compatibility, we use the `--blender-install-path` flag to specify a custom Blender installation path, as the availability of a user folder in Colab is limited."
      ],
      "metadata": {
        "id": "s-dziBU5LX2H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!blenderproc run examples/basics/basic/main.py examples/resources/camera_positions examples/resources/scene.obj examples/basics/basic/output --blender-install-path ./"
      ],
      "metadata": {
        "id": "OFwP8hSJJx93"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We genearated two example images, which we now will take a look at. As you will see, Blender Proc did not only generate the colors, but also the normals and depth information.\n"
      ],
      "metadata": {
        "id": "t1QNfmcLL6yp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%run \"-m\" \"blenderproc\" \"vis\" \"hdf5\" \"examples/basics/basic/output/0.hdf5\""
      ],
      "metadata": {
        "id": "uvbOjivnMCvx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%run \"-m\" \"blenderproc\" \"vis\" \"hdf5\" \"examples/basics/basic/output/1.hdf5\""
      ],
      "metadata": {
        "id": "ltjsX4lAMTIL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you saw before, executing BlenderProc differs from running a regular Python file. Instead of directly executing the Python script, BlenderProc runs within the context of Blender's integrated Python environment. This integration allows Blender to execute the specified Python script and perform the desired tasks using its internal functionality and capabilities.\n",
        "\n",
        "```\n",
        "!blenderproc run examples/basics/basic/main.py\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "rTxOAJZZNKNt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.1. Getting started with BlenderProc\n",
        "\n",
        "Colab allows us to open a script in an integrated editor. To do so, open the \"Files\" panel on the left (File icon), navigate to `Resources_DNN4VC_Synthetic/scripts/` and double click `quickstart.py`. The file will open on the right side of the screen. Get familiar with the script and test it using the code block below.\n",
        "\n",
        "\n",
        "**Task 1a)**: Do the following things:\n",
        "\n",
        "1. Use `bproc.loader.load_obj` to load `obj_000002.ply` (path was given above, when viewing the Mesh).\n",
        "2. Set a custom camera matrix with `bproc.camera.set_intrinsics_from_K_matrix` [docs](https://dlr-rm.github.io/BlenderProc/blenderproc.api.camera.html). Use the camera matrix given below. Render images with the size 320 by 240. Caution, the camera calibration is given for a resolution of 640 by 480. It has to be scaled accordingly.\n",
        "3. Adapt the light position and transform mat according to the scale of our example object. For the example object the unit is cm while for the Monkey it is m.\n",
        "\n",
        "\\begin{equation*}\n",
        "\\mathbf{C} = \\begin{bmatrix}\n",
        "537.5 & 0 & 318.9 \\\\\n",
        "0 & 536.1 & 238.4 \\\\\n",
        "0 & 0 & 1\n",
        "\\end{bmatrix}\n",
        "\\end{equation*}"
      ],
      "metadata": {
        "id": "o6QsiO1AT9w2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "While adapting the code, you can check the generated image using the following code lines. The output is shown by executing the code box below. You can play around with the code to get familiar with BlenderProc.\n",
        "\n",
        "**IMPORTANT** When you are ready with code, make sure to save a copy, as the files will not be necessarily be saved when restarting the notebook."
      ],
      "metadata": {
        "id": "V-4o5wFPU9pB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!blenderproc run /content/Resources_DNN4VC_Synthetic/scripts/quickstart.py --blender-install-path ./\n",
        "%run \"-m\" \"blenderproc\" \"vis\" \"hdf5\" \"output/0.hdf5\""
      ],
      "metadata": {
        "id": "4mNVNTrHODXy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2 Generating the training dataset\n",
        "\n",
        "Now, we will generate the actual dataset. Similar to our test data, the object should be placed in upright position in a room (without ceiling) shaped test environment with random material.\n",
        "\n",
        "**Task 1b)**:\n",
        "The script to do so is already given, but a couple of tasks have to be performed.\n",
        "\n",
        "1. Use correct camera matrix (see Task 1a).\n",
        "2. Load and place distractor objects\n",
        "3. Complete the 'room' environment\n",
        "4. Sample random camera poses\n",
        "5. Activate segmentation output\n",
        "6. Save dataset in the same format as the testing data.\n",
        "\n",
        "A detailed despriction is given directly in the script and should be added there.\n",
        "\n",
        "The code box below allows to test the script. The initial version can be executed, but will only save an example output."
      ],
      "metadata": {
        "id": "clCUPJNtT4ZG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%env BASE_PATH=/content/Resources_DNN4VC_Synthetic\n",
        "!blenderproc run $BASE_PATH/scripts/dataset_generator.py \\\n",
        "    --mesh_path $BASE_PATH/resources/lm \\\n",
        "    --cc_textures_path $BASE_PATH/resources/cctextures \\\n",
        "    --output_dir $BASE_PATH/train_data \\\n",
        "    --num_samples 1 \\\n",
        "    --start_index 1 \\\n",
        "    --blender-install-path ./"
      ],
      "metadata": {
        "id": "uGnxe9vbYikf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "207002a1-adc0-4eaf-d2e8-13788c58ef04"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "env: BASE_PATH=/content/Resources_DNN4VC_Synthetic\n",
            "Using blender in ./blender-3.3.1-linux-x64\n",
            "Using temporary directory: /dev/shm/blender_proc_d2aaa412802f43879ecf7948bb9f628e\n",
            "Blender 3.3.1 (hash b292cfe5a936 built 2022-10-05 00:14:35)\n",
            "Selecting render devices...\n",
            "Device Tesla T4 of type OPTIX found and used.\n",
            "Device Intel Xeon CPU @ 2.20GHz of type CPU found and used.\n",
            "bob: /content/Resources_DNN4VC_Synthetic/resources/lm, dataset_path: /content/Resources_DNN4VC_Synthetic/resources\n",
            "dataset: lm\n",
            "\n",
            "Successfully imported '/content/Resources_DNN4VC_Synthetic/resources/lm/models/obj_000002.ply' in 3.230 sec\n",
            "bob: /content/Resources_DNN4VC_Synthetic/resources/lm, dataset_path: /content/Resources_DNN4VC_Synthetic/resources\n",
            "dataset: lm\n",
            "\n",
            "Successfully imported '/content/Resources_DNN4VC_Synthetic/resources/lm/models/obj_000007.ply' in 1.605 sec\n",
            "\n",
            "Successfully imported '/content/Resources_DNN4VC_Synthetic/resources/lm/models/obj_000009.ply' in 0.898 sec\n",
            "\n",
            "Successfully imported '/content/Resources_DNN4VC_Synthetic/resources/lm/models/obj_000006.ply' in 1.666 sec\n",
            "\n",
            "Successfully imported '/content/Resources_DNN4VC_Synthetic/resources/lm/models/obj_000010.ply' in 1.934 sec\n",
            "\n",
            "Successfully imported '/content/Resources_DNN4VC_Synthetic/resources/lm/models/obj_000011.ply' in 0.738 sec\n",
            "\n",
            "Successfully imported '/content/Resources_DNN4VC_Synthetic/resources/lm/models/obj_000001.ply' in 0.567 sec\n",
            "Trying to put obj_000002\n",
            "/content/BlenderProc/blenderproc/python/sampler/UpperRegionSampler.py:73: RuntimeWarning: invalid value encountered in true_divide\n",
            "  normal /= np.linalg.norm(normal)\n",
            "Placed object \"obj_000002\" successfully at [0.33511844 0.39928222 0.12020516] after 1 iterations!\n",
            "Trying to put obj_000007\n",
            "Placed object \"obj_000007\" successfully at [0.11764727 0.05526701 0.04731131] after 1 iterations!\n",
            "Trying to put obj_000009\n",
            "Bad spacing after drop, retrying!\n",
            "Placed object \"obj_000009\" successfully at [-0.30006763  0.11867985  0.04284859] after 2 iterations!\n",
            "Trying to put obj_000006\n",
            "Placed object \"obj_000006\" successfully at [-0.29836762 -0.13558538  0.05872834] after 1 iterations!\n",
            "Trying to put obj_000007.001\n",
            "Bad spacing after drop, retrying!\n",
            "Bad spacing after drop, retrying!\n",
            "Bad spacing after drop, retrying!\n",
            "Bad spacing after drop, retrying!\n",
            "Placed object \"obj_000007.001\" successfully at [-0.07403912  0.30779722  0.04731119] after 5 iterations!\n",
            "Trying to put obj_000009.001\n",
            "Bad spacing after drop, retrying!\n",
            "Placed object \"obj_000009.001\" successfully at [0.37968615 0.06374115 0.04284859] after 2 iterations!\n",
            "Trying to put obj_000009.002\n",
            "Bad spacing after drop, retrying!\n",
            "Bad spacing after drop, retrying!\n",
            "Placed object \"obj_000009.002\" successfully at [ 0.20451398 -0.16794157  0.04284847] after 3 iterations!\n",
            "Trying to put obj_000010\n",
            "Bad spacing after drop, retrying!\n",
            "Bad spacing after drop, retrying!\n",
            "Bad spacing after drop, retrying!\n",
            "Bad spacing after drop, retrying!\n",
            "Bad spacing after drop, retrying!\n",
            "Bad spacing after drop, retrying!\n",
            "Bad spacing after drop, retrying!\n",
            "Bad spacing after drop, retrying!\n",
            "Bad spacing after drop, retrying!\n",
            "Bad spacing after drop, retrying!\n",
            "Bad spacing after drop, retrying!\n",
            "Bad spacing after drop, retrying!\n",
            "Bad spacing after drop, retrying!\n",
            "Placed object \"obj_000010\" successfully at [-0.2775898  -0.34709078  0.03462076] after 14 iterations!\n",
            "Trying to put obj_000011\n",
            "Bad spacing after drop, retrying!\n",
            "Bad spacing after drop, retrying!\n",
            "Placed object \"obj_000011\" successfully at [ 0.3385365  -0.36551493  0.0864079 ] after 3 iterations!\n",
            "Trying to put obj_000006.001\n",
            "Bad spacing after drop, retrying!\n",
            "Bad spacing after drop, retrying!\n",
            "Placed object \"obj_000006.001\" successfully at [-0.01119737 -0.39526817  0.05872822] after 3 iterations!\n",
            "Trying to put obj_000001\n",
            "Bad spacing after drop, retrying!\n",
            "Bad spacing after drop, retrying!\n",
            "Bad spacing after drop, retrying!\n",
            "Bad spacing after drop, retrying!\n",
            "Bad spacing after drop, retrying!\n",
            "Bad spacing after drop, retrying!\n",
            "Bad spacing after drop, retrying!\n",
            "Bad spacing after drop, retrying!\n",
            "Bad spacing after drop, retrying!\n",
            "Bad spacing after drop, retrying!\n",
            "Bad spacing after drop, retrying!\n",
            "Bad spacing after drop, retrying!\n",
            "Bad spacing after drop, retrying!\n",
            "Bad spacing after drop, retrying!\n",
            "Bad spacing after drop, retrying!\n",
            "Bad spacing after drop, retrying!\n",
            "Bad spacing after drop, retrying!\n",
            "Bad spacing after drop, retrying!\n",
            "Bad spacing after drop, retrying!\n",
            "Bad spacing after drop, retrying!\n",
            "Bad spacing after drop, retrying!\n",
            "Bad spacing after drop, retrying!\n",
            "Bad spacing after drop, retrying!\n",
            "Bad spacing after drop, retrying!\n",
            "Bad spacing after drop, retrying!\n",
            "Bad spacing after drop, retrying!\n",
            "Bad spacing after drop, retrying!\n",
            "Bad spacing after drop, retrying!\n",
            "Bad spacing after drop, retrying!\n",
            "Bad spacing after drop, retrying!\n",
            "Bad spacing after drop, retrying!\n",
            "Bad spacing after drop, retrying!\n",
            "Placed object \"obj_000001\" successfully at [-0.0618769  -0.15487574  0.04588461] after 33 iterations!\n",
            "Rendering 1 frames of colors, segmap...\n",
            "\u001b[2K\u001b[32mTotal\u001b[0m         \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[36m-:--:--\u001b[0m  \n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[32mTotal\u001b[0m         \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[36m-:--:--\u001b[0m  \n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[32mTotal\u001b[0m         \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[36m-:--:--\u001b[0m  \n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[32mTotal\u001b[0m         \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[36m-:--:--\u001b[0m  \n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[32mTotal\u001b[0m         \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[36m-:--:--\u001b[0m  \n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[32mTotal\u001b[0m         \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[36m-:--:--\u001b[0m  \n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[32mTotal\u001b[0m         \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[36m-:--:--\u001b[0m  \n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[32mTotal\u001b[0m         \u001b[90m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[36m-:--:--\u001b[0m Rendering frame 1 of 1               \n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[32mTotal\u001b[0m         \u001b[90m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[36m-:--:--\u001b[0m Rendering frame 1 of 1               \n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[32mTotal\u001b[0m         \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[36m-:--:--\u001b[0m Rendering frame 1 of 1           \n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[32mTotal\u001b[0m         \u001b[90m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[36m-:--:--\u001b[0m Rendering frame 1 of 1               \n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[32mTotal\u001b[0m         \u001b[90m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[36m-:--:--\u001b[0m Rendering frame 1 of 1               \n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[32mTotal\u001b[0m         \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[36m-:--:--\u001b[0m Rendering frame 1 of 1           \n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[32mTotal\u001b[0m         \u001b[90m━\u001b[0m \u001b[36m…\u001b[0m Rendering frame 1 of 1                                        \n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[32mTotal\u001b[0m         \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[36m-:--:--\u001b[0m Rendering frame 1 of 1\n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[32mTotal\u001b[0m         \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[36m-:--:--\u001b[0m Rendering frame 1 of 1\n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[32mTotal\u001b[0m         \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[36m-:--:--\u001b[0m Rendering frame 1 of 1           \n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[32mTotal\u001b[0m         \u001b[90m━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[36m-:--:--\u001b[0m Rendering frame 1 of 1              \n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[32mTotal\u001b[0m         \u001b[90m━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[36m-:--:--\u001b[0m Rendering frame 1 of 1              \n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[32mTotal\u001b[0m         \u001b[90m━━━━━━\u001b[0m \u001b[36m-:--:…\u001b[0m Rendering frame 1 of 1                              \n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[32mTotal\u001b[0m         \u001b[90m━━━━━━\u001b[0m \u001b[36m-:--:…\u001b[0m Rendering frame 1 of 1                              \n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[32mTotal\u001b[0m         \u001b[90m━━━━━━━━━━━━━━━━\u001b[0m \u001b[36m-:--:--\u001b[0m Rendering frame 1 of 1                   \n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[32mTotal\u001b[0m         \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[36m-:--:--\u001b[0m Rendering frame 1 of 1\n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[32mTotal\u001b[0m         \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[36m-:--:--\u001b[0m Rendering frame 1 of 1\n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[32mTotal\u001b[0m         \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[36m-:--:--\u001b[0m Rendering frame 1 of 1\n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[32mTotal\u001b[0m         \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[36m-:--:--\u001b[0m Rendering frame 1 of 1\n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[32mTotal\u001b[0m         \u001b[90m━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[36m-:--:--\u001b[0m Rendering frame 1 of 1              \n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[32mTotal\u001b[0m         \u001b[90m━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[36m-:--:--\u001b[0m Rendering frame 1 of 1              \n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[32mTotal\u001b[0m         \u001b[90m━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[36m-:--:--\u001b[0m Rendering frame 1 of 1              \n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[32mTotal\u001b[0m         \u001b[90m━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[36m-:--:--\u001b[0m Rendering frame 1 of 1              \n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[32mTotal\u001b[0m         \u001b[90m━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[36m-:--:--\u001b[0m Rendering frame 1 of 1              \n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[32mTotal\u001b[0m         \u001b[90m━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[36m-:--:--\u001b[0m Rendering frame 1 of 1              \n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[32mTotal\u001b[0m         \u001b[90m━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[36m-:--:--\u001b[0m Rendering frame 1 of 1              \n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[32mTotal\u001b[0m         \u001b[90m━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[36m-:--:--\u001b[0m Rendering frame 1 of 1              \n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[32mTotal\u001b[0m         \u001b[90m━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[36m-:--:--\u001b[0m Rendering frame 1 of 1              \n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[32mTotal\u001b[0m         \u001b[90m━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[36m-:--:--\u001b[0m Rendering frame 1 of 1              \n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[32mTotal\u001b[0m         \u001b[90m━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[36m-:--:--\u001b[0m Rendering frame 1 of 1              \n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[32mTotal\u001b[0m         \u001b[90m━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[36m-:--:--\u001b[0m Rendering frame 1 of 1              \n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[32mTotal\u001b[0m         \u001b[90m━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[36m-:--:--\u001b[0m Rendering frame 1 of 1              \n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[32mTotal\u001b[0m         \u001b[90m━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[36m-:--:--\u001b[0m Rendering frame 1 of 1              \n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[32mTotal\u001b[0m         \u001b[90m━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[36m-:--:--\u001b[0m Rendering frame 1 of 1              \n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[32mTotal\u001b[0m         \u001b[90m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[36m-:--:--\u001b[0m Rendering frame 1 of 1                 \n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[32mTotal\u001b[0m         \u001b[90m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[36m-:--:--\u001b[0m Rendering frame 1 of 1                 \n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[32mTotal\u001b[0m         \u001b[90m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[36m-:--:--\u001b[0m Rendering frame 1 of 1                 \n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[32mTotal\u001b[0m         \u001b[90m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[36m-:--:--\u001b[0m Rendering frame 1 of 1                 \n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[32mTotal\u001b[0m         \u001b[90m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[36m-:--:--\u001b[0m Rendering frame 1 of 1                 \n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[32mTotal\u001b[0m         \u001b[90m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[36m-:--:--\u001b[0m Rendering frame 1 of 1                 \n",
            "\u001b[33mCurrent frame\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[36m0:00:00\u001b[0m Compositing | De-initializing execution\n",
            "\u001b[1A\u001b[2K\u001b[1A\u001b[2KFinished rendering after 4.817 seconds\n",
            "dict_keys(['has_label_segmaps', 'instance_attribute_maps', 'colors'])\n",
            "\n",
            "Blender quit\n",
            "Cleaning temporary directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**IMPORTANT** When you are ready with code, make sure to save a copy, as the files will not be necessarily be saved when restarting the notebook."
      ],
      "metadata": {
        "id": "71XYU2-DYcvu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Final dataset creation:**\n",
        "7. Each execution of the scripts creates n samples. Use start_index and num samples and generate at least 256 images (16 calls with each 16 images). Make sure that the output is correct before generating the output. You could create a loop in python to automate everything\n",
        "`os.system(f\"blenderproc run ...\")`. Generating the dataset might take 10 minutes."
      ],
      "metadata": {
        "id": "o0VzsBUoXhxL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Generate the whole dataset (256 images) by calling the script 16 times for 16 samples each.\n",
        "Perhaps you find an elegant solution.\n",
        "'''"
      ],
      "metadata": {
        "id": "BeoPZEteXKDg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "BASE_PATH = '/content/Resources_DNN4VC_Synthetic/'\n",
        "num_samples = 16\n",
        "start_index = 1\n",
        "loops = 16\n",
        "blender_install_path = './'\n",
        "\n",
        "for i in range(loops):  # Replace 10 with the desired number of iterations\n",
        "    print(i, start_index)\n",
        "    # Set the environment variable\n",
        "    os.environ['START_INDEX'] = str(start_index)\n",
        "\n",
        "    # Execute the command\n",
        "    os.system(f\"blenderproc run {BASE_PATH}/scripts/dataset_generator_solution.py \\\n",
        "        --mesh_path {BASE_PATH}/resources/lm \\\n",
        "        --cc_textures_path {BASE_PATH}/resources/cctextures \\\n",
        "        --output_dir {BASE_PATH}/train_data \\\n",
        "        --num_samples {num_samples} \\\n",
        "        --start_index {start_index} \\\n",
        "        --blender-install-path {blender_install_path}\")\n",
        "\n",
        "    # Increment the start_index\n",
        "    start_index += num_samples"
      ],
      "metadata": {
        "id": "msCqtMrhC3rh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**IMPORTANT**: Download you generated dataset as a backup.First create a tar file and the download the file via the \"Files\" plane. Also download your edited scripts (if not done already)."
      ],
      "metadata": {
        "id": "3oKekYl9LVL3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!tar -czf /content/Resources_DNN4VC_Synthetic/train_data.tar.gz /content/Resources_DNN4VC_Synthetic/train_data"
      ],
      "metadata": {
        "id": "gacnJjFxLb86"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Part 2: Data augmentation\n",
        "\n",
        "Now, we are going to build a data loader, which performs data augmentation to create more variation in our training data."
      ],
      "metadata": {
        "id": "BgMazXXHLbOn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/Resources_DNN4VC_Synthetic/"
      ],
      "metadata": {
        "id": "h6SSjf9hPDAm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "First all packages, we will need later will be importet. This example will mainly use torch and torchvision."
      ],
      "metadata": {
        "id": "ZD15tTCwSh0N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import os\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms.functional as TF\n",
        "\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision.models.mobilenetv2 import InvertedResidual\n",
        "\n",
        "from utils.utils import plot_batch, plot_output\n",
        "from utils.imgaug_pipeline import heavy_augmentation\n",
        "\n"
      ],
      "metadata": {
        "id": "YcUVVBREPWNv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1 Preparing dataset and dataloader\n",
        "\n",
        "First a custom dataset is created.\n",
        "\n",
        "In the constructor a list of all files in the \"img\" dir is prepared.\n",
        "The `__getitem__` function will load a sample and transform the mask and the image with a **torchvision** transformation.\n",
        "\n",
        "It will then return a single sample constiting of an image and a mask."
      ],
      "metadata": {
        "id": "TCIISwWQTMeJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomDataset(Dataset):\n",
        "    def __init__(\n",
        "        self, root_dir, transform, transform_mask, geometric_augmentation=False, imgaug_heavy_augmentation=False\n",
        "    ):\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.transform_mask = transform_mask\n",
        "\n",
        "        self.img_dir = os.path.join(root_dir, \"img\")\n",
        "        self.mask_dir = os.path.join(root_dir, \"masks\")\n",
        "        self.img_filenames = sorted(os.listdir(self.img_dir))\n",
        "        self.geometric_augmentation = geometric_augmentation  # This will be used later\n",
        "        self.imgaug_heavy_augmentation = imgaug_heavy_augmentation # This will be used later\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_filenames)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_filename = self.img_filenames[idx]\n",
        "        img_path = os.path.join(self.img_dir, img_filename)\n",
        "        mask_path = os.path.join(self.mask_dir, img_filename)\n",
        "\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "        mask = Image.open(mask_path).convert(\"L\")\n",
        "\n",
        "        # This will be used later\n",
        "        if self.geometric_augmentation:\n",
        "            image, mask = self.common_transform(image, mask)\n",
        "\n",
        "        # This will be used later\n",
        "        if self.imgaug_heavy_augmentation:\n",
        "            image = self.apply_imgaug_pipeline(image)\n",
        "\n",
        "        image = self.transform(image)\n",
        "        mask = self.transform_mask(mask)\n",
        "\n",
        "\n",
        "        return image, mask\n",
        "\n",
        "    def common_transform(self, image, mask, rotation_angle=15, crop_offset=16, img_size=224):\n",
        "\n",
        "        '''\n",
        "        Add augmentation here and use the same parameters for image and mask\n",
        "        IMPORTANT: Use BILINEAR inperpolation when rotating, scaling the image and nearest neighbor interpolation when doing the same with the mask.\n",
        "        '''\n",
        "        # Random rotation (hint: random.uniform, TF.rotate)\n",
        "        # ...\n",
        "\n",
        "        # Random crop  (hint: transforms.RandomCrop.get_params, TF.crop)\n",
        "        # ...\n",
        "\n",
        "        # Resize to 224x224\n",
        "        # image = ...\n",
        "        # mask = ...\n",
        "\n",
        "        # Random horizontal flipping\n",
        "        #if random.random() > 0.5:\n",
        "        #    image = ...\n",
        "        #    mask = ...\n",
        "\n",
        "        # Random horizontal flipping\n",
        "        # ...\n",
        "        return image, mask\n",
        "\n",
        "    def apply_imgaug_pipeline(self, image):\n",
        "      image = np.array(image)\n",
        "      image = heavy_augmentation(image=image)\n",
        "\n",
        "      return Image.fromarray(image)\n",
        "\n"
      ],
      "metadata": {
        "id": "nhGAATCdRf8d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomDataset(Dataset):\n",
        "    def __init__(\n",
        "        self, root_dir, transform, transform_mask, geometric_augmentation=False, imgaug_heavy_augmentation=False\n",
        "    ):\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.transform_mask = transform_mask\n",
        "\n",
        "        self.img_dir = os.path.join(root_dir, \"img\")\n",
        "        self.mask_dir = os.path.join(root_dir, \"masks\")\n",
        "        self.img_filenames = sorted(os.listdir(self.img_dir))\n",
        "        self.geometric_augmentation = geometric_augmentation  # This will be used later\n",
        "        self.imgaug_heavy_augmentation = imgaug_heavy_augmentation # This will be used later\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_filenames)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_filename = self.img_filenames[idx]\n",
        "        img_path = os.path.join(self.img_dir, img_filename)\n",
        "        mask_path = os.path.join(self.mask_dir, img_filename)\n",
        "\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "        mask = Image.open(mask_path).convert(\"L\")\n",
        "\n",
        "        # This will be used later\n",
        "        if self.geometric_augmentation:\n",
        "            image, mask = self.common_transform(image, mask)\n",
        "\n",
        "        # This will be used later\n",
        "        if self.imgaug_heavy_augmentation:\n",
        "            image = self.apply_imgaug_pipeline(image)\n",
        "\n",
        "        image = self.transform(image)\n",
        "        mask = self.transform_mask(mask)\n",
        "\n",
        "\n",
        "        return image, mask\n",
        "\n",
        "    def common_transform(self, image, mask, rotation_angle=15, crop_offset=16, img_size=224):\n",
        "        # # Random rotation\n",
        "        angle = random.uniform(-15, 15)\n",
        "        image = TF.rotate(image, angle, interpolation=TF.InterpolationMode.BILINEAR)\n",
        "        mask = TF.rotate(mask, angle)\n",
        "\n",
        "        # Random crop\n",
        "        crop_size = 224 + random.randint(-16, 16)\n",
        "        i, j, h, w = transforms.RandomCrop.get_params(image, output_size=(crop_size, crop_size))\n",
        "        image = TF.crop(image, i, j, h, w)\n",
        "        mask = TF.crop(mask, i, j, h, w)\n",
        "\n",
        "        # Resize to 224x224\n",
        "        image = TF.resize(image, (224, 224), interpolation=TF.InterpolationMode.BILINEAR)\n",
        "        mask = TF.resize(mask, (224, 224))\n",
        "\n",
        "        # Random horizontal flipping\n",
        "        if random.random() > 0.5:\n",
        "            image = TF.hflip(image)\n",
        "            mask = TF.hflip(mask)\n",
        "\n",
        "        # # Random horizontal flipping\n",
        "        if random.random() > 0.5:\n",
        "            image = TF.vflip(image)\n",
        "            mask = TF.vflip(mask)\n",
        "\n",
        "        return image, mask\n",
        "\n",
        "    def apply_imgaug_pipeline(self, image):\n",
        "      image = np.array(image)\n",
        "      image = heavy_augmentation(image=image)\n",
        "\n",
        "      return Image.fromarray(image)\n"
      ],
      "metadata": {
        "id": "jBeHY5ZaYyBx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The transformation for image and mask are prepared for the test data. `transforms.Compose` creates a transformation pipeline from several operations applied one after another.\n",
        "\n",
        "The test dataset is created and a `DataLoader` is generated. It will load a batch of data (a group of samples) and return them stacked along the first dimension.\n",
        "\n",
        "You can create a test batch and display it with the following code. As you see, the files are sorted in the order of the sequence since `shuffle=False` which is common for test data.\n",
        "\n",
        "In this example, we crop all test images around the center. The input resolution of the network we will construct later is 224."
      ],
      "metadata": {
        "id": "jtwG9kiiUMW4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the paths to your train and test data folders\n",
        "train_data_path = \"train_data\"\n",
        "test_data_path = \"test_data\"\n",
        "\n",
        "# Define the transformations to be applied to the images and masks\n",
        "transform_img = transforms.Compose(\n",
        "    [\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "    ]\n",
        ")\n",
        "\n",
        "transform_mask = transforms.Compose(\n",
        "    [\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Create the testing dataset\n",
        "test_dataset = CustomDataset(\n",
        "    root_dir=test_data_path,\n",
        "    transform=transform_img,\n",
        "    transform_mask=transform_mask\n",
        ")\n",
        "\n",
        "# Define the batch size for the DataLoader\n",
        "batch_size = 16\n",
        "\n",
        "# Create the testing data loader\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Test the data loaders\n",
        "batch_images, batch_masks = next(iter(test_loader))\n",
        "plot_batch(batch_images, batch_masks)"
      ],
      "metadata": {
        "id": "jZx5-fFPQ5NK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2. Build an input pipeline with torchvision\n",
        "\n",
        "**Task 2a)** Crate a train_dataset.\n",
        "\n",
        "Create a input pipeline with simple augmentation and use [transforms.RandomApply](https://pytorch.org/vision/stable/generated/torchvision.transforms.RandomApply.html).\n",
        "\n",
        "The pipeline might use `transforms.ColorJitter`, `transforms.GaussianBlur` and `transforms.Grayscale`.\n",
        "\n",
        "`transforms.ColorJitter` should be applied for brightness, contrast, saturation and hue separately. For hue a very small strength e.g. 0.05 is sufficient. The strength of the other augmentations can be higher (e.g. 0.5).\n",
        "`transforms.GaussianBlur` should use a kernel size of 3.\n",
        "`transforms.Grayscale` should be applyed at the end of the pipeline with a low probabily (max 10%).\n"
      ],
      "metadata": {
        "id": "afBXWBszWNod"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transform_img_augmented = transforms.Compose(\n",
        "    [\n",
        "        transforms.CenterCrop(224),\n",
        "        #add more transforms here\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "    ]\n",
        ")\n",
        "\n",
        "transform_mask_augmented = transforms.Compose(\n",
        "    [\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "    ]\n",
        ")\n",
        "\n",
        "train_dataset = CustomDataset(\n",
        "    root_dir=train_data_path,\n",
        "    transform=transform_img_augmented,\n",
        "    transform_mask=transform_mask_augmented,\n",
        "    geometric_augmentation=False\n",
        ")\n",
        "\n",
        "# Create the training data loader\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Test the data loaders\n",
        "batch_images, batch_masks = next(iter(train_loader))\n",
        "plot_batch(batch_images, batch_masks)"
      ],
      "metadata": {
        "id": "vQ8q8S9EY8nv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform_img_augmented = transforms.Compose(\n",
        "    [\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.RandomApply([transforms.ColorJitter(brightness=0.5)], p=0.33),\n",
        "        transforms.RandomApply([transforms.ColorJitter(contrast=0.5)], p=0.33),\n",
        "        transforms.RandomApply([transforms.ColorJitter(saturation=0.5)], p=0.33),\n",
        "        transforms.RandomApply([transforms.ColorJitter(hue=0.1)], p=0.33),\n",
        "        transforms.RandomApply([transforms.GaussianBlur(kernel_size=3)], p=0.2),\n",
        "        transforms.RandomApply([transforms.Grayscale(num_output_channels=3)], p=0.1),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "    ]\n",
        ")\n",
        "\n",
        "transform_mask_augmented = transforms.Compose(\n",
        "    [\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "    ]\n",
        ")\n",
        "\n",
        "train_dataset = CustomDataset(\n",
        "    root_dir=train_data_path,\n",
        "    transform=transform_img_augmented,\n",
        "    transform_mask=transform_mask_augmented,\n",
        "    geometric_augmentation=True\n",
        ")\n",
        "\n",
        "# Create the training data loader\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Test the data loaders\n",
        "batch_images, batch_masks = next(iter(train_loader))\n",
        "plot_batch(batch_images, batch_masks)"
      ],
      "metadata": {
        "id": "kldOUbDyRI3F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2 Adding geometric augmentations for rgb image and mask\n",
        "\n",
        "More common augmentations include random flipping of the image, random cropping the image and random rotation.\n",
        "\n",
        "The reason we did not apply them in the pipeline is that we exactly the same crop, rotation for both the image and the mask to have consistent training data.\n",
        "\n",
        "So far, we used two separate pipelines and unfortunatley they do not share parameters.\n",
        "\n",
        "**Task 2b)** Implement the function `common_augmentation` in `CustomDataset(Dataset)` (above) to apply, random cropping, rescaling, random rotation and random fipping. You already find the function above, but it has to be finished.\n",
        "\n",
        "Adapt the codebox above to use the color as well as the geometric augmentations.\n",
        "\n",
        "**Implement the augmentations and visualize a batch.**"
      ],
      "metadata": {
        "id": "7N8xxkxsZ2D4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Augmentation with imgaug\n",
        "\n",
        "Augmentation does not end here and there are much more options.\n",
        "Currently not all of them are implemented in torchvision.\n",
        "\n",
        "An alternative is the [imgaug](https://github.com/aleju/imgaug#documentation) library.\n",
        "\n",
        "**Task 2c) (optional)**: Take a look at the prepared imgaug pipline in /content/Resources_DNN4VC_Synthetic/utils/imgaug_pipeline.py and see which operations are used there.\n",
        "\n",
        "Test the pipeline with the script below. Feel free to explore more combinations of augmentations by changing `imgaug_pipeline.py `."
      ],
      "metadata": {
        "id": "j7RyMPWhf4G0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transform_img_augmented = transforms.Compose(\n",
        "    [\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "    ]\n",
        ")\n",
        "\n",
        "transform_mask_augmented = transforms.Compose(\n",
        "    [\n",
        "        transforms.ToTensor(),\n",
        "    ]\n",
        ")\n",
        "\n",
        "train_dataset = CustomDataset(\n",
        "    root_dir=train_data_path,\n",
        "    transform=transform_img_augmented,\n",
        "    transform_mask=transform_mask_augmented,\n",
        "    geometric_augmentation=True,\n",
        "    imgaug_heavy_augmentation=True\n",
        ")\n",
        "\n",
        "# Create the training data loader\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Test the data loaders\n",
        "batch_images, batch_masks = next(iter(train_loader))\n",
        "plot_batch(batch_images, batch_masks)"
      ],
      "metadata": {
        "id": "3Bi0wkNnhnlN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 3 Training a segmentation model\n",
        "Now, after the training data and the augmentation is ready, we want to actually use the data for our initial task.\n",
        "Predicting a sementic segmentation for our test object.\n",
        "\n",
        "\n",
        "## 3.1 Building a decoder for a given backbone - Implementing a U-Net with MobilenetV2\n",
        "\n",
        "Our segmentation network should combine several concepts from the lecture. We will use [MobileNetV2](https://pytorch.org/hub/pytorch_vision_mobilenet_v2/) as a backbone and want to construct a U-Net-like decoder on top of it.\n",
        "\n",
        "**Task 3a)**: Finish the implementation by constructing the decoder.\n",
        "\n",
        "1. Inplement the `DecoderBlock` each decoder block should upsample the input features by a factor of two (in width and height), then concatenate the skip connection to the features and then pass the features through six more layers (2x conv layer, batchnorm layer, relu layer).\n",
        "2. Implement a `MobileNetV2Decoder` decoder with 4 decoder blocks using skip connections.\n",
        "  * After the 4 decoder blocks, perform an additional upsampling step.\n",
        "  * Apply a final 1x1 convolution to reduce the feature dimension to match the number of classes.\n",
        "3. Implement the `__init__` and `forward_pass` functions of `SegmentationModel`"
      ],
      "metadata": {
        "id": "09XNkNFNoOGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(DecoderBlock, self).__init__()\n",
        "\n",
        "        '''\n",
        "        init layers here  (Upsample, conv, bn, relu, conv, bn, relu)\n",
        "        '''\n",
        "        # ...\n",
        "\n",
        "    def forward(self, x, skip_connection):\n",
        "        '''\n",
        "        upsample first, then concat skip connection, then pass trough remaining layers\n",
        "        '''\n",
        "        # ...\n",
        "\n",
        "        return x\n",
        "\n",
        "class MobileNetV2Decoder(nn.Module):\n",
        "    def __init__(self, encoder_channels, num_classes):\n",
        "        super(MobileNetV2Decoder, self).__init__()\n",
        "\n",
        "        self.decoder_blocks = nn.ModuleList()\n",
        "\n",
        "        # add four decoder blocks to module list\n",
        "        #for i, encoder_out_channels in enumerate(encoder_channels[:-1]):\n",
        "            # input dim of a decoder block is the input dim of the previous block plus the input dim of the side input\n",
        "\n",
        "        # init final upsampling layer and final conv layer (kernel size=1)\n",
        "\n",
        "\n",
        "    def forward(self, x, skip_connections):\n",
        "\n",
        "        # pass through the decoder blocks and add the approporiate skip connections\n",
        "        # for i, decoder_block in enumerate(self.decoder_blocks):\n",
        "        #      ....\n",
        "\n",
        "        # upsample\n",
        "\n",
        "        # pass through final conv layer\n",
        "\n",
        "        return x\n",
        "\n",
        "class SegmentationModel(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(SegmentationModel, self).__init__()\n",
        "\n",
        "        # Load the pre-trained MobileNetV2 backbone\n",
        "        self.backbone = models.mobilenet_v2(weights=models.MobileNet_V2_Weights.DEFAULT).features\n",
        "        # Modify the MobileNetV2 backbone to remove the last two layers\n",
        "        self.backbone = nn.Sequential(*list(self.backbone.children())[:-2])\n",
        "\n",
        "        # Assuming we connect the backbone to the given blocks,\n",
        "        # we want to find out the out_channels of the respective layers to build the skip connections\n",
        "        self.connectors = [1, 3, 6, 10, 16]\n",
        "\n",
        "        '''\n",
        "        collect number of output channels from backbone\n",
        "        '''\n",
        "        encoder_channels = []\n",
        "        #for i, module in enumerate(self.backbone):\n",
        "            #if i in self.connectors:\n",
        "            #    ...\n",
        "\n",
        "        # Reverse the list to match with decoder blocks\n",
        "        ...\n",
        "        # init the MobileNetV2Decoder\n",
        "        ...\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Pass the input through the backbone\n",
        "        backbone_out = []\n",
        "\n",
        "        '''\n",
        "        apply the backbone module by module and store the side outputs\n",
        "        '''\n",
        "        #for i, module in enumerate(self.backbone):\n",
        "            #x = module(x)\n",
        "            #if i in self.connectors[:-1]:\n",
        "            #    ...\n",
        "        # Reverse the order of backbone_out to match with decoder blocks\n",
        "        # ...\n",
        "        # Pass the output of the backbone through the decoder\n",
        "        # ...\n",
        "\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "l-ReyAH_KcVD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(DecoderBlock, self).__init__()\n",
        "\n",
        "        self.upsample = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=False)\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.relu1 = nn.ReLU(inplace=True)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "        self.relu2 = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, x, skip_connection):\n",
        "        x = self.upsample(x)\n",
        "        x = torch.cat((x, skip_connection), dim=1)\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.bn2(x)\n",
        "        x = self.relu2(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class MobileNetV2Decoder(nn.Module):\n",
        "    def __init__(self, encoder_channels, num_classes):\n",
        "        super(MobileNetV2Decoder, self).__init__()\n",
        "\n",
        "        self.decoder_blocks = nn.ModuleList()\n",
        "\n",
        "        for i, encoder_out_channels in enumerate(encoder_channels[:-1]):\n",
        "            # input dim of a decoder block is the input dim of the input plus the input dim of the side input\n",
        "            decoder_in_channels = encoder_out_channels + encoder_channels[i + 1]\n",
        "            decoder_out_channels = encoder_channels[i + 1]\n",
        "            self.decoder_blocks.append(DecoderBlock(decoder_in_channels, decoder_out_channels))\n",
        "\n",
        "        self.upsample = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=False)\n",
        "        self.final_conv = nn.Conv2d(encoder_channels[-1], num_classes, kernel_size=1)\n",
        "\n",
        "    def forward(self, x, skip_connections):\n",
        "        for i, decoder_block in enumerate(self.decoder_blocks):\n",
        "            skip_connection = skip_connections[i]\n",
        "            x = decoder_block(x, skip_connection)\n",
        "        x = self.upsample(x)\n",
        "        x = self.final_conv(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class SegmentationModel(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(SegmentationModel, self).__init__()\n",
        "\n",
        "        # Load the pre-trained MobileNetV2 backbone\n",
        "        self.backbone = models.mobilenet_v2(weights=models.MobileNet_V2_Weights.DEFAULT).features\n",
        "        # Modify the MobileNetV2 backbone to remove the last two layers\n",
        "        self.backbone = nn.Sequential(*list(self.backbone.children())[:-2])\n",
        "\n",
        "        # Assuming each block consists of an InvertedResidual layer, we want to find out the out_channels of the respective layers to build the skip connections\n",
        "        self.connectors = [1, 3, 6, 10, 16]\n",
        "\n",
        "        encoder_channels = []\n",
        "        for i, module in enumerate(self.backbone):\n",
        "            if i in self.connectors:\n",
        "                encoder_channels.append(module.out_channels)\n",
        "        encoder_channels = encoder_channels[::-1]\n",
        "\n",
        "        # Define the decoder layers\n",
        "        self.decoder = MobileNetV2Decoder(encoder_channels, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Pass the input through the backbone\n",
        "        backbone_out = []\n",
        "\n",
        "        # apply the backbone module by module and store the side outputs\n",
        "        for i, module in enumerate(self.backbone):\n",
        "            x = module(x)\n",
        "            if i in self.connectors[:-1]:\n",
        "                backbone_out.append(x)\n",
        "\n",
        "        # Reverse the order of backbone_out to match with decoder blocks\n",
        "        backbone_out = backbone_out[::-1]\n",
        "        # Pass the output of the backbone through the decoder\n",
        "        x = self.decoder(x, backbone_out)\n",
        "\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "O5JR5hScomVt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.2 Implementing Test Metrics\n",
        "\n",
        "In the lecture different evaluation metrics have been presented to evaluate semantic segmentation.\n",
        "\n",
        "**Task 3b)** Implement the *F1 score* and the *Intersection over Union* metrics to evaluate the performance of the network on the validation/test data."
      ],
      "metadata": {
        "id": "wEK5VUy3pIpC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_iou(pred, target):\n",
        "    # pred and target are in binary format\n",
        "    # to-do: implement iou\n",
        "    # return batch average\n",
        "    return 0\n",
        "\n",
        "\n",
        "def compute_f1_score(pred, target):\n",
        "    # pred and target are in binary format\n",
        "    # to-do: implement f1\n",
        "    # return batch average\n",
        "    return 0"
      ],
      "metadata": {
        "id": "P6JUNPQgOXZT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_iou(pred, target):\n",
        "    intersection = torch.logical_and(pred, target)\n",
        "    union = torch.logical_or(pred, target)\n",
        "\n",
        "    iou_per_sample = torch.sum(intersection, dim=(1, 2)).float() / torch.sum(union, dim=(1, 2)).float()\n",
        "    average_iou = torch.mean(iou_per_sample)\n",
        "\n",
        "    return average_iou\n",
        "\n",
        "\n",
        "def compute_f1_score(pred, target):\n",
        "    true_positives = torch.logical_and(pred, target).sum(dim=(1, 2))\n",
        "    false_positives = (pred.logical_not() & target).sum(dim=(1, 2))\n",
        "    false_negatives = (pred & target.logical_not()).sum(dim=(1, 2))\n",
        "\n",
        "    precision = true_positives.float() / (true_positives + false_positives + 1e-7)\n",
        "    recall = true_positives.float() / (true_positives + false_negatives + 1e-7)\n",
        "\n",
        "    f1_score = 2 * (precision * recall) / (precision + recall + 1e-7)\n",
        "    average_f1_score = f1_score.mean()\n",
        "\n",
        "    return average_f1_score"
      ],
      "metadata": {
        "id": "tQHeLM8jpLj8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.3 Training the model\n",
        "\n",
        "In the following, the actual training and validation loop is already given.\n",
        "\n",
        "**Task 3c)** Train the model with our generated training data with and without augmentation. You can either use the heavy imgaug augmentation or the augmentation with torchvision augmentation.\n",
        "\n",
        "The network will plot a graph of test and train losses as well as the test metrics. Output is saved in /content/Resources_DNN4VC_Synthetic/output.\n",
        "Also the estimated masks are saved.\n",
        "\n",
        "Compare the results. You will see that regarding the metrics the effect of the augmentation is rather limited in our particular case.\n",
        "\n",
        "How would you interpret the results?\n"
      ],
      "metadata": {
        "id": "7Sw92X7ioo_6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output_interval = 5\n",
        "learning_rate = 0.001\n",
        "num_classes = 2\n",
        "num_epochs = 30\n",
        "\n",
        "random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "\n",
        "output_dir = \"output/iteration_output\"  # Directory to save iteration outputs\n",
        "\n",
        "# Create the output directory if it doesn't exist\n",
        "if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)\n",
        "\n",
        "# Create an instance of the SegmentationModel\n",
        "model = SegmentationModel(num_classes)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Move the model to the appropriate device (e.g., GPU if available)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Training loop\n",
        "train_losses = []\n",
        "test_losses = []\n",
        "miou_scores = []\n",
        "mf1_scores = []\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss = 0.0\n",
        "    val_loss = 0.0\n",
        "    total_iou = 0.0\n",
        "    total_f1 = 0.0\n",
        "    num_samples = 0\n",
        "\n",
        "    model.train()  # Set the model to training mode\n",
        "\n",
        "    # Initialize the progress bar\n",
        "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\", ncols=80)\n",
        "\n",
        "    for img, mask in pbar:\n",
        "        # Move the data to the appropriate device\n",
        "        img = img.to(device)\n",
        "        mask = mask.squeeze(1).long().to(device)\n",
        "        # Zero the gradients\n",
        "        optimizer.zero_grad()\n",
        "        # Forward pass\n",
        "        output = model(img)\n",
        "        # Compute the loss\n",
        "        loss = criterion(output, mask)\n",
        "        # Backward pass and optimization\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        # Update the progress bar description with the current loss\n",
        "        pbar.set_postfix({\"Loss\": loss.item()})\n",
        "        train_loss += loss.item()\n",
        "\n",
        "    train_loss /= len(train_loader)\n",
        "    train_losses.append(train_loss)\n",
        "\n",
        "    # Print epoch summary\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs} | Training Loss: {train_loss}\")\n",
        "\n",
        "    # Validation loop\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    iteration = 0  # Initialize the iteration counter\n",
        "    with torch.no_grad():\n",
        "        for val_img, val_mask in test_loader:\n",
        "            val_img = val_img.to(device)\n",
        "            val_mask = val_mask.squeeze(1).long().to(device)\n",
        "            val_output = model(val_img)\n",
        "            val_loss += criterion(val_output, val_mask).item()\n",
        "\n",
        "            # Compute IoU\n",
        "            val_pred = torch.argmax(val_output, dim=1)\n",
        "\n",
        "            iou = compute_iou(val_pred, val_mask)  # Function to calculate IoU\n",
        "\n",
        "            total_iou += iou.sum().item() * val_img.size(0)\n",
        "\n",
        "            f1 = compute_f1_score(val_pred, val_mask)  # Function to calculate IoU\n",
        "\n",
        "            total_f1 += f1.sum().item() * val_img.size(0)\n",
        "\n",
        "            num_samples += val_img.size(0)\n",
        "\n",
        "            if (epoch + 1) % output_interval == 0:\n",
        "                output_filename = f\"iteration_{iteration}.png\"\n",
        "                output_path = os.path.join(output_dir, output_filename)\n",
        "                plot_batch(val_img, val_pred.float().unsqueeze(1), output_path)\n",
        "\n",
        "            iteration += 1\n",
        "\n",
        "        val_loss /= len(test_loader)\n",
        "        mean_iou = total_iou / num_samples\n",
        "        mean_f1 = total_f1 / num_samples\n",
        "\n",
        "        test_losses.append(val_loss)\n",
        "        miou_scores.append(mean_iou)\n",
        "        mf1_scores.append(mean_f1)\n",
        "\n",
        "        plot_output(train_losses, test_losses, miou_scores, mf1_scores, epoch, output_dir)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs} | Validation Loss: {val_loss} | mIoU: {mean_iou} | f1: {mean_f1}\")\n"
      ],
      "metadata": {
        "id": "SJSSBHdgopp_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**IMPORTANT**: Download you generated outputs.First create a tar file and the download the file via the \"Files\" plane."
      ],
      "metadata": {
        "id": "KcyluZXDtXtC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!tar -czf /content/Resources_DNN4VC_Synthetic/output.tar.gz /content/Resources_DNN4VC_Synthetic/output"
      ],
      "metadata": {
        "id": "Unvs1EactaBc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusion\n",
        "\n",
        "In this exercise, we gained knowledge and practical experience in generating synthetic training data that closely resembles real-world images, which we utilized to train a semantic segmentation network. Additionally, we explored different methods of data augmentation implementation.\n",
        "\n",
        "What do you think, in which other cases could we make use of synthetic training data?\n",
        "In which cases do you think is data augmentation especially important?\n",
        "\n",
        "We will discuss these questions during the exercise. I hope that you can use the gained insights in your future vision projects. Please let me know if you had any troubles.\n",
        "\n",
        "**Please remember to hand in all the requested files (see Introduction).**\n",
        "\n"
      ],
      "metadata": {
        "id": "qHKYQxSVPUFb"
      }
    }
  ]
}