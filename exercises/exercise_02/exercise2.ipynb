{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9eRCDI4iOdus",
      "metadata": {
        "id": "9eRCDI4iOdus"
      },
      "outputs": [],
      "source": [
        "%pip install --upgrade plotly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "1498e4b9-7b8b-4534-88a1-792d31834c07",
      "metadata": {
        "id": "1498e4b9-7b8b-4534-88a1-792d31834c07"
      },
      "outputs": [],
      "source": [
        "import plotly\n",
        "import plotly.graph_objects as go\n",
        "from plotly.offline import init_notebook_mode, iplot\n",
        "import torch\n",
        "from torch import nn, Tensor\n",
        "from torch.optim import Optimizer\n",
        "from torch.utils.data import DataLoader, Subset, random_split\n",
        "\n",
        "from torchvision import datasets\n",
        "from torchvision.utils import make_grid\n",
        "import torchvision.transforms as T\n",
        "\n",
        "from copy import deepcopy\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from plot_loss import get_loss_grid, plot_contour, plot_surface, plot_losses, get_state_directions\n",
        "from convert_net import get_conv, get_lin\n",
        "from utils import get_batch, plot_examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "HHKu_Ys8PgRG",
      "metadata": {
        "id": "HHKu_Ys8PgRG"
      },
      "outputs": [],
      "source": [
        "def conf_pltly():\n",
        "    import IPython\n",
        "    display(IPython.core.display.HTML(\"\"\"\n",
        "      <script src=\"/static/components/requirejs/require.js\"></script>\n",
        "      <script>\n",
        "        requirejs.config({\n",
        "          paths: '/static/base',\n",
        "          plotly: 'https://cds.plot.ly/plotly-2.23.2.min.js?noext'\n",
        "        });\n",
        "      </script>\n",
        "    \"\"\"))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "de03a78e-c2a9-48d0-8b7a-43d2afab9ac4",
      "metadata": {
        "id": "de03a78e-c2a9-48d0-8b7a-43d2afab9ac4"
      },
      "source": [
        "# Optimizing Neural Networks\n",
        "\n",
        "In this exercise, you will learn how to implement optimizers such as Gradient Descent to use in PyTorch. You will evaluate the impact of learning rates and different parameter update strategies. For this you will use the dataset from previous exercise and improve the convergence of a simple classifier."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "049694f6-39f9-4dac-a01c-e2cec60b9a07",
      "metadata": {
        "id": "049694f6-39f9-4dac-a01c-e2cec60b9a07",
        "tags": []
      },
      "source": [
        "## Prepare Dataset\n",
        "\n",
        "We first define useful functions and data loaders. For this, we need a pipeline to convert the data samples into Tensors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "83a7e8d5-0bcf-42de-b0bb-b47ccbfc7c85",
      "metadata": {
        "id": "83a7e8d5-0bcf-42de-b0bb-b47ccbfc7c85"
      },
      "outputs": [],
      "source": [
        "transforms = T.Compose([\n",
        "    T.ToTensor(),\n",
        "    T.Normalize(0.5, 0.5)\n",
        "])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "23f465fa-a1ae-4163-8750-f57f58e7e79d",
      "metadata": {
        "id": "23f465fa-a1ae-4163-8750-f57f58e7e79d"
      },
      "source": [
        "We then load the data and create a small subset of the dataset to speed up tests of the implemented optimizers. Look at the [pytorch](https://pytorch.org/docs/stable/data.html) documentation and create\n",
        "\n",
        "- a small subset of 50 batches with size 64\n",
        "- two dataloaders that iterate over all samples and batched samples, respectively"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "be9957f2-4a20-4e28-9768-321d946082de",
      "metadata": {
        "id": "be9957f2-4a20-4e28-9768-321d946082de"
      },
      "outputs": [],
      "source": [
        "batch_size = 64\n",
        "batches = 50\n",
        "\n",
        "data = datasets.MNIST(root=\"./\", transform=transforms, target_transform=None, download=True)\n",
        "data = Subset(...)\n",
        "\n",
        "gd_data_loader = DataLoader(...)\n",
        "data_loader = DataLoader(...)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "278e3827-9b6c-4a90-bfdf-9f2ed5016ead",
      "metadata": {
        "id": "278e3827-9b6c-4a90-bfdf-9f2ed5016ead"
      },
      "source": [
        "You can use `plot_examples()` to visualize some examples of the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e2767f24-10b3-49a9-89c5-b0ed18e5bf5f",
      "metadata": {
        "id": "e2767f24-10b3-49a9-89c5-b0ed18e5bf5f"
      },
      "outputs": [],
      "source": [
        "plot_examples(data)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "f6a906ce-b06f-4fc3-bb76-6a3c9935f204",
      "metadata": {
        "id": "f6a906ce-b06f-4fc3-bb76-6a3c9935f204"
      },
      "source": [
        "## Optimizer\n",
        "\n",
        "In this section we will compare different optimizer that were introduced to improve the convergence behavior of neural networks. The nice thing about PyTorch is that all gradients are computed automatically when using the implemented backward pass and saved for Tensor `x` in `x.grad`. Therefore, we first implement the network used in previous exercise using the PyTorch implementations of:\n",
        "\n",
        "- nn.Sequential\n",
        "- nn.Flatten\n",
        "- nn.Linear\n",
        "\n",
        "Transfer the net to the GPU before returning it, to use all available compute."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "0210adc9-d43f-49df-95ef-3ed62dd6563d",
      "metadata": {
        "id": "0210adc9-d43f-49df-95ef-3ed62dd6563d"
      },
      "outputs": [],
      "source": [
        "def get_OneFCNet():\n",
        "    return ..."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "2aeac1fe-6b38-49fd-a802-7633e92637de",
      "metadata": {
        "id": "2aeac1fe-6b38-49fd-a802-7633e92637de"
      },
      "source": [
        "## Gradient Descent\n",
        "\n",
        "Now that we have a network to optimize, let's have a look at the optimizer class of PyTorch. To implement an optimizer, we need to implement the `step()` function to actually update the parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "323e8d4b-a29f-444f-bc3a-141edb90f03d",
      "metadata": {
        "id": "323e8d4b-a29f-444f-bc3a-141edb90f03d"
      },
      "outputs": [],
      "source": [
        "class GD(Optimizer):\n",
        "    \n",
        "    def __init__(self, params, lr=0.2) -> None:\n",
        "        super().__init__(params, {'lr': lr})\n",
        "        self.lr = lr\n",
        "    \n",
        "    def step(self):\n",
        "        # do not consider the next steps for gradient calculations\n",
        "        with torch.no_grad():\n",
        "            \n",
        "            # iter over all parameters\n",
        "            for p in self.param_groups[0]['params']:\n",
        "                \n",
        "                # if the gradient is set, update it\n",
        "                if p.grad is not None:\n",
        "                    \n",
        "                    # update parameters\n",
        "                    # hint: in torch each function (e.g. Tensor.add()) has an inplace variant\n",
        "                    # which modifies the tensor inplace: Tensor.add_()\n",
        "                    ..."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "ea7c932b-8487-4a12-8932-497b39c3d1bf",
      "metadata": {
        "id": "ea7c932b-8487-4a12-8932-497b39c3d1bf"
      },
      "source": [
        "Let's define a training step by defining the `loss_fn` globally and implementing a step with the following substeps:\n",
        "\n",
        "- all gradients must be reseted by `optimizer.zero_grad()`\n",
        "- get the result of a forward pass of the network\n",
        "- calculate the loss for this batch\n",
        "- do a backwardpass using `.backward()` on the calculated loss\n",
        "- do an optimizer step"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "1f647f68-0df2-418c-8857-eecdb69ba755",
      "metadata": {
        "id": "1f647f68-0df2-418c-8857-eecdb69ba755"
      },
      "outputs": [],
      "source": [
        "def training_step(net, optimizer, loss_fn, batch):\n",
        "    img, gt = batch\n",
        "    \n",
        "    # implement training step\n",
        "    ...\n",
        "    \n",
        "    return loss"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "2ec7f1dc-9892-4719-84e5-51b190b84922",
      "metadata": {
        "id": "2ec7f1dc-9892-4719-84e5-51b190b84922"
      },
      "source": [
        "To do multiple steps, we implement a function `the_loop` that iterates over a dataloader. It should do a training step per batch for `epochs`. After one epoch, the loss on the validation set should be calculated."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "455de34d-53b2-4fe2-b303-45396f5136c7",
      "metadata": {
        "id": "455de34d-53b2-4fe2-b303-45396f5136c7"
      },
      "outputs": [],
      "source": [
        "def the_loop(net, optimizer, train_loader, val_loader=None, epochs=None, swa_model=None, swa_start=5):\n",
        "    if epochs is None:\n",
        "        raise Exception(\"a training duration must be given: set epochs\")\n",
        "    \n",
        "    log_iterval = 1\n",
        "    running_mean = 0.\n",
        "    loss = Tensor([0.]).cuda()\n",
        "\n",
        "    losses = []\n",
        "    val_losses = []\n",
        "    states = []\n",
        "    i, j = 0, 0\n",
        "    \n",
        "    pbar = tqdm(train_loader, desc=f\"epoch {i}\", postfix={\"loss\": loss.item(), \"step\": j})\n",
        "\n",
        "    for i in range(epochs):\n",
        "        running_mean = 0.\n",
        "        j = 0\n",
        "\n",
        "        pbar.set_description(f\"epoch {i}\")\n",
        "        pbar.refresh()\n",
        "        pbar.reset()\n",
        "        for j, batch in enumerate(train_loader):\n",
        "\n",
        "            # implement training step by \n",
        "            # - appending the current states to `states`\n",
        "            # - doing a training_step\n",
        "            # - appending the current loss to the `losses` list\n",
        "            # - update the running_mean for logging\n",
        "\n",
        "            ...\n",
        "\n",
        "            if j % log_iterval == 0 and j != 0:\n",
        "                pbar.set_postfix({\"loss\": running_mean.item(), \"step\": j})\n",
        "                running_mean = 0.\n",
        "            pbar.update()\n",
        "        \n",
        "        if i > swa_start and swa_model is not None:\n",
        "            swa_model.update_parameters(net)\n",
        "        \n",
        "        if val_loader is not None:\n",
        "            \n",
        "            # evaluate the current net on the validation data loader and\n",
        "            # collect all losses in the ´val_loss´ list\n",
        "           \n",
        "            ...\n",
        "\n",
        "    pbar.refresh()\n",
        "    \n",
        "    if val_loader is not None:\n",
        "        return losses, states, val_losses\n",
        "    \n",
        "    return losses, states"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "c8726d5a-df13-4405-bb99-afdb4de6481a",
      "metadata": {
        "id": "c8726d5a-df13-4405-bb99-afdb4de6481a"
      },
      "source": [
        "Now train a OneFCNet using your Gradient Descent optimizer, the data loader which iterates over all samples in one batch with a Cross Entropy loss (hint: there is an implementation of PyTorch for this loss). For testing the optimizers we are not yet interested in the validation loss. So no need to provide a validation loader."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "69f33556-a226-4870-b580-b5bd59fa5fce",
      "metadata": {
        "id": "69f33556-a226-4870-b580-b5bd59fa5fce"
      },
      "outputs": [],
      "source": [
        "net = ...\n",
        "epochs = 10\n",
        "optimizer = GD(net.parameters(), 0.002)\n",
        "loss_fn = ...\n",
        "\n",
        "losses, states = ...\n",
        "\n",
        "fig = plot_losses(losses)\n",
        "iplot(fig)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "4150cb8c-58f5-4742-83d5-89794a447ce4",
      "metadata": {
        "id": "4150cb8c-58f5-4742-83d5-89794a447ce4"
      },
      "source": [
        "We can now use a method to plot the loss surface of the network by projecting the parameter updates into two dimensions. You can find more information on that [here](https://proceedings.neurips.cc/paper_files/paper/2018/hash/a41b3bb3e6b050b6c9067c67f663b915-Abstract.html). But you can just use the provided code. The contour plot will show how the loss will change if you would follow the two main directions of the past parameter updates.\n",
        "\n",
        "Think about the challenges and the optimization process of this landscape. What could impede the convergence of the net?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "58b2b3c5-1db5-4d2c-b992-acf354ca908b",
      "metadata": {
        "id": "58b2b3c5-1db5-4d2c-b992-acf354ca908b"
      },
      "outputs": [],
      "source": [
        "# project states onto the main directions of the gradient updates using n samples over all steps starting from sample x\n",
        "# the directions are calculated using the last sample as a reference\n",
        "directions, state_ids, loss_coordinates = get_state_directions(states, n_states=10, start_from=0, reference_id=-1)\n",
        "\n",
        "# compute the losses over the main directions of the gradient updates\n",
        "x, y, Z, _ = get_loss_grid(net, data_loader, loss_fn, directions=directions, resolution=(20, 20), scale=loss_coordinates.abs().max().item())\n",
        "\n",
        "# plot the landscape as a contour plot\n",
        "fig = plot_contour(np.copy(x), np.copy(y), np.copy(Z), scale=True)\n",
        "fig.add_traces(go.Scatter(x=np.copy(loss_coordinates[0].cpu().numpy()),\n",
        "                          y=np.copy(loss_coordinates[1].cpu().numpy())))\n",
        "\n",
        "print('loss samples:', np.array(losses)[state_ids])\n",
        "\n",
        "conf_pltly()\n",
        "init_notebook_mode(connected=False)\n",
        "iplot(fig)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "44a5d4ee-a031-4cc0-8d81-9aed5fc7f002",
      "metadata": {
        "id": "44a5d4ee-a031-4cc0-8d81-9aed5fc7f002"
      },
      "source": [
        "### Garment Classifier\n",
        "\n",
        "Below there is a net given, that start to use convolutions to consider the advantages of the spatial structure of images. Test the gradient descent method on this net and compare it.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9527ac35-d358-42f8-b2f5-645cbb7bacbd",
      "metadata": {
        "id": "9527ac35-d358-42f8-b2f5-645cbb7bacbd"
      },
      "outputs": [],
      "source": [
        "def get_GarmentClassifier():\n",
        "    return nn.Sequential(\n",
        "            nn.Conv2d(1, 6, kernel_size=5),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2,2),\n",
        "            nn.Conv2d(6, 16, kernel_size=5),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2,2),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(16 * 4 * 4, 120),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(120, 84),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(84, 10)\n",
        "        ).cuda()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a50202af-d62a-43d4-91e6-12d4d1ae9dc3",
      "metadata": {
        "id": "a50202af-d62a-43d4-91e6-12d4d1ae9dc3"
      },
      "outputs": [],
      "source": [
        "net = ...\n",
        "epochs = 10\n",
        "optimizer = GD(net.parameters(), 0.02)\n",
        "\n",
        "losses, states = ...\n",
        "\n",
        "fig = plot_losses(losses)\n",
        "conf_pltly()\n",
        "init_notebook_mode(connected=False)\n",
        "iplot(fig)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "27f73f16-2685-468d-8d6e-082062db45cf",
      "metadata": {
        "id": "27f73f16-2685-468d-8d6e-082062db45cf"
      },
      "outputs": [],
      "source": [
        "# project states onto the main directions of the gradient updates using n samples over all steps starting from sample x\n",
        "# the directions are calculated using the last sample as a reference\n",
        "directions, state_ids, loss_coordinates = get_state_directions(states, n_states=10, start_from=0, reference_id=-1)\n",
        "\n",
        "# compute the losses over the main directions of the gradient updates\n",
        "x, y, Z, _ = get_loss_grid(net, data_loader, loss_fn, directions=directions, resolution=(20, 20), scale=loss_coordinates.abs().max().item())\n",
        "\n",
        "# plot the landscape as a contour plot\n",
        "fig = plot_contour(np.copy(x), np.copy(y), np.copy(Z), scale=True)\n",
        "fig.add_traces(go.Scatter(x=np.copy(loss_coordinates[0].cpu().numpy()),\n",
        "                          y=np.copy(loss_coordinates[1].cpu().numpy())))\n",
        "\n",
        "print('loss samples:', np.array(losses)[state_ids])\n",
        "\n",
        "conf_pltly()\n",
        "init_notebook_mode(connected=False)\n",
        "iplot(fig)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "e802c14d-5a45-46ba-9c60-b4e738ca2c05",
      "metadata": {
        "id": "e802c14d-5a45-46ba-9c60-b4e738ca2c05"
      },
      "source": [
        "What are the differences of the contour plots and convergence behaviors?"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "c7570963-a5e9-4068-a925-833327c6884c",
      "metadata": {
        "id": "c7570963-a5e9-4068-a925-833327c6884c"
      },
      "source": [
        "## Stochastic Gradient Descent\n",
        "\n",
        "For Gradient Descent, we calculated the gradients for each sample individually. To improve the convergence behavior and to speed up the training process, we now consider batches of gradients. This stabilizes the convergence and prevents the parameters from being push in one direction and directly afterwards being pulled in the opposite direction. Therefore, we can use the gradient descent implementation and just swap the data loader to use the batched variant. The bad thing is that we need to iterate now multiple times over the dataset.\n",
        "\n",
        "Is there any difference?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a7181df4-1e07-4072-8b55-9f7d6012e09c",
      "metadata": {
        "id": "a7181df4-1e07-4072-8b55-9f7d6012e09c"
      },
      "outputs": [],
      "source": [
        "net = ...\n",
        "epochs = 10\n",
        "optimizer = GD(net.parameters(), 0.02)\n",
        "\n",
        "losses, states = ...\n",
        "\n",
        "fig = plot_losses(losses)\n",
        "conf_pltly()\n",
        "init_notebook_mode(connected=False)\n",
        "iplot(fig)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "05de6d75-ca1f-4fea-a4cb-28b4acd370b0",
      "metadata": {
        "id": "05de6d75-ca1f-4fea-a4cb-28b4acd370b0"
      },
      "outputs": [],
      "source": [
        "# project states onto the main directions of the gradient updates using n samples over all steps starting from sample x\n",
        "# the directions are calculated using the last sample as a reference\n",
        "directions, state_ids, loss_coordinates = get_state_directions(states, n_states=10, start_from=400, reference_id=-1)\n",
        "\n",
        "# compute the losses over the main directions of the gradient updates\n",
        "x, y, Z, _ = get_loss_grid(net, data_loader, loss_fn, directions=directions, resolution=(20, 20), scale=loss_coordinates.abs().max().item())\n",
        "\n",
        "# plot the landscape as a contour plot\n",
        "fig = plot_contour(np.copy(x), np.copy(y), np.copy(Z), scale=True)\n",
        "fig.add_traces(go.Scatter(x=np.copy(loss_coordinates[0].cpu().numpy()),\n",
        "                          y=np.copy(loss_coordinates[1].cpu().numpy())))\n",
        "\n",
        "print('loss samples:', np.array(losses)[state_ids])\n",
        "\n",
        "conf_pltly()\n",
        "init_notebook_mode(connected=False)\n",
        "iplot(fig)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "7d0b876b-7a0b-4bf4-b006-f724ac5fdd63",
      "metadata": {
        "id": "7d0b876b-7a0b-4bf4-b006-f724ac5fdd63"
      },
      "source": [
        "### Stochastic Gradient Descent with Momentum\n",
        "\n",
        "A next improvement to our optimizer is inspired by the physical momentum. A typical problem for gradient descent in complex optimization functions is, that one could end in a local minima. Therefore, we not only consider the current gradient but also the last update weighted by a momentum factor:\n",
        "\n",
        "$$ v = \\eta \\cdot ( -\\frac{\\partial L(\\theta)}{\\partial \\theta} ) + v^\\text{old} \\cdot mtm $$\n",
        "\n",
        "with $\\eta$ being the learning rate, $\\theta$ denoting the network parameters and $L(\\theta)$ being the current loss of with the current set of parameters. The update is given by:\n",
        "\n",
        "$$ \\theta = \\theta + v $$\n",
        "\n",
        "Now use the implementation of Stochastic Gradient Descent and add momemtum to it. Again, train a network and see if you can spot any differences to the previous optimization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3bb34d63-4377-4164-96da-694823b8d646",
      "metadata": {
        "id": "3bb34d63-4377-4164-96da-694823b8d646"
      },
      "outputs": [],
      "source": [
        "class SGDwM(Optimizer):\n",
        "    \n",
        "    def __init__(self, params, lr=0.2, momentum=0.9) -> None:\n",
        "        super().__init__(params, {'lr': lr})\n",
        "        self.lr = lr\n",
        "        \n",
        "        # maybe you need some more code here\n",
        "    \n",
        "    def step(self):\n",
        "        with torch.no_grad():\n",
        "            for i, p in enumerate(self.param_groups[0]['params']):\n",
        "                if p.grad is not None:\n",
        "                    \n",
        "                    # update parameters\n",
        "                    ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b8fdd15-da1a-4755-b0b3-a2ab025cfdd7",
      "metadata": {
        "id": "5b8fdd15-da1a-4755-b0b3-a2ab025cfdd7"
      },
      "outputs": [],
      "source": [
        "net = get_GarmentClassifier()\n",
        "epochs = 10\n",
        "optimizer = SGDwM(net.parameters(), 0.02)\n",
        "\n",
        "losses, states = the_loop(net, optimizer, data_loader, epochs=epochs)\n",
        "\n",
        "fig = plot_losses(losses)\n",
        "conf_pltly()\n",
        "init_notebook_mode(connected=False)\n",
        "iplot(fig)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b5ebb9a3-8760-4aa5-83f0-fd2b6c2872bf",
      "metadata": {
        "id": "b5ebb9a3-8760-4aa5-83f0-fd2b6c2872bf"
      },
      "outputs": [],
      "source": [
        "# project states onto the main directions of the gradient updates using n samples over all steps starting from sample x\n",
        "# the directions are calculated using the last sample as a reference\n",
        "directions, state_ids, loss_coordinates = get_state_directions(states, n_states=10, start_from=400, reference_id=-1)\n",
        "\n",
        "# compute the losses over the main directions of the gradient updates\n",
        "x, y, Z, _ = get_loss_grid(net, data_loader, loss_fn, directions=directions, resolution=(20, 20), scale=loss_coordinates.abs().max().item())\n",
        "\n",
        "# plot the landscape as a contour plot\n",
        "fig = plot_contour(np.copy(x), np.copy(y), np.copy(Z), scale=True)\n",
        "fig.add_traces(go.Scatter(x=np.copy(loss_coordinates[0].cpu().numpy()),\n",
        "                          y=np.copy(loss_coordinates[1].cpu().numpy())))\n",
        "\n",
        "print('loss samples:', np.array(losses)[state_ids])\n",
        "\n",
        "conf_pltly()\n",
        "init_notebook_mode(connected=False)\n",
        "iplot(fig)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "9fb7231c-1109-4216-9d02-e845d09421e6",
      "metadata": {
        "id": "9fb7231c-1109-4216-9d02-e845d09421e6"
      },
      "source": [
        "### Nesterov Accelerated Gradient Descent\n",
        "\n",
        "A variant of Stochastic Gradient Descent also considers the penultimate update to include more statistics for optimization:\n",
        "\n",
        "$$ \\tilde{\\theta} = \\theta + v^\\text{old} \\cdot mtm $$\n",
        "$$ v = v^\\text{old} \\cdot mtm + \\eta \\cdot (-\\frac{\\partial L(\\tilde{\\theta})}{\\partial \\theta}) $$\n",
        "$$ \\theta = \\tilde{\\theta} $$\n",
        "\n",
        "Are there differences to previous methods and why do they exists?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a439125c-5a1e-4ed1-9d13-637fb11f8e4f",
      "metadata": {
        "id": "a439125c-5a1e-4ed1-9d13-637fb11f8e4f"
      },
      "outputs": [],
      "source": [
        "class NAGD(Optimizer):\n",
        "    \n",
        "    def __init__(self, params, lr=0.2, momentum=0.9) -> None:\n",
        "        super().__init__(params, {'lr': lr, 'momentum': momentum})\n",
        "        self.lr = lr\n",
        "        \n",
        "        # maybe you need some more code here\n",
        "    \n",
        "    def step(self):\n",
        "        with torch.no_grad():\n",
        "            for i, p in enumerate(self.param_groups[0]['params']):\n",
        "                if p.grad is not None:\n",
        "                    \n",
        "                    # update parameters\n",
        "                    ...\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb6d92ba-ad5e-437f-970e-68604602a796",
      "metadata": {
        "id": "bb6d92ba-ad5e-437f-970e-68604602a796"
      },
      "outputs": [],
      "source": [
        "net = get_GarmentClassifier()\n",
        "epochs = 10\n",
        "optimizer = NAGD(net.parameters(), 0.02)\n",
        "\n",
        "losses, states = the_loop(net, optimizer, data_loader, epochs=epochs)\n",
        "\n",
        "fig = plot_losses(losses)\n",
        "conf_pltly()\n",
        "init_notebook_mode(connected=False)\n",
        "iplot(fig)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "296584fd-cac0-40e4-915b-fc4a7863bca0",
      "metadata": {
        "id": "296584fd-cac0-40e4-915b-fc4a7863bca0"
      },
      "outputs": [],
      "source": [
        "# project states onto the main directions of the gradient updates using n samples over all steps starting from sample x\n",
        "# the directions are calculated using the last sample as a reference\n",
        "directions, state_ids, loss_coordinates = get_state_directions(states, n_states=10, start_from=400, reference_id=-1)\n",
        "\n",
        "# compute the losses over the main directions of the gradient updates\n",
        "x, y, Z, _ = get_loss_grid(net, data_loader, loss_fn, directions=directions, resolution=(20, 20), scale=1)\n",
        "\n",
        "# plot the landscape as a contour plot\n",
        "fig = plot_contour(np.copy(x), np.copy(y), np.copy(Z), scale=True)\n",
        "fig.add_traces(go.Scatter(x=np.copy(loss_coordinates[0].cpu().numpy()),\n",
        "                          y=np.copy(loss_coordinates[1].cpu().numpy())))\n",
        "\n",
        "print('loss samples:', np.array(losses)[state_ids])\n",
        "\n",
        "conf_pltly()\n",
        "init_notebook_mode(connected=False)\n",
        "iplot(fig)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "71cc382c-8ddb-4ba6-aa83-96cf21679028",
      "metadata": {
        "id": "71cc382c-8ddb-4ba6-aa83-96cf21679028"
      },
      "source": [
        "### Learning Rate Decay\n",
        "\n",
        "Let's apply a last idea. What about adjusting the learning rate for each update so that later updates have less impact and therefore prevent oscillations around the optimum. Use the implementation of SGD with momentum as a starting point and adjust the learning rate for each step $t$ by a decay factor $d$:\n",
        "\n",
        "$$ \\eta_t = \\frac{\\eta_0}{1 + d \\cdot t} $$\n",
        "\n",
        "Which decay factor works the best?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f708f2a8-5b0a-4eee-8a90-e23c32e0f6a7",
      "metadata": {
        "id": "f708f2a8-5b0a-4eee-8a90-e23c32e0f6a7"
      },
      "outputs": [],
      "source": [
        "class SGDwMLD(Optimizer):\n",
        "    \n",
        "    def __init__(self, params, lr=0.2, momentum=0.9, decay=0.01) -> None:\n",
        "        super().__init__(params, {'lr': lr, 'momentum': momentum, 'decay': decay})\n",
        "        self.lr = lr\n",
        "        \n",
        "        # maybe you need some more code here\n",
        "    \n",
        "    def step(self):\n",
        "        with torch.no_grad():\n",
        "            for i, p in enumerate(self.param_groups[0]['params']):\n",
        "                if p.grad is not None:\n",
        "                    \n",
        "                    # update parameters here\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a2c6c8d5-6b26-4895-a9d9-f527ae564d7f",
      "metadata": {
        "id": "a2c6c8d5-6b26-4895-a9d9-f527ae564d7f"
      },
      "outputs": [],
      "source": [
        "net = get_GarmentClassifier()\n",
        "epochs = 20\n",
        "optimizer = SGDwMLD(net.parameters(), 0.02, 0.9, lr_d)\n",
        "\n",
        "losses, states = the_loop(net, optimizer, data_loader, epochs=epochs)\n",
        "\n",
        "fig = plot_losses(losses)\n",
        "conf_pltly()\n",
        "init_notebook_mode(connected=False)\n",
        "iplot(fig)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3a9b6a42-c400-4bff-8316-a819d8b71f76",
      "metadata": {
        "id": "3a9b6a42-c400-4bff-8316-a819d8b71f76"
      },
      "outputs": [],
      "source": [
        "# project states onto the main directions of the gradient updates using n samples over all steps starting from sample x\n",
        "# the directions are calculated using the last sample as a reference\n",
        "directions, state_ids, loss_coordinates = get_state_directions(states, n_states=10, start_from=800, reference_id=-1)\n",
        "\n",
        "# compute the losses over the main directions of the gradient updates\n",
        "x, y, Z, _ = get_loss_grid(net, data_loader, loss_fn, directions=directions, resolution=(20, 20), scale=1)\n",
        "\n",
        "# plot the landscape as a contour plot\n",
        "fig = plot_contour(np.copy(x), np.copy(y), np.copy(Z), scale=True)\n",
        "fig.add_traces(go.Scatter(x=np.copy(loss_coordinates[0].cpu().numpy()),\n",
        "                          y=np.copy(loss_coordinates[1].cpu().numpy())))\n",
        "\n",
        "print('loss samples:', np.array(losses)[state_ids])\n",
        "\n",
        "conf_pltly()\n",
        "init_notebook_mode(connected=False)\n",
        "iplot(fig)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "de45e976-b4d0-4da7-b93a-700748ea0543",
      "metadata": {
        "id": "de45e976-b4d0-4da7-b93a-700748ea0543"
      },
      "source": [
        "## Adaptive Learning Rates\n",
        "\n",
        "### Adaptive Gradient (AdaGrad)\n",
        "\n",
        "Taking the learning rate decay a step further, we take the past gradients into account and with this adjust the learning rate influence:\n",
        "\n",
        "$$ g_t = \\frac{\\partial L(\\theta_t)}{\\partial \\theta} $$\n",
        "$$ V_t = \\sqrt{\\sum_{i=1}^t (g_i)^2} + \\epsilon $$\n",
        "$$ \\theta_{t+1} = \\theta_t - \\eta \\frac{g_t}{V_t} $$\n",
        "\n",
        "- How does the AdaGrad perform compared to the Learning Rate Decay?\n",
        "- What could be a disadvantage of these update rules?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fa6008ce-e52d-4aa7-9f9f-199582d70580",
      "metadata": {
        "id": "fa6008ce-e52d-4aa7-9f9f-199582d70580"
      },
      "outputs": [],
      "source": [
        "class AdaGrad(Optimizer):\n",
        "    \n",
        "    def __init__(self, params, lr=0.01, eps=1e-10) -> None:\n",
        "        super().__init__(params, {'lr': lr})\n",
        "        self.lr = lr\n",
        "        \n",
        "        # maybe you need more code here\n",
        "    \n",
        "    def step(self):\n",
        "        with torch.no_grad():\n",
        "            for i, p in enumerate(self.param_groups[0]['params']):\n",
        "                if p.grad is not None:\n",
        "                    \n",
        "                    # update parameters\n",
        "                    ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5cf71e28-0e8c-4123-9a54-bbf66529646a",
      "metadata": {
        "id": "5cf71e28-0e8c-4123-9a54-bbf66529646a"
      },
      "outputs": [],
      "source": [
        "net = get_GarmentClassifier()\n",
        "epochs = 20\n",
        "optimizer = AdaGrad(net.parameters())\n",
        "\n",
        "losses, states = the_loop(net, optimizer, data_loader, epochs=epochs)\n",
        "\n",
        "fig = plot_losses(losses)\n",
        "conf_pltly()\n",
        "init_notebook_mode(connected=False)\n",
        "iplot(fig)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5d810d6c-7963-49f2-a5cd-0e3aa179b285",
      "metadata": {
        "id": "5d810d6c-7963-49f2-a5cd-0e3aa179b285"
      },
      "outputs": [],
      "source": [
        "# project states onto the main directions of the gradient updates using n samples over all steps starting from sample x\n",
        "# the directions are calculated using the last sample as a reference\n",
        "directions, state_ids, loss_coordinates = get_state_directions(states, n_states=10, start_from=900, reference_id=-1)\n",
        "\n",
        "# compute the losses over the main directions of the gradient updates\n",
        "x, y, Z, _ = get_loss_grid(net, data_loader, loss_fn, directions=directions, resolution=(20, 20), scale=loss_coordinates.abs().max().item())\n",
        "\n",
        "# plot the landscape as a contour plot\n",
        "fig = plot_contour(np.copy(x), np.copy(y), np.copy(Z), scale=True)\n",
        "fig.add_traces(go.Scatter(x=np.copy(loss_coordinates[0].cpu().numpy()),\n",
        "                          y=np.copy(loss_coordinates[1].cpu().numpy())))\n",
        "\n",
        "print('loss samples:', np.array(losses)[state_ids])\n",
        "\n",
        "conf_pltly()\n",
        "init_notebook_mode(connected=False)\n",
        "iplot(fig)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "fff6e5cb-9a3d-4f82-9be5-73da6589e4c7",
      "metadata": {
        "id": "fff6e5cb-9a3d-4f82-9be5-73da6589e4c7"
      },
      "source": [
        "### RMSProb\n",
        "\n",
        "RMSProb improves AdaGrad by using a second-order cumulative momentum using a decay factor $\\beta$:\n",
        "\n",
        "$$ V_t = \\sqrt{\\beta V_{t-1} + (1-\\beta)(g_t)^2} + \\epsilon $$\n",
        "\n",
        "- What is the benifit of this update rule compared to the $V_t$ of AdaGrad?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "369bf4f2-c433-49ec-bd54-3f91fbcb914c",
      "metadata": {
        "id": "369bf4f2-c433-49ec-bd54-3f91fbcb914c"
      },
      "outputs": [],
      "source": [
        "class RMSProb(Optimizer):\n",
        "    \n",
        "    def __init__(self, params, lr=0.01, eps=1e-10, decay=0.1) -> None:\n",
        "        super().__init__(params, {'lr': lr})\n",
        "        self.lr = lr\n",
        "        \n",
        "        # maybe you need more code here\n",
        "    \n",
        "    def step(self):\n",
        "        with torch.no_grad():\n",
        "            for i, p in enumerate(self.param_groups[0]['params']):\n",
        "                if p.grad is not None:\n",
        "                    \n",
        "                    # update parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "711d7dd1-842d-42ec-b765-7b71217fbd90",
      "metadata": {
        "id": "711d7dd1-842d-42ec-b765-7b71217fbd90"
      },
      "outputs": [],
      "source": [
        "net = get_GarmentClassifier()\n",
        "epochs = 20\n",
        "optimizer = RMSProb(net.parameters())\n",
        "\n",
        "losses, states = the_loop(net, optimizer, data_loader, epochs=epochs)\n",
        "\n",
        "fig = plot_losses(losses)\n",
        "conf_pltly()\n",
        "init_notebook_mode(connected=False)\n",
        "iplot(fig)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "68adc4d3-64a6-4a8f-8d1d-f2d9a2a1c343",
      "metadata": {
        "id": "68adc4d3-64a6-4a8f-8d1d-f2d9a2a1c343"
      },
      "outputs": [],
      "source": [
        "# project states onto the main directions of the gradient updates using n samples over all steps starting from sample x\n",
        "# the directions are calculated using the last sample as a reference\n",
        "directions, state_ids, loss_coordinates = get_state_directions(states, n_states=10, start_from=800, reference_id=-1)\n",
        "\n",
        "# compute the losses over the main directions of the gradient updates\n",
        "x, y, Z, _ = get_loss_grid(net, data_loader, loss_fn, directions=directions, resolution=(20, 20), scale=loss_coordinates.abs().max().item())\n",
        "\n",
        "# plot the landscape as a contour plot\n",
        "fig = plot_contour(np.copy(x), np.copy(y), np.copy(Z), scale=True)\n",
        "fig.add_traces(go.Scatter(x=np.copy(loss_coordinates[0].cpu().numpy()),\n",
        "                          y=np.copy(loss_coordinates[1].cpu().numpy())))\n",
        "\n",
        "print('loss samples:', np.array(losses)[state_ids])\n",
        "\n",
        "conf_pltly()\n",
        "init_notebook_mode(connected=False)\n",
        "iplot(fig)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "e206e871-c9ad-4430-9af6-dcce399ce6d9",
      "metadata": {
        "id": "e206e871-c9ad-4430-9af6-dcce399ce6d9"
      },
      "source": [
        "### Adaptive Moment Estimation (Adam)\n",
        "\n",
        "For the lecture, you should know the Adam optimizer. Implement it and compare it to previous methods.\n",
        "\n",
        "- What is the advantage of Adam compared to RMSProb?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cedd8989-05f6-45bd-ad66-15c40730cb6a",
      "metadata": {
        "id": "cedd8989-05f6-45bd-ad66-15c40730cb6a"
      },
      "outputs": [],
      "source": [
        "class Adam(Optimizer):\n",
        "    \n",
        "    def __init__(self, params, lr=0.01, grad_decay=0.9, squared_grad_decay=0.999, eps=1e-8) -> None:\n",
        "        super().__init__(params, {'lr': lr})\n",
        "        self.lr = lr\n",
        "        \n",
        "        # maybe you need more code here\n",
        "    \n",
        "    def step(self):\n",
        "        with torch.no_grad():\n",
        "            for i, p in enumerate(self.param_groups[0]['params']):\n",
        "                if p.grad is not None:\n",
        "                    \n",
        "                    # update parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4603df77-6abe-4991-83cc-50a57c1b7e9d",
      "metadata": {
        "id": "4603df77-6abe-4991-83cc-50a57c1b7e9d"
      },
      "outputs": [],
      "source": [
        "net = get_GarmentClassifier()\n",
        "epochs = 20\n",
        "optimizer = Adam(net.parameters(), lr=0.002)\n",
        "\n",
        "losses, states = the_loop(net, optimizer, data_loader, epochs=epochs)\n",
        "\n",
        "fig = plot_losses(losses)\n",
        "conf_pltly()\n",
        "init_notebook_mode(connected=False)\n",
        "iplot(fig)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e9132f1f-3f9c-4833-8439-97a23a1a5aa3",
      "metadata": {
        "id": "e9132f1f-3f9c-4833-8439-97a23a1a5aa3"
      },
      "outputs": [],
      "source": [
        "# project states onto the main directions of the gradient updates using n samples over all steps starting from sample x\n",
        "# the directions are calculated using the last sample as a reference\n",
        "directions, state_ids, loss_coordinates = get_state_directions(states, n_states=10, start_from=800, reference_id=-1)\n",
        "\n",
        "# compute the losses over the main directions of the gradient updates\n",
        "x, y, Z, _ = get_loss_grid(net, data_loader, loss_fn, directions=directions, resolution=(20, 20), scale=loss_coordinates.abs().max().item())\n",
        "\n",
        "# plot the landscape as a contour plot\n",
        "fig = plot_contour(np.copy(x), np.copy(y), np.copy(Z), scale=True)\n",
        "fig.add_traces(go.Scatter(x=np.copy(loss_coordinates[0].cpu().numpy()),\n",
        "                          y=np.copy(loss_coordinates[1].cpu().numpy())))\n",
        "\n",
        "print('loss samples:', np.array(losses)[state_ids])\n",
        "\n",
        "conf_pltly()\n",
        "init_notebook_mode(connected=False)\n",
        "iplot(fig)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "50d7a0b4-928e-4ec9-9f46-d90a0b212397",
      "metadata": {
        "id": "50d7a0b4-928e-4ec9-9f46-d90a0b212397"
      },
      "source": [
        "## Stochastic Variance Reduction Gradient (SVRG)\n",
        "\n",
        "A last strategy to further improve convergence, stochastic variance reduction averages the last $n$ parameters to reduce oscillations in the last training episode. PyTorch already offers an implementation.\n",
        "\n",
        "- Does this improve the convergence in our case?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "90e4b165-23fa-4e64-a774-bf3d5bed6266",
      "metadata": {
        "id": "90e4b165-23fa-4e64-a774-bf3d5bed6266"
      },
      "outputs": [],
      "source": [
        "from torch.optim.swa_utils import AveragedModel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "67d2204f-b9a9-4cad-8221-9ea2448b6d6c",
      "metadata": {
        "id": "67d2204f-b9a9-4cad-8221-9ea2448b6d6c"
      },
      "outputs": [],
      "source": [
        "net = get_GarmentClassifier()\n",
        "avg_net = AveragedModel(net)\n",
        "epochs = 20\n",
        "optimizer = Adam(net.parameters(), lr=0.002)\n",
        "\n",
        "losses, states = the_loop(net, optimizer, data_loader, epochs=epochs, swa_model=avg_net, swa_start=15)\n",
        "\n",
        "fig = plot_losses(losses)\n",
        "conf_pltly()\n",
        "init_notebook_mode(connected=False)\n",
        "iplot(fig)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6b0f72c2-dd3a-4988-b1c2-0bac0799f7ee",
      "metadata": {
        "id": "6b0f72c2-dd3a-4988-b1c2-0bac0799f7ee"
      },
      "outputs": [],
      "source": [
        "# project states onto the main directions of the gradient updates using n samples over all steps starting from sample x\n",
        "# the directions are calculated using the last sample as a reference\n",
        "directions, state_ids, loss_coordinates = get_state_directions(states, n_states=10, start_from=800, reference_id=-1)\n",
        "\n",
        "# compute the losses over the main directions of the gradient updates\n",
        "x, y, Z, _ = get_loss_grid(net, data_loader, loss_fn, directions=directions, resolution=(20, 20), scale=loss_coordinates.abs().max().item())\n",
        "\n",
        "# plot the landscape as a contour plot\n",
        "fig = plot_contour(np.copy(x), np.copy(y), np.copy(Z), scale=True)\n",
        "fig.add_traces(go.Scatter(x=np.copy(loss_coordinates[0].cpu().numpy()),\n",
        "                          y=np.copy(loss_coordinates[1].cpu().numpy())))\n",
        "\n",
        "print('loss samples:', np.array(losses)[state_ids])\n",
        "\n",
        "conf_pltly()\n",
        "init_notebook_mode(connected=False)\n",
        "iplot(fig)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "11f8178b-33b4-4578-8f8d-88ff0e392251",
      "metadata": {
        "id": "11f8178b-33b4-4578-8f8d-88ff0e392251"
      },
      "source": [
        "## Hyperparameter Tuning\n",
        "\n",
        "As a last task, find the best training strategy from (SGDwMLD, AdaGrad, RMSProb, Adam) and their hyperparameters to classify MNIST with the GarmentClassifier with 1 epoch of training.\n",
        "\n",
        "For this we load all data and use a train, validation, and test split of 90%, 5%, 5%."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "132fe59b-cdf2-44e0-8de0-de294ad7921f",
      "metadata": {
        "id": "132fe59b-cdf2-44e0-8de0-de294ad7921f"
      },
      "outputs": [],
      "source": [
        "data = datasets.MNIST(root=\"./\", transform=transforms, target_transform=None)\n",
        "train, val, test = random_split(data, [0.9, 0.05, 0.05])\n",
        "\n",
        "train_loader = DataLoader(train, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val, batch_size=batch_size)\n",
        "test_loader = DataLoader(test, batch_size=batch_size)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
