{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "tomqFnE1NSKZ"
   },
   "source": [
    "# Introduction\n",
    "\n",
    "In this exercise, we want to extend on the abilites of the U-Net of the last exercise and create a Generative Adversarial Network. First we will apply it to a more complex segmentation task, while the U-Net is certainly suited to this task it can be improved by using GAN approaches for selfsupervision. Furthermore, the final GAN should be able to invert the task and recreate original images from the segmented images. Since the training of a GAN usually takes a while we will not achive realistic images but the structures and certain details should generate and be recognizable.\n",
    "\n",
    "We will\n",
    "\n",
    "1. Modify the Datapipe for a new Dataset\n",
    "\n",
    "2. Extend the Model to create a GAN\n",
    "\n",
    "3. Improving the Discriminator\n",
    "\n",
    "4. Generate Images\n",
    "\n",
    "For results the different Networks should be running and the Output of the Last Network (Section 4.) Should be submitted for after at least 20 Epochs. Any Question should be answered in as few sentences or bulletpoints as possible."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "rNqvIQgNPPlR"
   },
   "source": [
    "# 0. Prequisites\n",
    "\n",
    "We will first import the necessary data. Please, before running the following code, access the link below, right click on the .zip file, then click \"Add shortcut to Drive\" and select your Drive folder:\n",
    "\n",
    "https://drive.google.com/file/d/1BLMs1klSrIgZ48arCKYxpn0jTCrV3Hg6/view?usp=sharing\n",
    "\n",
    "Like always:\n",
    "Make sure to request GPUs from Colab.\n",
    "Make a copy to allow saving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "anwM9q0aPNBK"
   },
   "outputs": [],
   "source": [
    "#from google.colab import drive\n",
    "# mount drive\n",
    "#drive.mount('/content/drive')\n",
    "\n",
    "#!unzip -qq '/content/drive/MyDrive/data/CMP_facade_DB.zip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "mpVsbxvJGtbo"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/vol/fob-vol4/mi17/christod/Visual_Computing/exercises/exercise_06/content/CMP_facade_DB\n"
     ]
    }
   ],
   "source": [
    "%cd content/CMP_facade_DB/"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "D6-4cE1rQJYg"
   },
   "source": [
    "This will handle all the imports we will need for this Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EIAWWZExQIs6"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.models.mobilenetv2 import InvertedResidual\n",
    "\n",
    "from utils.SegmentationModel import SegmentationModel\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "00WaUfT1QneO"
   },
   "source": [
    "# 1. Modify the Datapipe for a new Dataset\n",
    "\n",
    "This time we are going to work with the segmentation of facades. A good first step is always to download the data and look into it. The folder structure should be the same as in the last exercise but some things will be different and we will need to modify the CustomDataset Class to make it work for our new Data.\n",
    "\n",
    "1. The Masks have a different file type\n",
    "2. The Images are neither of the same size nor do they have a constant aspect ratio\n",
    "  - The images are a bit too detailed for the time we have for training so in a first step we want to half the size of the image\n",
    "  -  We already have a random crop function we can use. But now not all images are at a minimum 256x256. So we have to change the function so that it will work with smaller inputs. Consider that the output size still needs to be 256x256! (Feel free to look up the code of the original *get_crop_param *function that we used)\n",
    "3. We do not need nor want the same data augmentation.\n",
    "  - In this usecase we will reduce most of the preprocessing and only apply the randomcrop and a horizontal flip.\n",
    "  - Of course more preprocessing steps could be useful but some might not make sense with the data we were given. Why would we want or not want to apply: **Random rotation, Vertical flipping, Color Jitter, Gaussian Blur and Grayscale**?\n",
    "\n",
    "*Optional: Resize the images by half or even to a quarter of the original size to make the training faster. (Recommended for testing and faster results)*\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "lYuaoT-jBus1"
   },
   "source": [
    "***Answer Questions here***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YrMsX7-cMgD-"
   },
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(\n",
    "        self, root_dir, transform, transform_mask):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.transform_mask = transform_mask\n",
    "\n",
    "        self.img_dir = os.path.join(root_dir, \"rgb\")\n",
    "        self.mask_dir = os.path.join(root_dir, \"segmentation\")\n",
    "        self.img_filenames = sorted(os.listdir(self.img_dir))\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        #TODO: Load the masks\n",
    "        img_filename = self.img_filenames[idx]\n",
    "        img_path = os.path.join(self.img_dir, img_filename)\n",
    "        mask_path = os.path.join(self.mask_dir, img_filename)\n",
    "\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        mask = Image.open(mask_path).convert(\"RGB\")\n",
    "\n",
    "        image, mask = self.common_transform(image, mask)\n",
    "\n",
    "        image = self.transform(image)\n",
    "        mask = self.transform_mask(mask)\n",
    "\n",
    "        return image, mask\n",
    "\n",
    "    def get_crop_param(self, img, output_size):\n",
    "        #TODO: Adjust get_crop_param\n",
    "\n",
    "\n",
    "    def common_transform(self, image, mask):\n",
    "        #TODO: resize\n",
    "\n",
    "        # Random crop\n",
    "        crop_size = 256\n",
    "        i, j, h, w = transforms.RandomCrop.get_params(image, output_size=(crop_size, crop_size))\n",
    "        image = TF.crop(image, i, j, h, w)\n",
    "        mask = TF.crop(mask, i, j, h, w)\n",
    "\n",
    "        # # Random horizontal flipping\n",
    "        if random.random() > 0.5:\n",
    "            image = TF.hflip(image)\n",
    "            mask = TF.hflip(mask)\n",
    "\n",
    "        #Optional: More resizing\n",
    "\n",
    "        return image, mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9pUKgn90E2Oq"
   },
   "outputs": [],
   "source": [
    "# Define the paths to your train and test data folders\n",
    "train_data_path = \"train_data\"\n",
    "test_data_path = \"test_data\"\n",
    "\n",
    "# Define the transformations to be applied to the images and masks\n",
    "transform_img = transforms.Compose(\n",
    "    [\n",
    "        transforms.CenterCrop(256), #Optional: Change if you are resizing the images\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "transform_mask = transforms.Compose(\n",
    "    [\n",
    "        transforms.CenterCrop(256), #Optional: Change if you are resizing the images\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create the training dataset\n",
    "train_dataset = CustomDataset(\n",
    "    root_dir=train_data_path,\n",
    "    transform=transform_img,\n",
    "    transform_mask=transform_mask\n",
    ")\n",
    "\n",
    "# Create the testing dataset\n",
    "test_dataset = CustomDataset(\n",
    "    root_dir=test_data_path,\n",
    "    transform=transform_img,\n",
    "    transform_mask=transform_mask\n",
    ")\n",
    "\n",
    "# Define the batch size for the DataLoader\n",
    "batch_size = 16\n",
    "\n",
    "# Create the testing data loader\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "sVOlG3UWTtZf"
   },
   "source": [
    "This is a basic training loop that should work for the U-NET. Add the output function to check if your random crop and resize works and Run the U-NET with the new Data. The results should look like a \"blobby\" segmentations with mainly blue and red coloring. We do not need to train the network for long since a full training would take far too much time so we should not expect the results to be precice at this point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oJsmFDw1WBoj"
   },
   "outputs": [],
   "source": [
    "import torchvision.utils as vutils\n",
    "\n",
    "def plot_batch(batch_images, batch_masks):\n",
    "    grid_images = vutils.make_grid(batch_images, nrow=4, padding=10, normalize=True)\n",
    "    grid_masks = vutils.make_grid(batch_masks, nrow=4, padding=10, normalize=True)\n",
    "\n",
    "    fig, axes = plt.subplots(2, 1, figsize=(10, 10))\n",
    "    axes[0].imshow(grid_images.permute(1, 2, 0))\n",
    "    axes[0].set_title(\"Images\")\n",
    "    axes[0].axis(\"off\")\n",
    "\n",
    "    axes[1].imshow(grid_masks.permute(1, 2, 0))\n",
    "    axes[1].set_title(\"Masks\")\n",
    "    axes[1].axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ztuCL69HTtEX"
   },
   "outputs": [],
   "source": [
    "output_interval = 3\n",
    "learning_rate = 0.0002\n",
    "num_epochs = 12\n",
    "\n",
    "random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Create an instance of the UNet\n",
    "gen_model = SegmentationModel(3)\n",
    "\n",
    "# Define loss functions\n",
    "criterion = nn.L1Loss()\n",
    "\n",
    "# Define optimizers for generator and discriminator\n",
    "gen_optimizer = optim.Adam(gen_model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Move the model to the appropriate device (e.g., GPU if available)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "gen_model.to(device)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = 0.0\n",
    "    val_loss = 0.0\n",
    "    total_l1 = 0.0\n",
    "    num_samples = 0\n",
    "\n",
    "    gen_model.train()  # Set the model to training mode\n",
    "\n",
    "    # Initialize the progress bar\n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\", ncols=80)\n",
    "\n",
    "    for img, mask in pbar:\n",
    "        # Move the data to the appropriate device\n",
    "        img = img.to(device)\n",
    "        mask = mask.squeeze(1).to(device)\n",
    "\n",
    "        #### -------------- ####\n",
    "        #    Generator step    #\n",
    "        #### -------------- ####\n",
    "\n",
    "        # Set generator gradients to zero\n",
    "        gen_optimizer.zero_grad()\n",
    "        # Forward pass\n",
    "        output = gen_model(img)\n",
    "        # Compute losses\n",
    "        gen_loss = criterion(output, mask)\n",
    "        loss = gen_loss\n",
    "\n",
    "        # Backward pass and optimization step\n",
    "        loss.backward()\n",
    "        gen_optimizer.step()\n",
    "\n",
    "        # Update the progress bar description with the current loss\n",
    "        pbar.set_postfix({\"Loss\": loss.item()})\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    train_loss /= len(train_loader)\n",
    "\n",
    "    # Print epoch summary\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} | Training Loss: {train_loss}\")\n",
    "\n",
    "    # Validation loop\n",
    "    gen_model.eval()  # Set the model to evaluation mode\n",
    "    iteration = 0  # Initialize the iteration counter\n",
    "    with torch.no_grad():\n",
    "        for val_img, val_mask in test_loader:\n",
    "            val_img = val_img.to(device)\n",
    "            val_mask = val_mask.squeeze(1).to(device)\n",
    "            val_output = gen_model(val_img)\n",
    "            val_loss += criterion(val_output, val_mask).item()\n",
    "\n",
    "            if (epoch + 1) % output_interval == 0 and iteration == 0:\n",
    "                plot_batch(val_img.cpu(), val_output.cpu())\n",
    "\n",
    "            iteration += 1\n",
    "\n",
    "        # update validation loss\n",
    "        val_loss /= len(test_loader)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} | Validation Loss: {val_loss}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "7AVqddhbUxlO"
   },
   "source": [
    "# 2. Extend the Model to create a GAN\n",
    "\n",
    "GANs introduce a second Network, the discriminator to enable additional supervision in the trainings process. The discriminators' task is to decide if a given image is a real image or generated. it will be trained by its success on that task while the original Network will get feedback if and how easy this task was.\n",
    "\n",
    "In our case the U-Net is the generator now we are missing the discriminator. At a first step we implement a simple discriminator Network. Four layers of both **Convolution** and **LeakyReLU** and a **Linear layer** outputting onto one Class. Add an **InstanceNorm2d** layer directly after the first Convolution layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q783t3tkVR83"
   },
   "outputs": [],
   "source": [
    "class simpleDiscriminator(nn.Module):\n",
    "    def __init__(self, in_channels=3):\n",
    "        super(simpleDiscriminator, self).__init__()\n",
    "\n",
    "        def discriminator_block(in_filters, out_filters, normalization=True):\n",
    "          #TODO: Implement the discriminator block\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            *discriminator_block(in_channels, 64, normalization=False),\n",
    "            *discriminator_block(64, 128),\n",
    "            *discriminator_block(128, 256),\n",
    "            *discriminator_block(256, 512),\n",
    "            *discriminator_block(512, 1024),\n",
    "        )\n",
    "\n",
    "        # The height and width of downsampled image\n",
    "        ds_size = 256 // (2 ** 5)\n",
    "\n",
    "        self.out_layer = #...\n",
    "\n",
    "    def forward(self, img_input):\n",
    "        x = self.model(img_input)\n",
    "        # Flattens the output to one per image in batch\n",
    "        return self.out_layer(x.view(x.shape[0], -1))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "OELOV7AiWUE5"
   },
   "source": [
    "We can adjust the training loop from the last exercise by adding the new losses and the new Network.\n",
    " 1. The discriminator has to be included added to the trainings loop so it can be trained\n",
    " 2. The gan_criterion has be be added to the Generator step.\n",
    " \t- Since the losses are of different magnitude, weights have to be added. **Weight the existing l1Loss 100 times the gan Loss**\n",
    " 3. Add the training for the Discriminator in the Discriminator step\n",
    "\n",
    " Run the GAN now. The Network should quickly manage to color the image mainly in blue background and add some light blue, red, and green segmentation to many objects in the Image. Again we do not need to train the network for long time just for a few outputs to be created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CyTg093TWTx6"
   },
   "outputs": [],
   "source": [
    "### BasicGAN TODO:add the Discriminator and the new GAN and Discriminator losses\n",
    "\n",
    "output_interval = 3\n",
    "learning_rate = 0.0002\n",
    "num_epochs = 12\n",
    "\n",
    "random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Create an instance of the Networks\n",
    "gen_model = SegmentationModel(3)\n",
    "\n",
    "\n",
    "# Define loss functions\n",
    "criterion = nn.L1Loss()\n",
    "gan_criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# Define optimizers for generator and discriminator\n",
    "gen_optimizer = optim.Adam(gen_model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Move the model to the appropriate device (e.g., GPU if available)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "gen_model.to(device)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = 0.0\n",
    "    val_loss = 0.0\n",
    "    total_l1 = 0.0\n",
    "    num_samples = 0\n",
    "\n",
    "    # Set the models to training mode\n",
    "    gen_model.train()\n",
    "\n",
    "    # Initialize the progress bar\n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\", ncols=80)\n",
    "\n",
    "    for img, mask in pbar:\n",
    "        # Move the data to the appropriate device\n",
    "        img = img.to(device)\n",
    "        mask = mask.squeeze(1).to(device)\n",
    "\n",
    "        # Define ground truths for the GAN\n",
    "        valid = torch.ones((img.shape[0], 1), dtype=img.dtype, device=img.device, requires_grad=False)\n",
    "        fake = #...\n",
    "\n",
    "        #### -------------- ####\n",
    "        #    Generator step    #\n",
    "        #### -------------- ####\n",
    "\n",
    "        # Set generator gradients to zero\n",
    "        gen_optimizer.zero_grad()\n",
    "        # Forward pass\n",
    "        output = gen_model(img)\n",
    "        # Compute losses\n",
    "        gen_loss = criterion(output, mask)\n",
    "        gan_loss = #...\n",
    "        loss = #...\n",
    "\n",
    "        # Backward pass and optimization step\n",
    "        loss.backward()\n",
    "        gen_optimizer.step()\n",
    "\n",
    "        # Update the progress bar description with the current loss\n",
    "        pbar.set_postfix({\"Loss\": loss.item()})\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        #### ------------------ ####\n",
    "        #    Discriminator step    #\n",
    "        #### ------------------ ####\n",
    "\n",
    "        # Set discriminator gradients to zero\n",
    "\n",
    "        # Compute losses\n",
    "\n",
    "        # Backward pass and optimization #del all\n",
    "\n",
    "\n",
    "    train_loss /= len(train_loader)\n",
    "\n",
    "    # Print epoch summary\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} | Training Loss: {train_loss}\")\n",
    "\n",
    "    # Validation loop\n",
    "    gen_model.eval()  # Set the model to evaluation mode\n",
    "    iteration = 0  # Initialize the iteration counter\n",
    "    with torch.no_grad():\n",
    "        for val_img, val_mask in test_loader:\n",
    "            val_img = val_img.to(device)\n",
    "            val_mask = val_mask.squeeze(1).to(device)\n",
    "            val_output = gen_model(val_img)\n",
    "            val_loss += criterion(val_output, val_mask).item()\n",
    "\n",
    "            if (epoch + 1) % output_interval == 0 and iteration == 0:\n",
    "                plot_batch(val_img.cpu(), val_output.cpu())\n",
    "\n",
    "            iteration += 1\n",
    "\n",
    "        # update validation loss\n",
    "        val_loss /= len(test_loader)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} | Validation Loss: {val_loss}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "Zq8Mi-aiXCRs"
   },
   "source": [
    "#3. Improving the Discriminator\n",
    "\n",
    "So far the Discriminator is only looking at the image as a whole. A common approach to improve this is to use PatchGAN. For a transformation of an image A to B, in our case Image to Segmentation Mask, it will do two things:\n",
    "\n",
    "1. It will look at patches instead of the whole image. For this task we create another Discriminator, which instead of the last conv and linear layer has a **ZeroPad2d** and a **Conv2d** Layer. Removing the last Conv-Layer will result in a 32x32 image, the padding layer should pad zeroes on the left and the top. The last Conv layer should result in 30x30 matrix where each value represents the result of a patch.  \n",
    "\n",
    "2. it will use two images as input and will compare Image A to target B and Image A to generated A, the generator's task is to reduce the difference between those two comparisons while the discriminator's task is to increase the difference between them.\n",
    "\n",
    "*Hint: Despite these changes the code of the discriminator does not need to change a lot.*\n",
    "\n",
    "Furthermore we change the Crossentropy criterion to nn.MSELoss(). Why is that?\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "NwGFN5C7d58o"
   },
   "source": [
    "***Answer Questions here***\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CQq8qUg0Ai_J"
   },
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, in_channels=3):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        def discriminator_block(in_filters, out_filters, normalization=True):\n",
    "          #...\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            *discriminator_block(in_channels * 2, 64, normalization=False),\n",
    "            *discriminator_block(64, 128),\n",
    "            *discriminator_block(128, 256),\n",
    "            *discriminator_block(256, 512),\n",
    "            *discriminator_block(512, 1024),\n",
    "            #TODO: Modify last Layers\n",
    "        )\n",
    "\n",
    "    def forward(self, img_A, img_B):\n",
    "        # Concatenate image and condition image by channels to produce input\n",
    "        img_input = torch.cat((img_A, img_B), 1)\n",
    "        return self.model(img_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CpT64bZQAloK"
   },
   "outputs": [],
   "source": [
    "### patchgan TODO: Improve the Ground Truth (valid and fake) and pass two images to the discriminator. Also we want to improve the Validation outputs\n",
    "#HINT: Do not forget to add your changes from the simple GAN!\n",
    "\n",
    "output_interval = 3\n",
    "learning_rate = 0.0002\n",
    "num_epochs = 18\n",
    "\n",
    "random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Create an instance of the UNet\n",
    "gen_model = SegmentationModel(3)\n",
    "\n",
    "# get output shape of image discriminator (PatchGAN)\n",
    "patch = (256 // 2 ** 4, 256 // 2 ** 4) #Optional: If you changed the size of the input image you will need to change the values here\n",
    "\n",
    "# Define loss functions\n",
    "criterion = nn.L1Loss()\n",
    "gan_criterion = nn.MSELoss()\n",
    "\n",
    "# Define optimizers for generator and discriminator\n",
    "gen_optimizer = optim.Adam(gen_model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Move the model to the appropriate device (e.g., GPU if available)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "gen_model.to(device)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    gen_train_loss = 0.0\n",
    "    dis_train_loss = 0.0\n",
    "    gen_val_loss = 0.0\n",
    "    dis_val_loss = 0.0\n",
    "    num_samples = 0\n",
    "\n",
    "    # Set the models to training mode\n",
    "    gen_model.train()\n",
    "\n",
    "    # Initialize the progress bar\n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\", ncols=80)\n",
    "\n",
    "    for img, mask in pbar:\n",
    "        # Move the data to the appropriate device\n",
    "        img = img.to(device)\n",
    "        mask = mask.squeeze(1).to(device)\n",
    "\n",
    "        # Define ground truths for the GAN\n",
    "        # Hint: you will need the patch variable\n",
    "        valid = #...\n",
    "        fake = #...\n",
    "\n",
    "        #### -------------- ####\n",
    "        #    Generator step    #\n",
    "        #### -------------- ####\n",
    "\n",
    "        # Set generator gradients to zero\n",
    "        gen_optimizer.zero_grad()\n",
    "        # Forward pass\n",
    "        output = gen_model(img)\n",
    "        # Compute losses\n",
    "        gen_loss = criterion(output, mask)\n",
    "        #...\n",
    "\n",
    "        # Backward pass and optimization step\n",
    "        loss.backward()\n",
    "        gen_optimizer.step()\n",
    "\n",
    "        # Update the progress bar description with the current loss\n",
    "        pbar.set_postfix({\"Loss\": loss.item()})\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        #### ------------------ ####\n",
    "        #    Discriminator step    #\n",
    "        #### ------------------ ####\n",
    "\n",
    "        #...\n",
    "\n",
    "    gen_train_loss /= len(train_loader)\n",
    "    dis_train_loss /= len(train_loader)\n",
    "\n",
    "    # Print epoch summary\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} | Generator Training Loss: {gen_train_loss} | Discriminator Training Loss: {dis_train_loss}\")\n",
    "\n",
    "    # Validation loop\n",
    "    # Set the models to evaluation mode\n",
    "    gen_model.eval()\n",
    "\n",
    "\n",
    "    iteration = 0  # Initialize the iteration counter\n",
    "    with torch.no_grad():\n",
    "        for val_img, val_mask in test_loader:\n",
    "            val_img = val_img.to(device)\n",
    "            val_mask = val_mask.squeeze(1).to(device)\n",
    "            val_output = gen_model(val_img)\n",
    "\n",
    "            # Define ground truths for the GAN\n",
    "            valid = #...\n",
    "            fake = #...\n",
    "\n",
    "            # compute validation loss\n",
    "            gen_val_loss += criterion(val_output, val_mask).item()\n",
    "            #...\n",
    "\n",
    "            # compute discriminator validation loss\n",
    "            #...\n",
    "\n",
    "            if (epoch + 1) % output_interval == 0 and iteration == 0:\n",
    "                plot_batch(val_img.cpu(), val_output.cpu())\n",
    "\n",
    "            iteration += 1\n",
    "\n",
    "        # update validation loss\n",
    "        gen_val_loss /= len(test_loader)\n",
    "        dis_val_loss /= len(test_loader)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} | Validation Loss: {gen_val_loss} | Discriminator Validation Loss: {dis_val_loss}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "emaOJGvSZJ3v"
   },
   "source": [
    "# 4. Generate Images\n",
    "\n",
    "The Network we recreated so far is the Pix2Pix Network. While it is suitable for different tasks the original paper is famous for generating facades from  segmentation images. So as a last step we flip out inputs around.\n",
    "\n",
    " - You will have to modify the dataloader to change target and input. P*lease make it in a way that a flag can be set to activate the flip*\n",
    " - The training loop should be the same as it will work in both directions!\n",
    "\n",
    "Unfortunatly, the results presented in the paper should, in our case, generate after roughly 600 epochs. But we should already see structures that look like facades after 24 epochs.\n",
    "\n",
    "By Looking at the generated images and the source data, it is obvious that some parts of the original images are not generated at all. What could be the reason for it?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "aZX-Yg8gDV1e"
   },
   "source": [
    "***Answer Questions here***"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyM922Y5PiVA5r6HYE+MeZyv",
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
