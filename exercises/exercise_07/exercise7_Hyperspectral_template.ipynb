{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["### Neural Networks as Function Approximators for Hyperspectral Imaging\n","\n","In Neural Radiance Fields (NeRFs), Multi-Layer-Perceptrons (MLPs) are used to represent three-dimensional scenes.\n","\n","<img src=\"https://uploads-ssl.webflow.com/51e0d73d83d06baa7a00000f/5e700ef6067b43821ed52768_pipeline_website-01.png\" width=\"640px\" />\n","\n","To render an image, rays are cast into the scene for each image pixel, and query points along the ray are fed into an MLP to retrieve the density and color at this position.\n","\n","Thus, for NeRFs, the network takes a 5-dimensional input (position and angles), and produces a density and color value:\n","\n","<img src=\"https://uploads-ssl.webflow.com/51e0d73d83d06baa7a00000f/5e700a025ff238947d682a1f_pipeline_website-03.svg\" width=\"400px\" />\n","\n","NeRFs demonstrate that MLPs are powerful tools to approximate any high-dimensional function. In this exercise, we will apply this idea to represent images in the same fashion, instead of 3D scenes.\n","\n","We want to represent 2D images with an MLP $F$. Our image is defined as an intensity $I(x,y)$ over two coordinates, $x$ and $y$.\n","\n","Thus, our most basic network will take a two-dimensional input, and produce a one-dimensional output:\n","\n","$$ (x, y) → F_I → I $$\n","\n","As the network sees only one pair of coordinates at a time, we will have to query our network many times, for each and every pixel, to generate a full image."],"metadata":{"id":"dAyrjgHctWf0"}},{"cell_type":"markdown","source":["### Prerequisites\n","\n","If you have questions about this exercise or need help, you can contact Sophie at sophie.beckmann@hhi.fraunhofer.de\n","\n","First, we need to install the necessary packages and load the data.\n"],"metadata":{"id":"fHGGYXP41Yk3"}},{"cell_type":"markdown","source":["**Make sure to request GPUs from Colab!**\n","\n","-> Small arrow in the top-right corner -> \"View Resources\" -> \"Change runtime type\" (in the bottom)\n","\n","**Make a copy to allow saving!**"],"metadata":{"id":"-S8G-oEeG8wU"}},{"cell_type":"code","source":["!pip install torch torchvision imageio torchinfo matplotlib pandas"],"metadata":{"id":"roPugrkWT5Lh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torchvision\n","import torchinfo\n","import numpy as np\n","import imageio.v3 as iio\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","\n","# make the results reproducible\n","torch.manual_seed(42)\n","\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","\n","print(f\"Using {device} as PyTorch device\")"],"metadata":{"id":"EQ4h0ejt1ghA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["To import the data, please access the link below, right click on the .zip file, then click \"Add shortcut to Drive\" and select your Drive folder:\n","\n","https://drive.google.com/drive/folders/1wEeFvnKMXxpw8LWlFbCw8cmgydeFfeuX?usp=sharing"],"metadata":{"id":"UzCJoEX3RVcQ"}},{"cell_type":"code","source":["# load data\n","from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)\n","\n","img_file = \"/content/drive/MyDrive/resources_dnn4vc_hyperspectral.zip/st_michel.JPG\"\n","org_img = iio.imread(img_file, mode=\"L\")\n","\n","def imshow(tensor_img, colorbar=False):\n","    \"\"\"Display a tensor as an image, nearest interpolation.\"\"\"\n","    show = plt.imshow(tensor_img.squeeze(0).cpu(), interpolation=\"nearest\")\n","    if colorbar:\n","        plt.colorbar(show)\n","    plt.show()\n","\n","\n","def gray_imshow(tensor_img, colorbar=False):\n","    \"\"\"Display a tensor as a grayscale image (no color map), nearest interpolation.\"\"\"\n","    show = plt.imshow(tensor_img.squeeze(0).cpu(), cmap=\"gray\", interpolation=\"nearest\")\n","    if colorbar:\n","        plt.colorbar(show)\n","    plt.show()\n","\n","def downsampled_img_tensor(pil_img, img_scale=0.5):\n","\n","    # image as tensor\n","    # [batch x channels x height x width]\n","    org_img_tensor = torchvision.transforms.ToTensor()(pil_img).unsqueeze(0)\n","\n","    img_downsampled = torch.nn.functional.interpolate(\n","        org_img_tensor, scale_factor=img_scale, mode=\"bilinear\", align_corners=False\n","    )\n","    img_downsampled = img_downsampled.squeeze(0).squeeze(0).to(device)\n","\n","    return img_downsampled\n","\n","img_downsampled = downsampled_img_tensor(org_img, img_scale=1/2)\n"],"metadata":{"id":"pr27k6t4NVbf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We've loaded this image, which we want to represent with a network:"],"metadata":{"id":"GISPvrP8vMM1"}},{"cell_type":"code","source":["gray_imshow(img_downsampled)"],"metadata":{"id":"lRjnM8agvSt_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Representing a gray image with a simple MLP\n","\n","Now, let's set up a simple network, that maps from input coordinates to output intensity."],"metadata":{"id":"RektaiRavjK8"}},{"cell_type":"code","source":["class mlp(torch.nn.Module):\n","    \"\"\"A simple MLP that takes 2D coordinates as input and outputs an estimation of the grayscale value at this pixel coordinate.\n","       Thus, the input is of shape [batch x 2] and the output is of shape [batch x 1].\n","    \"\"\"\n","\n","    def __init__(self):\n","        super(mlp, self).__init__()\n","\n","        self.fc1 = torch.nn.Linear(2, 128)\n","        self.fc2 = torch.nn.Linear(128, 128)\n","        self.fc3 = torch.nn.Linear(128, 1)\n","        self.relu = torch.nn.ReLU()\n","        self.sigmoid = torch.nn.Sigmoid()\n","\n","    def forward(self, x):\n","        x = self.fc1(x)\n","        x = self.relu(x)\n","        x = self.fc2(x)\n","        x = self.relu(x)\n","        x = self.fc3(x)\n","        x = self.sigmoid(x)\n","\n","        return x"],"metadata":{"id":"6frX9Vxxn6NG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now we need a tensor that stores the coordinates for each pixel (to query the network with). We choose the coordinate range of [-1..1] for both x and y."],"metadata":{"id":"OnsZtpYkvuWo"}},{"cell_type":"code","source":["def create_coord_grid_2d(height, width):\n","    # Initialize grid with shape (2, height, width)\n","    # np.mgrid generates a grid that contains the coordinates of each pixel.\n","    # The coordinates are separated into two 2D arrays: one for the y (row) coordinates\n","    # and another for the x (column) coordinates.\n","    yx_grid = np.mgrid[:height, :width].astype(np.float32)\n","\n","    # Move grid coordinates to pixel center\n","    # This is done by adding 0.5 to all coordinates, which effectively shifts\n","    # the grid so that the coordinates refer to the pixel centers, not their top-left corners.\n","    yx_grid += 0.5\n","\n","    # Normalize grid coordinates to [0, 1] range\n","    yx_grid[0] /= height\n","    yx_grid[1] /= width\n","\n","    # Rescale grid coordinates to [-1, 1] range\n","    yx_grid = yx_grid * 2 - 1\n","\n","    # Transpose grid to [xy, h, w]\n","    # The yx_grid is currently in [yx, h, w] format,\n","    # where yx represents a stack of y (row) and x (column) coordinates.\n","    # Here we reverse the coordinate stack to [x, y], or [xy, h, w].\n","    xy_grid = yx_grid[::-1]\n","\n","    # Reshape grid to [h, w, xy] format and convert to PyTorch tensor\n","    xy_grid_t = torch.tensor(np.ascontiguousarray(xy_grid)).permute(1, 2, 0)\n","\n","    return xy_grid_t\n","\n","# [h, w, xy]\n","coord_grid = create_coord_grid_2d(\n","    height=img_downsampled.shape[-2], width=img_downsampled.shape[-1]\n",").to(device)"],"metadata":{"id":"MF4faTNvTBj5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Let's check the results, first let's look at the x coordinates for the whole image:"],"metadata":{"id":"HEyI2Y00wSkK"}},{"cell_type":"code","source":["# x coordinates are the first value of the last dimension of the grid\n","imshow(coord_grid[..., 0], colorbar=True)"],"metadata":{"id":"sJCxF_WswdJK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["And the y coordinates:"],"metadata":{"id":"pbnPwfzUwidM"}},{"cell_type":"code","source":["# y coordinates\n","imshow(coord_grid[..., 1], colorbar=True)"],"metadata":{"id":"9gAhiEY0wj3A"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["For the training loop, we feed the coordinate grid into the network. Remember: our network produces one intensity value for one pixel. Thus the height and width dimensions of our coordinate grid are effectively the batch dimensions of our network during training - the network never sees an image, it only sees individual coordinates."],"metadata":{"id":"IvbXSav-xVET"}},{"cell_type":"code","source":["def train(model, input, expected_output, num_epochs, loss_print_interval=100, show_interval=10000):\n","\n","    model_info = torchinfo.summary(model)\n","    # first two batch dimensions are always height and width\n","    print(f\"Image size: {input.shape[1]} x {input.shape[0]}\")\n","    print(\n","        f\"Image pixels per trainable parameter: {np.prod(input.shape[:2]) / model_info.trainable_params:.2f}\"\n","    )\n","    print(model_info)\n","\n","    optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)\n","    loss_fn = torch.nn.MSELoss()\n","\n","    for epoch in range(num_epochs):\n","        prediction = model(input).squeeze(-1)\n","\n","        loss = loss_fn(expected_output, prediction)\n","\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        if epoch % loss_print_interval == 0 or epoch == num_epochs - 1:\n","            print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n","\n","        if epoch > 0:\n","          if epoch % show_interval == 0 or epoch == num_epochs - 1:\n","              gray_imshow(prediction.detach(), colorbar=True)"],"metadata":{"id":"nG3F1LekxK1x"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Let's instantiate the model and train the network for 1000 epochs. When training is finished, it will show the predicted image."],"metadata":{"id":"5XbhK-Lcx09F"}},{"cell_type":"code","source":["model = mlp().to(device)\n","\n","train(model=model, input=coord_grid, expected_output=img_downsampled, num_epochs=1000, loss_print_interval=100, show_interval=10000)"],"metadata":{"id":"huciKGhex7Nz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["It looks like an artistic rendering of the input image, but we are missing all the details.\n","\n","### Task 1 - Experiment with different network sizes\n","\n","Rewrite the static MLP module definition, to make the number of layers and the layer width configurable. Then do a few experiments with different network configurations.\n","\n","What is the best image and lowest MSE loss you can produce, while using no more than one network parameter per 4 pixels?\n","\n","(Check \"Image pixels per trainable parameter\" >= 4 in the training output message).\n","\n","You should carefully watch the loss during training: if it gets stuck at the beginning (e.g. by only producing black images), you may have to decrease the learning rate.\n","\n","Here's the stub for your configurable network:"],"metadata":{"id":"YlyrdW9v0MO6"}},{"cell_type":"code","source":["class mlp_configurable(nn.Module):\n","    def __init__(self, input_dim=2, num_hidden_layers=3, hidden_layer_width=32):\n","        super(mlp_configurable, self).__init__()\n","\n","        # Input layer\n","        self.input_layer = nn.Linear(input_dim, hidden_layer_width)\n","\n","        # Hidden layers\n","        self.hidden_layers = nn.ModuleList()\n","        # TODO: implement\n","        ???\n","\n","        # Output layer\n","        self.output_layer = nn.Linear(hidden_layer_width, 1)\n","\n","        # Activation functions\n","        self.relu = nn.ReLU()\n","        self.sigmoid = nn.Sigmoid()\n","\n","    def forward(self, x):\n","        # Input layer\n","        x = self.relu(self.input_layer(x))\n","\n","        # Hidden layers\n","        # TODO: implement\n","        ???\n","\n","        # Output layer\n","        x = self.sigmoid(self.output_layer(x))\n","\n","        return x\n","\n","model_configurable = mlp_configurable().to(device)\n","\n","train(model=model_configurable, input=coord_grid, expected_output=img_downsampled, num_epochs=1000, loss_print_interval=100, show_interval=10000)"],"metadata":{"id":"80L6Zgfy1WaR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["As you will probably notice, it's really difficult for such a network to learn the high-frequency changes of the image signal. But there's a trick to help the network to learn better, by feeding it a different encoding of the input coordinates.\n","\n","### Positional Encoding (Frequency Coding)\n","\n","In positional encoding, an one-dimensional input coordinate is mapped into a higher dimension. In NeRFs, it is defined as\n","\n","$ p(x) = (sin(2^0 \\pi x), cos(2^0 \\pi x), ..., sin(2^{L-1} \\pi x), cos(2^{L-1} \\pi x)) $\n","\n","This function is applied separately to the input dimensions, in our case, to $x$ and $y$. $L$ is the number of frequencies that are used."],"metadata":{"id":"g47PIxr-2tcC"}},{"cell_type":"markdown","source":["### Task 2 - Positional Encoding Implementation\n","\n","Our network now changes from\n","\n","$$ (x, y) → F_I → I $$\n","\n","to\n","\n","$$ (p(x), p(y)) → F_E → I $$\n","\n","Complete the implementation of the positional encoding below. Then plot the encoded coordinate images (akin to the plots of the `coord_grid` above). To make sure your implementation is correct, check that it looks similar to the visualizations of the \"Encoded Values\" in https://docs.nerf.studio/en/latest/nerfology/model_components/visualize_encoders.html - showing different frequencies in different dimensions."],"metadata":{"id":"p_4b5XApqP26"}},{"cell_type":"code","source":["def positional_encoding(x, y, num_frequencies=4):\n","    \"\"\"\n","    Encodes x and y coordinates using sine and cosine functions with different frequencies.\n","\n","    This is the positional encoding scheme used in Neural Radiance Fields.\n","\n","    Args:\n","        x (torch.Tensor): The x coordinates.\n","        y (torch.Tensor): The y coordinates.\n","        num_frequencies (int): The number of different frequencies to use.\n","\n","    Returns:\n","        torch.Tensor: The encoded coordinates.\n","    \"\"\"\n","    # Initialize encoding\n","    encoding = []\n","\n","    # Encode x & y using sine and cosine functions with different frequencies\n","    for i in range(num_frequencies):\n","        frequency = 2**i * np.pi\n","\n","        # TODO: implement\n","        encoding.append(???)\n","        encoding.append(???)\n","\n","        encoding.append(???)\n","        encoding.append(???)\n","\n","    # Concatenate the encodings\n","    encoding = torch.stack(encoding, dim=-1)\n","\n","    return encoding\n","\n","# Assuming coord_grid is your coordinate grid tensor of shape [height, width, xy=2]\n","x_coords = coord_grid[..., 0]\n","y_coords = coord_grid[..., 1]\n","\n","# Encode the coordinates\n","# [h, w, coord_encoding]\n","NUM_FREQ = 8\n","encoded_coords = positional_encoding(x_coords, y_coords, num_frequencies=NUM_FREQ)\n","\n","# check: [height, width, num_freq * 4 (4=sin x, cos x, sin y, cos y)]\n","assert(encoded_coords.shape == (img_downsampled.shape[-2], img_downsampled.shape[-1], NUM_FREQ * 4))\n","coord_encoding_len = encoded_coords.shape[-1]"],"metadata":{"id":"Ze2d8xFaUesm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["With the implementation working, let's put it to use as the input for the MLP. Use the best MLP configuration that you were able to find in Task 1."],"metadata":{"id":"YNi9gWF17mzH"}},{"cell_type":"code","source":["# Note: our input dimension is no longer 2 (xy), it is now the length of the positional encoding\n","\n","enc_model = mlp_configurable(input_dim=coord_encoding_len, num_hidden_layers=1???, hidden_layer_width=128???).to(\n","    device\n",")\n","\n","train(model=enc_model, input=encoded_coords, expected_output=img_downsampled, num_epochs=1000, loss_print_interval=100, show_interval=10000)\n"],"metadata":{"id":"UQvCDX7W7vts"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["In the trained output, you should now see a much closer representation of our input image - although the size of our network has not changed (except for a few more parameters for the connection of the input to the first hidden layer)!\n","\n","### Task 3 - Image description\n","\n","Describe the differences you see in the output, of the network using positional encoding, compared to the simple coordinate encoding. Can you explain why the network behaves differently? Are there any noticable artefacts in the pos-encoded output image?\n","\n","**Answer:**\n","\n","..."],"metadata":{"id":"mrq9QHTi8P_r"}},{"cell_type":"markdown","source":["### Representing Color Images\n","\n","As the next step, we want to add color to the image. One option would be to have the network produce three output intensities instead of one (one each for the red, green, and blue channels). But, as we want to add more color channels later, we will go another route: making the color an additional input query.\n","\n","Thus, our updated network will look like this:\n","\n","$$ (p(x), p(y), c) → F_C → I $$\n","\n","To produce a full color image, we will have to query the network three times at each pixel, once for each channel.\n","\n","To represent our color $c$, we choose the values -1 for red, 0 for green and 1 for blue."],"metadata":{"id":"2y3zYqrppmFZ"}},{"cell_type":"code","source":["rgb_img = iio.imread(img_file)\n","rgb_img_downsampled = downsampled_img_tensor(rgb_img, img_scale=0.5)\n","\n","# let's repeat the encoded coordinates 3 times into a new dimension,\n","# which will become the color channel dimension\n","# [h, w, 3, coord_encoding]\n","encoded_coords_repeated = encoded_coords.unsqueeze(2).repeat((1, 1, 3, 1))\n","\n","def rgb_grid(height, width):\n","    \"\"\"This is the color encoding at each pixel.\"\"\"\n","    rgb_grid = torch.zeros(height, width, 3, dtype=torch.float32)\n","    # red\n","    rgb_grid[..., 0] = -1\n","    # green\n","    rgb_grid[..., 1] = 0\n","    # blue\n","    rgb_grid[..., 2] = 1\n","    return rgb_grid\n","\n","# [h, w, 3]\n","rgb_vals = rgb_grid(\n","    height=rgb_img_downsampled.shape[-2], width=rgb_img_downsampled.shape[-1]\n",").to(device)\n","\n","# color is now an additional input dimension to the network\n","# the value will always be -1, 0 or 1\n","rgb_input_dim = coord_encoding_len + 1\n","\n","# we now also have an additional (third) batch dimension, the color,\n","# as we need to query the network three times at each pixel\n","# [h, w, 3, rgb_input_dim]\n","rgb_coord_queries = torch.cat([encoded_coords_repeated, rgb_vals.unsqueeze(-1)], dim=-1)\n"],"metadata":{"id":"nWW3FaOlUmN3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Training this network, you should finally get to see green grass and a blue sky."],"metadata":{"id":"ic3ba8Yb_ZRs"}},{"cell_type":"code","source":["rgb_model = mlp_configurable(input_dim=rgb_input_dim, num_hidden_layers=1???, hidden_layer_width=128???).to(device)\n","\n","train(model=rgb_model, input=rgb_coord_queries, expected_output=rgb_img_downsampled.permute(1, 2, 0), num_epochs=1000, loss_print_interval=100, show_interval=1000)"],"metadata":{"id":"_0ER52BK_aqv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Wavelength response\n","\n","The network has only ever seen discrete color inputs at [-1, 0, -1]. What does it do when queried for in-between values? Let's find out, by querying it for some pixel over many samples of the complete range of $c$ from -1 to 1."],"metadata":{"id":"e8DrBSmXOkbV"}},{"cell_type":"code","source":["def plot_wavelen_response(model, coord_encoding, x_coord, y_coord,\n","                          min_wavelen_nm=450,\n","                          max_wavelen_nm=600):\n","\n","    # [coord_encoding_len]\n","    xy_encoded = coord_encoding[y_coord, x_coord]\n","\n","    samples = torch.arange(-100, 101) / 100\n","    samples = samples.to(device)\n","\n","    # repeat the xy encoding into a new sample dimension for the wavelength\n","    # [coord_encoding_len, num_samples]\n","    xy_encoded_repeated = xy_encoded.unsqueeze(-1).repeat((1, samples.shape[0]))\n","\n","    # switch around the dimensions,\n","    # so that the sampling dimension becomes the batch dimension\n","    # [num_samples, coord_encoding_len]\n","    # TODO: implement\n","    xy_encoded_repeated = ???\n","    assert xy_encoded_repeated.shape == (len(samples), len(xy_encoded))\n","\n","    # concat the wavelength sample query to the coordinates\n","    # [num_samples, coord_encoding_len + 1]\n","    query = torch.cat([xy_encoded_repeated, samples.unsqueeze(-1)], dim=-1)\n","    result = model(query)\n","\n","    intensity_at_samples = result.squeeze().cpu().detach()\n","\n","    samples_nm = (samples + 1) / 2 * (max_wavelen_nm - min_wavelen_nm) + min_wavelen_nm\n","\n","    series = pd.Series(\n","        result.squeeze().cpu().detach(), index=samples_nm.cpu().detach().numpy()\n","    )\n","    series.plot(xlabel=\"wavelength [nm]\", ylabel=\"intensity\")\n","    plt.show()\n","\n","plot_wavelen_response(rgb_model, coord_encoding=encoded_coords, x_coord=80, y_coord=50)"],"metadata":{"id":"AuLyLbvJOs5E"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Task 4 - Wavelength response interpretation (RGB)\n","\n","This graph shows the network response for ~200 samples, while it was only trained with values at the extremes and in the center. Is the smoothness of the curve to your expectation?\n","\n","**Answer:**"],"metadata":{"id":"3rIyPXcshbcG"}},{"cell_type":"markdown","source":[],"metadata":{"id":"bpkgFWn_hX64"}},{"cell_type":"markdown","source":["### Hyperspectral Imaging\n","\n","For the final section of this excercise, we want to add more resolution in the color channel of our trained networks. For that, we use the recording from a hyperspectral camera - a camera that records light at many different wavelengths at the same time, not just three.\n","\n","With hyperspectral cameras, it becomes possible to differentiate materials and surfaces much better than with images taken with a conventional camera. Use cases include medical and agricultural use, and it's an active area of research.\n","\n","Let's load up a hyperspectral image:"],"metadata":{"id":"f2J7l7hsiSIg"}},{"cell_type":"code","source":["hs_img_file = \"/content/drive/MyDrive/resources_dnn4vc_hyperspectral.zip/2023-05-17_1_Rcor_enh.png\"\n","bayer_img = iio.imread(hs_img_file)\n","hs_img = torchvision.transforms.ToTensor()(bayer_img).squeeze(0).to(device)\n","hs_img_tensor_norm = hs_img / 65535\n","\n","gamma = 1.8\n","hs_img_tensor_norm = torchvision.transforms.functional.adjust_gamma(hs_img_tensor_norm, gamma=1/gamma)\n","\n","imshow(hs_img_tensor_norm)\n","\n","wavelengths_nm = torch.tensor([460.6594, 466.0551, 476.3369, 484.0140, 494.1334, 506.4071, 513.1407,\n","        524.3460, 535.8630, 544.3856, 552.9605, 563.5027, 570.5034, 582.2681,\n","        588.0899, 596.6161]).to(device)\n"],"metadata":{"id":"KWMSy7X7ZXfr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["It has only one channel, giving us the direct values from the sensor. Let's look more closely:"],"metadata":{"id":"5hodHBUJjr2R"}},{"cell_type":"code","source":["imshow(hs_img_tensor_norm[:16, :24])"],"metadata":{"id":"XmUXLVmuarhW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We can see a repeating pattern here. That is the 4x4 pattern, of 16 different filters of different wavelenghts, repeating over the image. Let's model the 4x4 pattern, and display it:"],"metadata":{"id":"W0CNsM2DkPkO"}},{"cell_type":"code","source":["bayer_pattern = torch.arange(16).reshape(4, 4)\n","print(f\"Sensor bayer pattern: \\n{wavelengths_nm[bayer_pattern]}\")\n","\n","img_pattern = bayer_pattern.repeat(hs_img.shape[0] // 4, hs_img.shape[1] // 4)\n","\n","min_wavelen = wavelengths_nm.min()\n","max_wavelen = wavelengths_nm.max()\n","\n","px_wavelens = wavelengths_nm[img_pattern]\n","\n","# [0..1]\n","px_wavelens_norm = (px_wavelens - min_wavelen) / (max_wavelen - min_wavelen)\n","# [-1..1]\n","px_wavelens_norm = 2 * px_wavelens_norm - 1\n","\n","# [h, w, xy]\n","coord_grid_hs = create_coord_grid_2d(\n","    height=hs_img.shape[0], width=hs_img.shape[1]\n",").to(device)\n","encoded_coords_hs = positional_encoding(coord_grid_hs[..., 0], coord_grid_hs[..., 1], num_frequencies=8)\n","\n","# [h, w, coord_encoding + 1]\n","coords_wavelens = torch.cat([encoded_coords_hs, px_wavelens_norm.unsqueeze(-1)], dim=-1)\n","\n","imshow(px_wavelens[:16, :24], colorbar=True)"],"metadata":{"id":"rhs_sbGPbWR7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["As we already have a color query input, we don't need to change anything with our model. The difference to the RGB image is, that now we have many more samples along the color dimension, but each pixel only has a single sample (vs the RGB image, which has a sample for R, G and B each at every pixel, as it had already been processed from a RAW image)."],"metadata":{"id":"kusLqNSVkqRa"}},{"cell_type":"code","source":["hs_model = mlp_configurable(input_dim=rgb_input_dim, num_hidden_layers=4???, hidden_layer_width=128???).to(\n","    device\n",")\n","train(model=hs_model, input=coords_wavelens, expected_output=hs_img_tensor_norm, num_epochs=1000, loss_print_interval=100, show_interval=1000)"],"metadata":{"id":"RcW0c4jYcApu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["To render an RGB image, we can create three color channels the size of the image, and request the (-1, 0, 1) values again, to form the image.\n","\n","Because the camera is not sensitive in all areas of the human vision, and we made some simplifications, the colors may look slightly unnatural. But if all went well, you should see an overall green image with a white stick in the middle, a blue label in the center right, and a yellow brick wall at the top right. Is there enough detail in the image for you to guess what was being recorded?"],"metadata":{"id":"YyerdMWklwgF"}},{"cell_type":"markdown","source":[],"metadata":{"id":"E7Cc79wNm1Cv"}},{"cell_type":"code","source":["def render_rgb_img(model, input_query):\n","    with torch.no_grad():\n","        red_query = input_query.clone()\n","        red_query[:, :, -1] = 1.0\n","        red_img = model(red_query).squeeze(-1)\n","        red_img /= red_img.max()\n","\n","        green_query = input_query.clone()\n","        green_query[:, :, -1] = 0\n","        green_img = model(green_query).squeeze(-1)\n","        green_img /= green_img.max()\n","\n","        blue_query = input_query.clone()\n","        blue_query[:, :, -1] = -1.0\n","        blue_img = model(blue_query).squeeze(-1)\n","        blue_img /= blue_img.max()\n","\n","        rgb_img = torch.stack([red_img, green_img, blue_img], dim=-1)\n","        imshow(rgb_img.detach())\n","\n","render_rgb_img(model=hs_model, input_query=coords_wavelens)"],"metadata":{"id":"nrbW4cW8cEIu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plot_wavelen_response(hs_model, coord_encoding=encoded_coords_hs, x_coord=1500, y_coord=400, min_wavelen_nm=min_wavelen, max_wavelen_nm=max_wavelen)"],"metadata":{"id":"8w2p0b_YdAMw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Task 5 - Wavelength response interpretation (hyperspectral)\n","\n","Now we have looked again at the wavelength response of the network, now trained with hyperspectral data. How does it look different from the earlier plot? Is that expected?\n","\n","**Answer:**"],"metadata":{"id":"zmqqnoaom_H0"}}]}