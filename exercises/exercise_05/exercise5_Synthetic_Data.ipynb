{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Synthetic Data Generation and Data Augmentation\n",
        "\n",
        "## Introduction\n",
        "\n",
        "In this exercise, we will explore an exciting approach to tackling vision problems when labeled ground truth data is either unavailable or limited in quantity. We aim to conduct experiments with the benefits of full supervision, even under such challenging circumstances.\n",
        "\n",
        "Our specific focus will be on solving the problem of semantic segmentation, where we aim to accurately segment a target object in an image.\n",
        "\n",
        "In our experiment, the target object is available as a 3D model and no labelled real-world training data is available.\n",
        "\n",
        "To still achieve our goal, we will follow a structured approach.\n",
        "\n",
        "1. **Synthetic Data Generation**: We will leverage the power of **BlenderProc**, a versatile tool, to create synthetic training data. This allows to generate labeled data without manual efford.\n",
        "\n",
        "2. **Data Augmentation**: To further increase the varity of the data during training, we will explore different data augmentation techniques. By augmenting our dataset, we can introduce variations and increase the robustness of the trained model.\n",
        "\n",
        "3. **Building the Segmentation Network**: We will construct a state-of-the-art segmentation network called **U-NET**. This network architecture employs a pretrained **MobileNetV2** backbone, which provides a strong foundation for accurate segmentation. We will train the network with our generated data and we will also use the implemented data augmentation.\n",
        "\n",
        "\n",
        "**IMPORTANT NOTE ON HAND-IN PROCESS**\n",
        "\n",
        "Besides the saved notebook, please also hand in the maniuplated files `dataset_generator.py` and `quickstart.py`, as well as a ***sample from your generated dataset*** (one image and mask) and the `loss_plot.png` image (several versions, if you did several tests) and a ***brief interpretation of the results***. Everything can be handed in as one zip file.\n",
        "\n",
        "**Besides this notebook the edited files are not saved in Colab persistently, so  remember to copy the intermediate results to your computer.**\n"
      ],
      "metadata": {
        "id": "OjosY2MvsyMS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prerequirities\n",
        "\n",
        "We will first import the necessary data. Please, before running the following code, access the link below, right click on the .zip file, then click \"Add shortcut to Drive\" and select your Drive folder:\n",
        "\n",
        "https://drive.google.com/drive/folders/1KoQG7jkQXk5_oi16gRfLWgvgFQQYcf2A?usp=sharing"
      ],
      "metadata": {
        "id": "T2gXSmdLzjqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!unzip -qq '/content/drive/MyDrive/Resources_DNN4VC_Synthetic.zip'"
      ],
      "metadata": {
        "id": "_qxuEWoGvB3N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Make shure to request GPUs from Colab!**\n",
        "\n",
        "-> Small arrow in the top-right corner -> \"View Resources\" -> \"Change runtime type\" (in the bottom)\n",
        "\n",
        "**Make a copy to allow saving!**\n"
      ],
      "metadata": {
        "id": "vSIBTMPvzdi1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 1: Data generation\n",
        "\n",
        "Next we are going to take a look at the object we want to recognize. Luckily, we have got a 3D scan of it.\n",
        "We can use the `trimesh` to take a look at the object."
      ],
      "metadata": {
        "id": "pM_o6PnY2RVP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install trimesh"
      ],
      "metadata": {
        "id": "shXfvS9P3IpM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import trimesh\n",
        "import numpy as np\n",
        "\n",
        "# Load the PLY file\n",
        "mesh = trimesh.load_mesh('/content/Resources_DNN4VC_Synthetic/resources/lm/models/obj_000002.ply')\n",
        "s = trimesh.Scene()\n",
        "s.add_geometry(mesh)\n",
        "s.show()\n"
      ],
      "metadata": {
        "id": "_hs4X4F62wmS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we will have a look at our test data. Use the following script to show example images. In the path, you can replace `test_data` with `train_data/example` to see a preview image similar to the dataset you will create later.\n",
        "\n",
        "\n",
        "**Important**: In our example we have ground truth mask for the test images to evaluate the models performance."
      ],
      "metadata": {
        "id": "HwQZ8H54DdBl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "\n",
        "img_no = 1\n",
        "# Load the color image and the mask image\n",
        "color_image = cv2.imread('/content/Resources_DNN4VC_Synthetic/test_data/img/{:04}.png'.format(img_no))\n",
        "mask_image = cv2.imread('/content/Resources_DNN4VC_Synthetic/test_data/masks/{:04}.png'.format(img_no))\n",
        "\n",
        "# Convert the images from BGR to RGB\n",
        "color_image = cv2.cvtColor(color_image, cv2.COLOR_BGR2RGB)\n",
        "mask_image = cv2.cvtColor(mask_image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "# Create a figure with two subplots\n",
        "fig, axs = plt.subplots(1, 2)\n",
        "\n",
        "axs[0].imshow(color_image)\n",
        "axs[0].axis('off')\n",
        "axs[0].set_title('Color Image')\n",
        "axs[1].imshow(mask_image)\n",
        "axs[1].axis('off')\n",
        "axs[1].set_title('Mask Image')\n",
        "\n",
        "# Show the figure\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "VolaNOOiDtXW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.1 Preparation of BlenderProc\n",
        "\n",
        "Next, we are going to use [BlenderProc](https://github.com/DLR-RM/BlenderProc]). We install it by cloning the official git repository and then install it via pip."
      ],
      "metadata": {
        "id": "PkGAFt4ZC1gy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/DLR-RM/BlenderProc.git\n",
        "%cd \"BlenderProc\"\n",
        "!pip install -e ."
      ],
      "metadata": {
        "id": "rSMAeYxfBZwr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To be able to use matplotlib inside BlenderProc, we have to install ipykernel inside blender's python environment. As this is the first blenderproc command, it will also install blender first:"
      ],
      "metadata": {
        "id": "38vWtXCvJxWU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!blenderproc pip install ipykernel --blender-install-path ./"
      ],
      "metadata": {
        "id": "IXCvaNuEJuKX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lastly, we execute the BlenderProc program by invoking blenderproc run along with the necessary command line arguments. The initial argument indicates the path to the Python file that should be executed. The second argument corresponds to the camera pose file. In this instance, we have specified two camera poses in the `examples/basics/basic/camera_positions` file. The third argument pertains to the output directory where the generated data will be saved.\n",
        "\n",
        "To ensure compatibility, we use the `--blender-install-path` flag to specify a custom Blender installation path, as the availability of a user folder in Colab is limited."
      ],
      "metadata": {
        "id": "s-dziBU5LX2H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!blenderproc run examples/basics/basic/main.py examples/resources/camera_positions examples/resources/scene.obj examples/basics/basic/output --blender-install-path ./"
      ],
      "metadata": {
        "id": "OFwP8hSJJx93"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We generated two example images, which we now will take a look at. As you will see, Blender Proc did not only generate the colors, but also the normals and depth information.\n"
      ],
      "metadata": {
        "id": "t1QNfmcLL6yp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%run \"-m\" \"blenderproc\" \"vis\" \"hdf5\" \"examples/basics/basic/output/0.hdf5\""
      ],
      "metadata": {
        "id": "uvbOjivnMCvx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%run \"-m\" \"blenderproc\" \"vis\" \"hdf5\" \"examples/basics/basic/output/1.hdf5\""
      ],
      "metadata": {
        "id": "ltjsX4lAMTIL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you saw before, executing BlenderProc differs from running a regular Python file. Instead of directly executing the Python script, BlenderProc runs within the context of Blender's integrated Python environment. This integration allows Blender to execute the specified Python script and perform the desired tasks using its internal functionality and capabilities.\n",
        "\n",
        "```\n",
        "!blenderproc run examples/basics/basic/main.py\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "rTxOAJZZNKNt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.1. Getting started with BlenderProc\n",
        "\n",
        "Colab allows us to open a script in an integrated editor. To do so, open the \"Files\" panel on the left (File icon), navigate to `Resources_DNN4VC_Synthetic/scripts/` and double click `quickstart.py`. The file will open on the right side of the screen. Get familiar with the script and test it using the code block below.\n",
        "\n",
        "\n",
        "**Task 1a)**: Do the following things:\n",
        "\n",
        "1. Use `bproc.loader.load_obj` to load `obj_000002.ply` (path was given above, when viewing the Mesh).\n",
        "2. Set a custom camera matrix with `bproc.camera.set_intrinsics_from_K_matrix` [docs](https://dlr-rm.github.io/BlenderProc/blenderproc.api.camera.html). Use the camera matrix given below. Render images with the size 320 by 240. Caution, the camera calibration is given for a resolution of 640 by 480. It has to be scaled accordingly.\n",
        "3. Adapt the light position and transform mat according to the scale of our example object. For the example object the unit is cm while for the Monkey it is m.\n",
        "\n",
        "\\begin{equation*}\n",
        "\\mathbf{C} = \\begin{bmatrix}\n",
        "537.5 & 0 & 318.9 \\\\\n",
        "0 & 536.1 & 238.4 \\\\\n",
        "0 & 0 & 1\n",
        "\\end{bmatrix}\n",
        "\\end{equation*}\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "o6QsiO1AT9w2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "While adapting the code, you can check the generated image using the following code lines. The output is shown by executing the code box below. You can play around with the code to get familiar with BlenderProc.\n",
        "\n",
        "**IMPORTANT** When you are ready with code, make sure to save a copy, as the files will not be necessarily be saved when restarting the notebook."
      ],
      "metadata": {
        "id": "V-4o5wFPU9pB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!blenderproc run /content/Resources_DNN4VC_Synthetic/scripts/quickstart.py --blender-install-path ./\n",
        "%run \"-m\" \"blenderproc\" \"vis\" \"hdf5\" \"output/0.hdf5\""
      ],
      "metadata": {
        "id": "4mNVNTrHODXy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2 Generating the training dataset\n",
        "\n",
        "Now, we will generate the actual dataset. Similar to our test data, the object should be placed in upright position in a room (without ceiling) shaped test environment with random material.\n",
        "\n",
        "**Task 1b)**:\n",
        "The script to do so is already given, but a couple of tasks have to be performed.\n",
        "\n",
        "1. Use correct camera matrix (see Task 1a).\n",
        "2. Load and place distractor objects\n",
        "3. Complete the 'room' environment\n",
        "4. Sample random camera poses\n",
        "5. Activate segmentation output\n",
        "6. Save dataset in the same format as the testing data.\n",
        "\n",
        "A detailed despriction is given directly in the script and should be added there.\n",
        "\n",
        "The code box below allows to test the script. The initial version can be executed, but will only save an example output.\n"
      ],
      "metadata": {
        "id": "clCUPJNtT4ZG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%env BASE_PATH=/content/Resources_DNN4VC_Synthetic\n",
        "!blenderproc run $BASE_PATH/scripts/dataset_generator.py \\\n",
        "    --mesh_path $BASE_PATH/resources/lm \\\n",
        "    --cc_textures_path $BASE_PATH/resources/cctextures \\\n",
        "    --output_dir $BASE_PATH/train_data \\\n",
        "    --num_samples 1 \\\n",
        "    --start_index 1 \\\n",
        "    --blender-install-path ./"
      ],
      "metadata": {
        "id": "uGnxe9vbYikf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**IMPORTANT** When you are ready with code, make sure to save a copy, as the files will not be necessarily be saved when restarting the notebook."
      ],
      "metadata": {
        "id": "71XYU2-DYcvu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Final dataset creation:**\n",
        "7. Each execution of the scripts creates n samples. Use start_index and num samples and generate at least 256 images (16 calls with each 16 images). Make sure that the output is correct before generating the output. You could create a loop in python to automate everything\n",
        "`os.system(f\"blenderproc run ...\")`. Generating the dataset might take 10 minutes."
      ],
      "metadata": {
        "id": "o0VzsBUoXhxL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Generate the whole dataset (256 images) by calling the script 16 times for 16 samples each.\n",
        "Perhaps you find an elegant solution.\n",
        "'''"
      ],
      "metadata": {
        "id": "BeoPZEteXKDg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**IMPORTANT**: Download you generated dataset as a backup.First create a tar file and the download the file via the \"Files\" plane. Also download your edited scripts (if not done already)."
      ],
      "metadata": {
        "id": "3oKekYl9LVL3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!tar -czf /content/Resources_DNN4VC_Synthetic/train_data.tar.gz /content/Resources_DNN4VC_Synthetic/train_data"
      ],
      "metadata": {
        "id": "gacnJjFxLb86"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Part 2: Data augmentation\n",
        "\n",
        "Now, we are going to build a data loader, which performs data augmentation to create more variation in our training data."
      ],
      "metadata": {
        "id": "BgMazXXHLbOn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/Resources_DNN4VC_Synthetic/"
      ],
      "metadata": {
        "id": "h6SSjf9hPDAm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "First all packages, we will need later will be importet. This example will mainly use torch and torchvision."
      ],
      "metadata": {
        "id": "ZD15tTCwSh0N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import os\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms.functional as TF\n",
        "\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision.models.mobilenetv2 import InvertedResidual\n",
        "\n",
        "from utils.utils import plot_batch, plot_output\n",
        "from utils.imgaug_pipeline import heavy_augmentation\n",
        "\n"
      ],
      "metadata": {
        "id": "YcUVVBREPWNv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1 Preparing dataset and dataloader\n",
        "\n",
        "First a custom dataset is created.\n",
        "\n",
        "In the constructor a list of all files in the \"img\" dir is prepared.\n",
        "The `__getitem__` function will load a sample and transform the mask and the image with a **torchvision** transformation.\n",
        "\n",
        "It will then return a single sample constiting of an image and a mask.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "TCIISwWQTMeJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomDataset(Dataset):\n",
        "    def __init__(\n",
        "        self, root_dir, transform, transform_mask, geometric_augmentation=False, imgaug_heavy_augmentation=False\n",
        "    ):\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.transform_mask = transform_mask\n",
        "\n",
        "        self.img_dir = os.path.join(root_dir, \"img\")\n",
        "        self.mask_dir = os.path.join(root_dir, \"masks\")\n",
        "        self.img_filenames = sorted(os.listdir(self.img_dir))\n",
        "        self.geometric_augmentation = geometric_augmentation  # This will be used later\n",
        "        self.imgaug_heavy_augmentation = imgaug_heavy_augmentation # This will be used later\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_filenames)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_filename = self.img_filenames[idx]\n",
        "        img_path = os.path.join(self.img_dir, img_filename)\n",
        "        mask_path = os.path.join(self.mask_dir, img_filename)\n",
        "\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "        mask = Image.open(mask_path).convert(\"L\")\n",
        "\n",
        "        # This will be used later\n",
        "        if self.geometric_augmentation:\n",
        "            image, mask = self.common_transform(image, mask)\n",
        "\n",
        "        # This will be used later\n",
        "        if self.imgaug_heavy_augmentation:\n",
        "            image = self.apply_imgaug_pipeline(image)\n",
        "\n",
        "        image = self.transform(image)\n",
        "        mask = self.transform_mask(mask)\n",
        "\n",
        "\n",
        "        return image, mask\n",
        "\n",
        "    def common_transform(self, image, mask, rotation_angle=15, crop_offset=16, img_size=224):\n",
        "\n",
        "        '''\n",
        "        Add augmentation here and use the same parameters for image and mask\n",
        "        IMPORTANT: Use BILINEAR inperpolation when rotating, scaling the image and nearest neighbor interpolation when doing the same with the mask.\n",
        "        '''\n",
        "        # Random rotation (hint: random.uniform, TF.rotate)\n",
        "        # ...\n",
        "\n",
        "        # Random crop  (hint: transforms.RandomCrop.get_params, TF.crop)\n",
        "        # ...\n",
        "\n",
        "        # Resize to 224x224\n",
        "        # image = ...\n",
        "        # mask = ...\n",
        "\n",
        "        # Random horizontal flipping\n",
        "        #if random.random() > 0.5:\n",
        "        #    image = ...\n",
        "        #    mask = ...\n",
        "\n",
        "        # Random horizontal flipping\n",
        "        # ...\n",
        "        return image, mask\n",
        "\n",
        "    def apply_imgaug_pipeline(self, image):\n",
        "      image = np.array(image)\n",
        "      image = heavy_augmentation(image=image)\n",
        "\n",
        "      return Image.fromarray(image)\n",
        "\n"
      ],
      "metadata": {
        "id": "nhGAATCdRf8d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The transformation for image and mask are prepared for the test data. `transforms.Compose` creates a transformation pipeline from several operations applied one after another.\n",
        "\n",
        "The test dataset is created and a `DataLoader` is generated. It will load a batch of data (a group of samples) and return them stacked along the first dimension.\n",
        "\n",
        "You can create a test batch and display it with the following code. As you see, the files are sorted in the order of the sequence since `shuffle=False` which is common for test data.\n",
        "\n",
        "In this example, we crop all test images around the center. The input resolution of the network we will construct later is 224."
      ],
      "metadata": {
        "id": "jtwG9kiiUMW4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the paths to your train and test data folders\n",
        "train_data_path = \"train_data\"\n",
        "test_data_path = \"test_data\"\n",
        "\n",
        "# Define the transformations to be applied to the images and masks\n",
        "transform_img = transforms.Compose(\n",
        "    [\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "    ]\n",
        ")\n",
        "\n",
        "transform_mask = transforms.Compose(\n",
        "    [\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Create the testing dataset\n",
        "test_dataset = CustomDataset(\n",
        "    root_dir=test_data_path,\n",
        "    transform=transform_img,\n",
        "    transform_mask=transform_mask\n",
        ")\n",
        "\n",
        "# Define the batch size for the DataLoader\n",
        "batch_size = 16\n",
        "\n",
        "# Create the testing data loader\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Test the data loaders\n",
        "batch_images, batch_masks = next(iter(test_loader))\n",
        "plot_batch(batch_images, batch_masks)"
      ],
      "metadata": {
        "id": "jZx5-fFPQ5NK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2. Build an input pipeline with torchvision\n",
        "\n",
        "**Task 2a)** Crate a train_dataset.\n",
        "\n",
        "Create a input pipeline with simple augmentation and use [transforms.RandomApply](https://pytorch.org/vision/stable/generated/torchvision.transforms.RandomApply.html).\n",
        "\n",
        "The pipeline might use `transforms.ColorJitter`, `transforms.GaussianBlur` and `transforms.Grayscale`.\n",
        "\n",
        "`transforms.ColorJitter` should be applied for brightness, contrast, saturation and hue separately. For hue a very small strength e.g. 0.05 is sufficient. The strength of the other augmentations can be higher (e.g. 0.5).\n",
        "`transforms.GaussianBlur` should use a kernel size of 3.\n",
        "`transforms.Grayscale` should be applyed at the end of the pipeline with a low probabily (max 10%).\n",
        "\n"
      ],
      "metadata": {
        "id": "afBXWBszWNod"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transform_img_augmented = transforms.Compose(\n",
        "    [\n",
        "        transforms.CenterCrop(224),\n",
        "        #add more transforms here\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "    ]\n",
        ")\n",
        "\n",
        "transform_mask_augmented = transforms.Compose(\n",
        "    [\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "    ]\n",
        ")\n",
        "\n",
        "train_dataset = CustomDataset(\n",
        "    root_dir=train_data_path,\n",
        "    transform=transform_img_augmented,\n",
        "    transform_mask=transform_mask_augmented,\n",
        "    geometric_augmentation=False\n",
        ")\n",
        "\n",
        "# Create the training data loader\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Test the data loaders\n",
        "batch_images, batch_masks = next(iter(train_loader))\n",
        "plot_batch(batch_images, batch_masks)"
      ],
      "metadata": {
        "id": "vQ8q8S9EY8nv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2 Adding geometric augmentations for rgb image and mask\n",
        "\n",
        "More common augmentations include random flipping of the image, random cropping the image and random rotation.\n",
        "\n",
        "The reason we did not apply them in the pipeline is that we exactly the same crop, rotation for both the image and the mask to have consistent training data.\n",
        "\n",
        "So far, we used two separate pipelines and unfortunatley they do not share parameters.\n",
        "\n",
        "**Task 2b)** Implement the function `common_augmentation` in `CustomDataset(Dataset)` (above) to apply, random cropping, rescaling, random rotation and random fipping. You already find the function above, but it has to be finished.\n",
        "\n",
        "Adapt the codebox above to use the color as well as the geometric augmentations.\n",
        "\n",
        "**Implement the augmentations and visualize a batch.**"
      ],
      "metadata": {
        "id": "7N8xxkxsZ2D4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Augmentation with imgaug\n",
        "\n",
        "Augmentation does not end here and there are much more options.\n",
        "Currently not all of them are implemented in torchvision.\n",
        "\n",
        "An alternative is the [imgaug](https://github.com/aleju/imgaug#documentation) library.\n",
        "\n",
        "**Task 2c) (optional)**: Take a look at the prepared imgaug pipline in /content/Resources_DNN4VC_Synthetic/utils/imgaug_pipeline.py and see which operations are used there.\n",
        "\n",
        "Test the pipeline with the script below. Feel free to explore more combinations of augmentations by changing `imgaug_pipeline.py `."
      ],
      "metadata": {
        "id": "j7RyMPWhf4G0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transform_img_augmented = transforms.Compose(\n",
        "    [\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "    ]\n",
        ")\n",
        "\n",
        "transform_mask_augmented = transforms.Compose(\n",
        "    [\n",
        "        transforms.ToTensor(),\n",
        "    ]\n",
        ")\n",
        "\n",
        "train_dataset = CustomDataset(\n",
        "    root_dir=train_data_path,\n",
        "    transform=transform_img_augmented,\n",
        "    transform_mask=transform_mask_augmented,\n",
        "    geometric_augmentation=True,\n",
        "    imgaug_heavy_augmentation=True\n",
        ")\n",
        "\n",
        "# Create the training data loader\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Test the data loaders\n",
        "batch_images, batch_masks = next(iter(train_loader))\n",
        "plot_batch(batch_images, batch_masks)"
      ],
      "metadata": {
        "id": "3Bi0wkNnhnlN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 3 Training a segmentation model\n",
        "Now, after the training data and the augmentation is ready, we want to actually use the data for our initial task.\n",
        "Predicting a sementic segmentation for our test object.\n",
        "\n",
        "\n",
        "## 3.1 Building a decoder for a given backbone - Implementing a U-Net with MobilenetV2\n",
        "\n",
        "Our segmentation network should combine several concepts from the lecture. We will use [MobileNetV2](https://pytorch.org/hub/pytorch_vision_mobilenet_v2/) as a backbone and want to construct a U-Net-like decoder on top of it.\n",
        "\n",
        "**Task 3a)**: Finish the implementation by constructing the decoder.\n",
        "\n",
        "1. Inplement the `DecoderBlock` each decoder block should upsample the input features by a factor of two (in width and height), then concatenate the skip connection to the features and then pass the features through six more layers (2x conv layer, batchnorm layer, relu layer).\n",
        "2. Implement a `MobileNetV2Decoder` decoder with 4 decoder blocks using skip connections.\n",
        "  * After the 4 decoder blocks, perform an additional upsampling step.\n",
        "  * Apply a final 1x1 convolution to reduce the feature dimension to match the number of classes.\n",
        "3. Implement the `__init__` and `forward_pass` functions of `SegmentationModel`"
      ],
      "metadata": {
        "id": "09XNkNFNoOGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(DecoderBlock, self).__init__()\n",
        "\n",
        "        '''\n",
        "        init layers here  (upsample, conv, bn, relu, conv, bn, relu)\n",
        "        '''\n",
        "        # ...\n",
        "\n",
        "    def forward(self, x, skip_connection):\n",
        "        '''\n",
        "        upsample first, then concat skip connection, then pass trough remaining layers\n",
        "        '''\n",
        "        # ...\n",
        "\n",
        "        return x\n",
        "\n",
        "class MobileNetV2Decoder(nn.Module):\n",
        "    def __init__(self, encoder_channels, num_classes):\n",
        "        super(MobileNetV2Decoder, self).__init__()\n",
        "\n",
        "        self.decoder_blocks = nn.ModuleList()\n",
        "\n",
        "        # add four decoder blocks to module list\n",
        "        #for i, encoder_out_channels in enumerate(encoder_channels[:-1]):\n",
        "            # input dim of a decoder block is the input dim of the previous block plus the input dim of the side input\n",
        "\n",
        "        # init final upsampling layer and final conv layer (kernel size=1)\n",
        "\n",
        "\n",
        "    def forward(self, x, skip_connections):\n",
        "\n",
        "        # pass through the decoder blocks and add the approporiate skip connections\n",
        "        # for i, decoder_block in enumerate(self.decoder_blocks):\n",
        "        #      ....\n",
        "\n",
        "        # upsample\n",
        "\n",
        "        # pass through final conv layer\n",
        "\n",
        "        return x\n",
        "\n",
        "class SegmentationModel(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(SegmentationModel, self).__init__()\n",
        "\n",
        "        # Load the pre-trained MobileNetV2 backbone\n",
        "        self.backbone = models.mobilenet_v2(weights=models.MobileNet_V2_Weights.DEFAULT).features\n",
        "        # Modify the MobileNetV2 backbone to remove the last two layers\n",
        "        self.backbone = nn.Sequential(*list(self.backbone.children())[:-2])\n",
        "\n",
        "        # Assuming we connect the backbone to the given blocks,\n",
        "        # we want to find out the out_channels of the respective layers to build the skip connections\n",
        "        self.connectors = [1, 3, 6, 10, 16]\n",
        "\n",
        "        '''\n",
        "        collect number of output channels from backbone\n",
        "        '''\n",
        "        encoder_channels = []\n",
        "        #for i, module in enumerate(self.backbone):\n",
        "            #if i in self.connectors:\n",
        "            #    ...\n",
        "\n",
        "        # Reverse the list to match with decoder blocks\n",
        "        ...\n",
        "        # init the MobileNetV2Decoder\n",
        "        ...\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Pass the input through the backbone\n",
        "        backbone_out = []\n",
        "\n",
        "        '''\n",
        "        apply the backbone module by module and store the side outputs\n",
        "        '''\n",
        "        #for i, module in enumerate(self.backbone):\n",
        "            #x = module(x)\n",
        "            #if i in self.connectors[:-1]:\n",
        "            #    ...\n",
        "        # Reverse the order of backbone_out to match with decoder blocks\n",
        "        # ...\n",
        "        # Pass the output of the backbone through the decoder\n",
        "        # ...\n",
        "\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "l-ReyAH_KcVD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.2 Implementing Test Metrics\n",
        "\n",
        "In the lecture different evaluation metrics have been presented to evaluate semantic segmentation.\n",
        "\n",
        "**Task 3b)** Implement the *F1 score* and the *Intersection over Union* metrics to evaluate the performance of the network on the validation/test data."
      ],
      "metadata": {
        "id": "wEK5VUy3pIpC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_iou(pred, target):\n",
        "    # pred and target are in binary format\n",
        "    # to-do: implement iou\n",
        "    # return batch average\n",
        "    return 0\n",
        "\n",
        "\n",
        "def compute_f1_score(pred, target):\n",
        "    # pred and target are in binary format\n",
        "    # to-do: implement f1\n",
        "    # return batch average\n",
        "    return 0"
      ],
      "metadata": {
        "id": "P6JUNPQgOXZT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.3 Training the model\n",
        "\n",
        "In the following, the actual training and validation loop is already given.\n",
        "\n",
        "**Task 3c)** Train the model with our generated training data with and without augmentation. You can either use the heavy imgaug augmentation or the augmentation with torchvision augmentation.\n",
        "\n",
        "The network will plot a graph of test and train losses as well as the test metrics. Output is saved in /content/Resources_DNN4VC_Synthetic/output.\n",
        "Also the estimated masks are saved.\n",
        "\n",
        "Compare the results. You will see that regarding the metrics the effect of the augmentation is rather limited in our particular case.\n",
        "\n",
        "How would you interpret the results?\n"
      ],
      "metadata": {
        "id": "7Sw92X7ioo_6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output_interval = 5\n",
        "learning_rate = 0.001\n",
        "num_classes = 2\n",
        "num_epochs = 30\n",
        "\n",
        "random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "\n",
        "output_dir = \"output/iteration_output\"  # Directory to save iteration outputs\n",
        "\n",
        "# Create the output directory if it doesn't exist\n",
        "if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)\n",
        "\n",
        "# Create an instance of the SegmentationModel\n",
        "model = SegmentationModel(num_classes)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Move the model to the appropriate device (e.g., GPU if available)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Training loop\n",
        "train_losses = []\n",
        "test_losses = []\n",
        "miou_scores = []\n",
        "mf1_scores = []\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss = 0.0\n",
        "    val_loss = 0.0\n",
        "    total_iou = 0.0\n",
        "    total_f1 = 0.0\n",
        "    num_samples = 0\n",
        "\n",
        "    model.train()  # Set the model to training mode\n",
        "\n",
        "    # Initialize the progress bar\n",
        "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\", ncols=80)\n",
        "\n",
        "    for img, mask in pbar:\n",
        "        # Move the data to the appropriate device\n",
        "        img = img.to(device)\n",
        "        mask = mask.squeeze(1).long().to(device)\n",
        "        # Zero the gradients\n",
        "        optimizer.zero_grad()\n",
        "        # Forward pass\n",
        "        output = model(img)\n",
        "        # Compute the loss\n",
        "        loss = criterion(output, mask)\n",
        "        # Backward pass and optimization\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        # Update the progress bar description with the current loss\n",
        "        pbar.set_postfix({\"Loss\": loss.item()})\n",
        "        train_loss += loss.item()\n",
        "\n",
        "    train_loss /= len(train_loader)\n",
        "    train_losses.append(train_loss)\n",
        "\n",
        "    # Print epoch summary\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs} | Training Loss: {train_loss}\")\n",
        "\n",
        "    # Validation loop\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    iteration = 0  # Initialize the iteration counter\n",
        "    with torch.no_grad():\n",
        "        for val_img, val_mask in test_loader:\n",
        "            val_img = val_img.to(device)\n",
        "            val_mask = val_mask.squeeze(1).long().to(device)\n",
        "            val_output = model(val_img)\n",
        "            val_loss += criterion(val_output, val_mask).item()\n",
        "\n",
        "            # Compute IoU\n",
        "            val_pred = torch.argmax(val_output, dim=1)\n",
        "\n",
        "            iou = compute_iou(val_pred, val_mask)  # Function to calculate IoU\n",
        "\n",
        "            total_iou += iou.sum().item() * val_img.size(0)\n",
        "\n",
        "            f1 = compute_f1_score(val_pred, val_mask)  # Function to calculate f1 score\n",
        "\n",
        "            total_f1 += f1.sum().item() * val_img.size(0)\n",
        "\n",
        "            num_samples += val_img.size(0)\n",
        "\n",
        "            if (epoch + 1) % output_interval == 0:\n",
        "                output_filename = f\"iteration_{iteration}.png\"\n",
        "                output_path = os.path.join(output_dir, output_filename)\n",
        "                plot_batch(val_img, val_pred.float().unsqueeze(1), output_path)\n",
        "\n",
        "            iteration += 1\n",
        "\n",
        "        val_loss /= len(test_loader)\n",
        "        mean_iou = total_iou / num_samples\n",
        "        mean_f1 = total_f1 / num_samples\n",
        "\n",
        "        test_losses.append(val_loss)\n",
        "        miou_scores.append(mean_iou)\n",
        "        mf1_scores.append(mean_f1)\n",
        "\n",
        "        plot_output(train_losses, test_losses, miou_scores, mf1_scores, epoch, output_dir)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs} | Validation Loss: {val_loss} | mIoU: {mean_iou} | f1: {mean_f1}\")\n"
      ],
      "metadata": {
        "id": "SJSSBHdgopp_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**IMPORTANT**: Download you generated outputs.First create a tar file and the download the file via the \"Files\" plane."
      ],
      "metadata": {
        "id": "U7ikmb-QtDUo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!tar -czf /content/Resources_DNN4VC_Synthetic/output.tar.gz /content/Resources_DNN4VC_Synthetic/output"
      ],
      "metadata": {
        "id": "WU5P7jCFtMHi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusion\n",
        "\n",
        "In this exercise, we gained knowledge and practical experience in generating synthetic training data that closely resembles real-world images, which we utilized to train a semantic segmentation network. Additionally, we explored different methods of data augmentation implementation.\n",
        "\n",
        "What do you think, in which other cases could we make use of synthetic training data?\n",
        "In which cases do you think is data augmentation especially important?\n",
        "\n",
        "We will discuss these questions during the exercise. I hope that you can use the gained insights in your future vision projects. Please let me know if you had any troubles.\n",
        "\n",
        "**Please remember to hand in all the requested files (see Introduction).**\n",
        "\n"
      ],
      "metadata": {
        "id": "qHKYQxSVPUFb"
      }
    }
  ]
}